


<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LooperXX's homepage">
      
      
      
        <meta name="author" content="Looper - Xiao Xu">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.1">
    
    
      
        <title>01 Introduction and Word Vectors - Science is interesting.</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.a676eddb.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-164217558-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    <body dir="">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lecture-01-introduction-and-word-vectors" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="">
    <a href=".." title="Science is interesting." class="md-header-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Science is interesting.
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              01 Introduction and Word Vectors
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
          

  

<nav class="md-tabs md-tabs--active" aria-label="" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." class="md-tabs__link">
        Home
      </a>
    
  </li>

      
        
  
  
    
    
  
  
    <li class="md-tabs__item">
      
        <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" class="md-tabs__link">
          NLP
        </a>
      
    </li>
  

  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../Normalization/" class="md-tabs__link">
          ML & DL
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-tabs__link md-tabs__link--active">
          CS224n学习笔记
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../MkDocs_demo/" class="md-tabs__link">
          For MkDocs
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Science is interesting." class="md-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    Science is interesting.
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      NLP
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="NLP" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        NLP
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-1" type="checkbox" id="nav-2-1">
    
    <label class="md-nav__link" for="nav-2-1">
      理论笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="理论笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-1">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        理论笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" title="自然语言处理简介" class="md-nav__link">
      自然语言处理简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%E7%9A%84%E5%B7%A8%E4%BA%BA%E8%82%A9%E8%86%80/" title="NLP的巨人肩膀" class="md-nav__link">
      NLP的巨人肩膀
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Attention/" title="Attention" class="md-nav__link">
      Attention
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2">
    
    <label class="md-nav__link" for="nav-2-2">
      代码学习
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="代码学习" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        代码学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../NCRF%2B%2B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="NCRF++" class="md-nav__link">
      NCRF++
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-3" type="checkbox" id="nav-2-3">
    
    <label class="md-nav__link" for="nav-2-3">
      书籍笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="书籍笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        书籍笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" title="Machine Reading Comprehension" class="md-nav__link">
      Machine Reading Comprehension
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%20Concepts/" title="Some Concepts" class="md-nav__link">
      Some Concepts
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NNDL%20%E4%B9%A0%E9%A2%98/" title="NNDL 习题" class="md-nav__link">
      NNDL 习题
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      ML & DL
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="ML & DL" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        ML & DL
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Normalization/" title="Normalization" class="md-nav__link">
      Normalization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%8A%B1%E4%B9%A6%E7%BB%8F%E9%AA%8C%E6%B3%95%E5%88%99/" title="花书经验法则" class="md-nav__link">
      花书经验法则
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      CS224n学习笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="CS224n学习笔记" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        CS224n学习笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" title="CS224n-2019简介" class="md-nav__link">
      CS224n-2019简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-Assignment/" title="CS224n-2019作业笔记" class="md-nav__link">
      CS224n-2019作业笔记
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        01 Introduction and Word Vectors
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="01 Introduction and Word Vectors" class="md-nav__link md-nav__link--active">
      01 Introduction and Word Vectors
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-01-introduction-and-word-vectors" class="md-nav__link">
    Lecture 01 Introduction and Word Vectors
  </a>
  
    <nav class="md-nav" aria-label="Lecture 01 Introduction and Word Vectors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#human-language-and-word-meaning" class="md-nav__link">
    Human language and word meaning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec-introduction" class="md-nav__link">
    Word2vec introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec-objective-function" class="md-nav__link">
    Word2vec objective function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec-prediction-function" class="md-nav__link">
    Word2vec prediction function
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-01-introduction-svd-and-word2vec" class="md-nav__link">
    Notes 01  Introduction, SVD and Word2Vec
  </a>
  
    <nav class="md-nav" aria-label="Notes 01  Introduction, SVD and Word2Vec">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-to-natural-language-processing" class="md-nav__link">
    Introduction to Natural Language Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word-vectors" class="md-nav__link">
    Word Vectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#svd-based-methods" class="md-nav__link">
    SVD Based Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#iteration-based-methods-word2vec" class="md-nav__link">
    Iteration Based Methods - Word2vec
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gensim-word-vectors-example" class="md-nav__link">
    Gensim word vectors example
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">
      02 Word Vectors 2 and Word Senses
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">
      03 Word Window Classification,Neural Networks, and Matrix Calculus
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">
      04 Backpropagation and Computation Graphs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-nav__link">
      05 Linguistic Structure Dependency Parsing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" title="06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">
      06 The probability of a sentence Recurrent Neural Networks and Language Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" title="07 Vanishing Gradients and Fancy RNNs" class="md-nav__link">
      07 Vanishing Gradients and Fancy RNNs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/" title="08 Machine Translation, Sequence-to-sequence and Attention" class="md-nav__link">
      08 Machine Translation, Sequence-to-sequence and Attention
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" title="09 Practical Tips for Final Projects" class="md-nav__link">
      09 Practical Tips for Final Projects
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/" title="10 Question Answering and the Default Final Project" class="md-nav__link">
      10 Question Answering and the Default Final Project
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" title="11 ConvNets for NLP" class="md-nav__link">
      11 ConvNets for NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/" title="12 Information from parts of words Subword Models" class="md-nav__link">
      12 Information from parts of words Subword Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/" title="13 Modeling contexts of use Contextual Representations and Pretraining" class="md-nav__link">
      13 Modeling contexts of use Contextual Representations and Pretraining
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/" title="14 Transformers and Self-Attention For Generative Models" class="md-nav__link">
      14 Transformers and Self-Attention For Generative Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-15-Natural%20Language%20Generation/" title="15 Natural Language Generation" class="md-nav__link">
      15 Natural Language Generation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-16-Coreference%20Resolution/" title="16 Coreference Resolution" class="md-nav__link">
      16 Coreference Resolution
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-17-Multitask%20Learning/" title="17 Multitask Learning" class="md-nav__link">
      17 Multitask Learning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/" title="18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment" class="md-nav__link">
      18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/" title="19 Safety, Bias, and Fairness" class="md-nav__link">
      19 Safety, Bias, and Fairness
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/" title="20 The Future of NLP + Deep Learning" class="md-nav__link">
      20 The Future of NLP + Deep Learning
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      For MkDocs
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="For MkDocs" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        For MkDocs
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../MkDocs_demo/" title="Demo" class="md-nav__link">
      Demo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Material%20Theme%20Tutorial/" title="Material Theme Tutorial" class="md-nav__link">
      Material Theme Tutorial
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-01-introduction-and-word-vectors" class="md-nav__link">
    Lecture 01 Introduction and Word Vectors
  </a>
  
    <nav class="md-nav" aria-label="Lecture 01 Introduction and Word Vectors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#human-language-and-word-meaning" class="md-nav__link">
    Human language and word meaning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec-introduction" class="md-nav__link">
    Word2vec introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec-objective-function" class="md-nav__link">
    Word2vec objective function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word2vec-prediction-function" class="md-nav__link">
    Word2vec prediction function
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-01-introduction-svd-and-word2vec" class="md-nav__link">
    Notes 01  Introduction, SVD and Word2Vec
  </a>
  
    <nav class="md-nav" aria-label="Notes 01  Introduction, SVD and Word2Vec">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-to-natural-language-processing" class="md-nav__link">
    Introduction to Natural Language Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#word-vectors" class="md-nav__link">
    Word Vectors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#svd-based-methods" class="md-nav__link">
    SVD Based Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#iteration-based-methods-word2vec" class="md-nav__link">
    Iteration Based Methods - Word2vec
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gensim-word-vectors-example" class="md-nav__link">
    Gensim word vectors example
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/LooperXX/LooperXX.github.io/edit/master/docs/CS224n-2019-01-Introduction and Word Vectors.md" title="编辑此页" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71,7.04C21.1,6.65 21.1,6 20.71,5.63L18.37,3.29C18,2.9 17.35,2.9 16.96,3.29L15.12,5.12L18.87,8.87M3,17.25V21H6.75L17.81,9.93L14.06,6.18L3,17.25Z" /></svg>
                  </a>
                
                
                  
                
                
                  <h1>01 Introduction and Word Vectors</h1>
                
                <h2 id="lecture-01-introduction-and-word-vectors">Lecture 01 Introduction and Word Vectors<a class="headerlink" href="#lecture-01-introduction-and-word-vectors" title="Permanent link">&para;</a></h2>
<p><strong>Lecture Plan</strong></p>
<ul>
<li>The course</li>
<li>Human language and word meaning</li>
<li>Word2vec introduction</li>
<li>Word2vec objective function gradients</li>
<li>Optimization basics</li>
<li>Looking at word vectors</li>
</ul>
<h3 id="human-language-and-word-meaning">Human language and word meaning<a class="headerlink" href="#human-language-and-word-meaning" title="Permanent link">&para;</a></h3>
<p>人类之所以比类人猿更“聪明”，是因为我们有语言，因此是一个人机网络，其中人类语言作为网络语言。人类语言具有 <strong>信息功能</strong> 和 <strong>社会功能</strong> 。</p>
<p>据估计，人类语言只有大约5000年的短暂历。语言是人类变得强大的主要原因。写作是另一件让人类变得强大的事情。它是使知识能够在空间上传送到世界各地，并在时间上传送的一种工具。</p>
<p>但是，相较于如今的互联网的传播速度而言，人类语言是一种缓慢的语言。然而，只需人类语言形式的几百位信息，就可以构建整个视觉场景。这就是自然语言如此迷人的原因。</p>
<p><strong>How do we represent the meaning of a word?</strong></p>
<p><strong><em>meaning</em></strong></p>
<ul>
<li>用一个词、词组等表示的概念。</li>
<li>一个人想用语言、符号等来表达的想法。</li>
<li>表达在作品、艺术等方面的思想</li>
</ul>
<p>理解意义的最普遍的语言方式(<strong>linguistic way</strong>) : 语言符号与语言符号的意义的转化
$$
\boxed{\text{signifier(symbol)}\Leftrightarrow \text{signified(idea or thing)}} \
= \textbf{denotational semantics}
$$</p>
<blockquote>
<p>denotational semantics 指称语义</p>
</blockquote>
<p><strong>How do we have usable meaning in a computer?</strong></p>
<p><strong><em>WordNet</em></strong>, 一个包含同义词集和上位词(“is a”关系) <strong><em>synonym sets and hypernyms</em></strong> 的列表的辞典</p>
<p><img alt="image-20191112172053381" src="../imgs/image-20191112172053381.png" /></p>
<p><strong>Problems with resources like WordNet</strong></p>
<ul>
<li>作为一个资源很好，但忽略了细微差别<ul>
<li>例如“proficient”被列为“good”的同义词。这只在某些上下文中是正确的。</li>
</ul>
</li>
<li>缺少单词的新含义<ul>
<li>难以持续更新</li>
<li>例如 wicked, badass, nifty, wizard, genius, ninja, bombest</li>
</ul>
</li>
<li>主观的</li>
<li>需要人类劳动来创造和调整</li>
<li>无法计算单词相似度</li>
</ul>
<p><strong>Representing words as discrete symbols</strong></p>
<p>在传统的自然语言处理中，我们把词语看作离散的符号: hotel, conference, motel - a <strong>localist</strong> representation。单词可以通过独热向量(one-hot vectors，只有一个1，其余均为0的稀疏向量) 。向量维度=词汇量(如500,000)。</p>
<div>
<div class="MathJax_Preview">
motel = [0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  1 \  0 \  0 \  0 \  0] \\ hotel = [0 \  0 \  0 \  0 \  0 \  0 \  0 \  1 \  0 \  0 \  0 \  0 \  0 \  0 \  0]
</div>
<script type="math/tex; mode=display">
motel = [0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  0 \  1 \  0 \  0 \  0 \  0] \\ hotel = [0 \  0 \  0 \  0 \  0 \  0 \  0 \  1 \  0 \  0 \  0 \  0 \  0 \  0 \  0]
</script>
</div>
<p><strong>Problem with words as discrete symbols</strong></p>
<p>所有向量是正交的。对于独热向量，没有关于相似性概念，并且向量维度过大。</p>
<p><strong>Solutions</strong></p>
<ul>
<li>使用类似 <strong><em>WordNet</em></strong> 的工具中的列表，获得相似度，但会因不够完整而失败</li>
<li>学习在向量本身中编码相似性</li>
</ul>
<p><strong>Representing words by their context</strong></p>
<ul>
<li><strong><u>Distributional semantics</u></strong> ：一个单词的意思是由经常出现在它附近的单词给出的<ul>
<li><em>“You shall know a word by the company it keeps”</em> (J. R. Firth 1957: 11)</li>
<li>现代统计NLP最成功的理念之一</li>
<li>有点物以类聚，人以群分的感觉</li>
</ul>
</li>
<li>当一个单词<span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)。</li>
<li>使用<span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>的许多上下文来构建<span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>的表示</li>
</ul>
<p><img alt="示例" src="../imgs/1560069660365.png" /></p>
<h3 id="word2vec-introduction">Word2vec introduction<a class="headerlink" href="#word2vec-introduction" title="Permanent link">&para;</a></h3>
<p>我们为每个单词构建一个 <strong>密集</strong> 的向量，使其与出现在相似上下文中的单词向量相似</p>
<p>词向量 <strong><em>word vectors</em></strong> 有时被称为词嵌入 <strong><em>word embeddings</em></strong>  或词表示 <strong><em>word representations</em></strong>  </p>
<p>它们是分布式表示 <strong><em>distributed representation</em></strong></p>
<p><img alt="1560142035593" src="../imgs/1560142035593.png" /></p>
<p><strong><em>Word2vec</em></strong> (Mikolov et al. 2013)是一个学习单词向量的 <strong>框架</strong> </p>
<p><strong>IDEA</strong>：</p>
<ul>
<li>我们有大量的文本 (corpus means 'body' in Latin. 复数为corpora)</li>
<li>固定词汇表中的每个单词都由一个向量表示</li>
<li>文本中的每个位置 <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>，其中有一个中心词 <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> 和上下文(“外部”)单词 <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span> </li>
<li>使用 <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> 和 <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span> 的 <strong>词向量的相似性</strong> 来计算给定 <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> 的 <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span> 的 <strong>概率</strong> (反之亦然)</li>
<li><strong>不断调整词向量</strong> 来最大化这个概率</li>
</ul>
<p>下图为窗口大小 <span><span class="MathJax_Preview">j=2</span><script type="math/tex">j=2</script></span> 时的 <span><span class="MathJax_Preview">P\left(w_{t+j} | w_{t}\right)</span><script type="math/tex">P\left(w_{t+j} | w_{t}\right)</script></span> 计算过程，center word分别为 <span><span class="MathJax_Preview">into</span><script type="math/tex">into</script></span> 和 <span><span class="MathJax_Preview">banking</span><script type="math/tex">banking</script></span></p>
<p><img alt="1560070410531" src="../imgs/1560070410531.png" /></p>
<p><img alt="1560070494437" src="../imgs/1560070494437.png" /></p>
<h3 id="word2vec-objective-function">Word2vec objective function<a class="headerlink" href="#word2vec-objective-function" title="Permanent link">&para;</a></h3>
<p>对于每个位置<span><span class="MathJax_Preview">t=1, \ldots, T</span><script type="math/tex">t=1, \ldots, T</script></span> ，在大小为<span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>的固定窗口内预测上下文单词，给定中心词 <span><span class="MathJax_Preview">w_j</span><script type="math/tex">w_j</script></span></p>
<div>
<div class="MathJax_Preview">
Likelihoood = L(\theta) = \prod^{T}_{t=1} \prod_{-m \leq j \leq m \atop j \neq 0} P(w_{t+j} | w_{t} ; \theta)
</div>
<script type="math/tex; mode=display">
Likelihoood = L(\theta) = \prod^{T}_{t=1} \prod_{-m \leq j \leq m \atop j \neq 0} P(w_{t+j} | w_{t} ; \theta)
</script>
</div>
<ul>
<li>其中，<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 为所有需要优化的变量</li>
</ul>
<p>目标函数<span><span class="MathJax_Preview">J(\theta)</span><script type="math/tex">J(\theta)</script></span> (有时被称为代价函数或损失函数) 是(平均)负对数似然</p>
<div>
<div class="MathJax_Preview">
J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right)
</div>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{-m \leq j \leq m \atop j \neq 0} \log P\left(w_{t+j} | w_{t} ; \theta\right)
</script>
</div>
<p>其中log形式是方便将连乘转化为求和，负号是希望将极大化似然率转化为极小化损失函数的等价问题。</p>
<blockquote>
<p>在连乘之前使用log转化为求和非常有效，特别是在做优化时
  $$
  \log \prod_i x_i = \sum_i \log x_i
  $$</p>
</blockquote>
<ul>
<li><strong>最小化目标函数 <span><span class="MathJax_Preview">\Leftrightarrow</span><script type="math/tex">\Leftrightarrow</script></span>  最大化预测精度</strong></li>
<li><u>问题</u>：如何计算 <span><span class="MathJax_Preview">P(w_{t+j} | w_{t} ; \theta)</span><script type="math/tex">P(w_{t+j} | w_{t} ; \theta)</script></span> ？</li>
<li><u>回答</u>：对于每个单词都是用两个向量<ul>
<li><span><span class="MathJax_Preview">v_w</span><script type="math/tex">v_w</script></span> 当 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 是中心词时</li>
<li><span><span class="MathJax_Preview">u_w</span><script type="math/tex">u_w</script></span> 当 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 是上下文词时</li>
</ul>
</li>
<li>于是对于一个中心词 <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> 和一个上下文词  <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span> </li>
</ul>
<div>
<div class="MathJax_Preview">
P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
</div>
<script type="math/tex; mode=display">
P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
</script>
</div>
<blockquote>
<p>公式中，向量 <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span> 和向量 <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span> 进行点乘。向量之间越相似，点乘结果越大，从而归一化后得到的概率值也越大。模型的训练正是为了使得具有相似上下文的单词，具有相似的向量。
</p>
</blockquote>
<h3 id="word2vec-prediction-function">Word2vec prediction function<a class="headerlink" href="#word2vec-prediction-function" title="Permanent link">&para;</a></h3>
<div>
<div class="MathJax_Preview">
P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
</div>
<script type="math/tex; mode=display">
P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)}
</script>
</div>
<ul>
<li>取幂使任何数都为正</li>
<li>点积比较o和c的相似性 <span><span class="MathJax_Preview">u^{T} v=u . v=\sum_{i=1}^{n} u_{i} v_{i}</span><script type="math/tex">u^{T} v=u . v=\sum_{i=1}^{n} u_{i} v_{i}</script></span> ，点积越大则概率越大</li>
<li>分母：对整个词汇表进行标准化，从而给出概率分布</li>
</ul>
<p><strong>softmax function</strong> <span><span class="MathJax_Preview">\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}</span><script type="math/tex">\mathbb{R}^{n} \rightarrow \mathbb{R}^{n}</script></span>
$$
\operatorname{softmax}\left(x_{i}\right)=\frac{\exp \left(x_{i}\right)}{\sum_{j=1}^{n} \exp \left(x_{j}\right)}=p_{i}
$$
将任意值 <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 映射到概率分布 <span><span class="MathJax_Preview">p_i</span><script type="math/tex">p_i</script></span></p>
<ul>
<li><strong>max</strong> ：因为放大了最大的概率</li>
<li><strong>soft</strong> ：因为仍然为较小的 <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 赋予了一定概率</li>
<li>深度学习中常用</li>
</ul>
<p>首先我们随机初始化 <span><span class="MathJax_Preview">u_{w}\in\mathbb{R}^d</span><script type="math/tex">u_{w}\in\mathbb{R}^d</script></span> 和 <span><span class="MathJax_Preview">v_{w}\in\mathbb{R}^d</span><script type="math/tex">v_{w}\in\mathbb{R}^d</script></span> ，而后使用梯度下降法进行更新</p>
<div>
<div class="MathJax_Preview">
\begin{align}
\frac{\partial}{\partial v_c}\log P(o|c)
&amp;=\frac{\partial}{\partial v_c}\log \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=\frac{\partial}{\partial v_c}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=\frac{\partial}{\partial v_c}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial v_c}\log P(o|c)
&=\frac{\partial}{\partial v_c}\log \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&=\frac{\partial}{\partial v_c}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&=\frac{\partial}{\partial v_c}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}
\end{align}
</script>
</div>
<blockquote>
<p>偏导数可以移进求和中，对应上方公式的最后两行的推导
  $$
  \frac{\partial}{\partial x}\sum_iy_i = \sum_i\frac{\partial}{\partial x}y_i
  $$
</p>
</blockquote>
<p>我们可以对上述结果重新排列如下，第一项是真正的上下文单词，第二项是预测的上下文单词。使用梯度下降法，模型的预测上下文将逐步接近真正的上下文。</p>
<div>
<div class="MathJax_Preview">
\begin{align}
\frac{\partial}{\partial v_c}\log P(o|c)
&amp;=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=u_o-\sum_{w\in V}\frac{\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}u_w\\
&amp;=u_o-\sum_{w\in V}P(w|c)u_w
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial v_c}\log P(o|c)
&=u_o-\frac{\sum_{w\in V}\exp(u_w^Tv_c)u_w}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&=u_o-\sum_{w\in V}\frac{\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}u_w\\
&=u_o-\sum_{w\in V}P(w|c)u_w
\end{align}
</script>
</div>
<p>再对 <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span> 进行偏微分计算，注意这里的 <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span> 是 <span><span class="MathJax_Preview">u_{w=o}</span><script type="math/tex">u_{w=o}</script></span> 的简写，故可知 <span><span class="MathJax_Preview">\frac{\partial}{\partial u_o}\sum_{w \in V } u_w^T v_c = \frac{\partial}{\partial u_o} u_o^T v_c = \frac{\partial u_o}{\partial u_o}v_c + \frac{\partial v_c}{\partial u_o}u_o= v_c</span><script type="math/tex">\frac{\partial}{\partial u_o}\sum_{w \in V } u_w^T v_c = \frac{\partial}{\partial u_o} u_o^T v_c = \frac{\partial u_o}{\partial u_o}v_c + \frac{\partial v_c}{\partial u_o}u_o= v_c</script></span> </p>
<div>
<div class="MathJax_Preview">
\begin{align}
\frac{\partial}{\partial u_o}\log P(o|c)
&amp;=\frac{\partial}{\partial u_o}\log \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=\frac{\partial}{\partial u_o}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=\frac{\partial}{\partial u_o}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&amp;=v_c-\frac{\sum\frac{\partial}{\partial u_o}\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=v_c - \frac{\exp(u_o^Tv_c)v_c}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&amp;=v_c - \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}v_c\\
&amp;=v_c - P(o|c)v_c\\
&amp;=(1-P(o|c))v_c
\end{align}
</div>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial u_o}\log P(o|c)
&=\frac{\partial}{\partial u_o}\log \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&=\frac{\partial}{\partial u_o}\left(\log \exp(u_o^Tv_c)-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&=\frac{\partial}{\partial u_o}\left(u_o^Tv_c-\log{\sum_{w\in V}\exp(u_w^Tv_c)}\right)\\
&=v_c-\frac{\sum\frac{\partial}{\partial u_o}\exp(u_w^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&=v_c - \frac{\exp(u_o^Tv_c)v_c}{\sum_{w\in V}\exp(u_w^Tv_c)}\\
&=v_c - \frac{\exp(u_o^Tv_c)}{\sum_{w\in V}\exp(u_w^Tv_c)}v_c\\
&=v_c - P(o|c)v_c\\
&=(1-P(o|c))v_c
\end{align}
</script>
</div>
<p>可以理解，当 <span><span class="MathJax_Preview">P(o|c) \to 1</span><script type="math/tex">P(o|c) \to 1</script></span> ，即通过中心词 <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> 我们可以正确预测上下文词 <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span> ，此时我们不需要调整 <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span> ，反之，则相应调整 <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span> 。</p>
<p>关于此处的微积分知识，可以通过<a href="https://nndl.github.io/">《神经网络与深度学习》</a>中的 <strong>附录B</strong> 了解。</p>
<h2 id="notes-01-introduction-svd-and-word2vec">Notes 01  Introduction, SVD and Word2Vec<a class="headerlink" href="#notes-01-introduction-svd-and-word2vec" title="Permanent link">&para;</a></h2>
<p><strong>Keyphrases</strong> : Natural Language Processing. Word Vectors. Singular Value Decomposition. Skip-gram. Continuous Bag of Words(CBOW). Negative Sampling. Hierarchical Softmax. Word2Vec.</p>
<p><strong>概述</strong>：这组笔记首先介绍了自然语言处理(NLP)的概念及其面临的问题。然后我们继续讨论将单词表示为数字向量的概念。最后，讨论了常用的词向量设计方法。</p>
<h3 id="introduction-to-natural-language-processing">Introduction to Natural Language Processing<a class="headerlink" href="#introduction-to-natural-language-processing" title="Permanent link">&para;</a></h3>
<p><strong>What is so special about NLP?</strong></p>
<p>Natural language is a discrete/symbolic/categorical system </p>
<blockquote>
<p>离散的/符号的/分类的</p>
</blockquote>
<p>人类的语言有什么特别之处？人类语言是一个专门用来表达意义的系统，而不是由任何形式的物理表现产生的。在这方面上，它与视觉或任何其他机器学习任务都有很大的不同。</p>
<p>大多数单词只是一个语言学以外的的符号：单词是一个映射到所指(signified 想法或事物)的能指(signifier)。</p>
<p>例如，“rocket”一词指的是火箭的概念，因此可以引申为火箭的实例。当我们使用单词和字母来表达符号时，也会有一些例外，例如“whoompaa”的使用。最重要的是，这些语言的符号可以被 编码成几种形式：声音、手势、文字等等，然后通过连续的信号传输给大脑，大脑本身似乎也能以一种连续的方式对这些信号进行解码。人们在语言哲学和语言学方面做了大量的工作来概念化人类语言，并将词语与其参照、意义等区分开来。</p>
<p><strong>Examples of tasks</strong></p>
<p>自然语言处理有不同层次的任务，从语言处理到语义解释再到语篇处理。自然语言处理的目标是通过设计算法使得计算机能够“理解”语言，从而能够执行某些特定的任务。不同的任务的难度是不一样的</p>
<p><strong>Easy</strong></p>
<ul>
<li>拼写检查  Spell Checking</li>
<li>关键词检索 Keyword Search</li>
<li>同义词查找  Finding Synonyms</li>
</ul>
<p><strong>Medium</strong></p>
<ul>
<li>解析来自网站、文档等的信息</li>
</ul>
<p><strong>Hard</strong></p>
<ul>
<li>机器翻译  Machine Translation</li>
<li>语义分析  Semantic Analysis</li>
<li>指代消解  Coreference</li>
<li>问答系统  Question Answering</li>
</ul>
<p><strong>How to represent words?</strong></p>
<p>在所有的NLP任务中，第一个也是可以说是最重要的共同点是我们如何将单词表示为任何模型的输入。在这里我们不会讨论早期的自然语言处理工作是将单词视为原子符号 atomic symbols。为了让大多数的自然语言处理任务能有更好的表现，我们首先需要了解单词之间的相似和不同。有了词向量，我们可以很容易地将其编码到向量本身中。</p>
<h3 id="word-vectors">Word Vectors<a class="headerlink" href="#word-vectors" title="Permanent link">&para;</a></h3>
<p>使用词向量编码单词，N维空间足够我们编码语言的所有语义，每一维度都会编码一些我们使用语言传递的信息。简单的one-hot向量无法给出单词间的相似性，我们需要将维度  <span><span class="MathJax_Preview">|V|</span><script type="math/tex">|V|</script></span>  减少至一个低纬度的子空间，来获得稠密的词向量，获得词之间的关系。</p>
<h3 id="svd-based-methods">SVD Based Methods<a class="headerlink" href="#svd-based-methods" title="Permanent link">&para;</a></h3>
<p>这是一类找到词嵌入的方法（即词向量），我们首先遍历一个很大的数据集和统计词的共现计数矩阵 X，然后对矩阵 X 进行 SVD 分解得到 <span><span class="MathJax_Preview">USV^{T}</span><script type="math/tex">USV^{T}</script></span> 。然后我们使用 U 的行来作为字典中所有词的词向量。我们来讨论一下矩阵 X 的几种选择。</p>
<p><strong>Word-Document Matrix</strong></p>
<p>我们最初的尝试，我们猜想相关连的单词在同一个文档中会经常出现。例如，“banks” “bonds” “stocks” “moneys”等等，出现在一起的概率会比较高。但是“banks” “octopus” “banana” “hockey”不大可能会连续地出现。我们根据这个情况来建立一个 <strong><em>Word-Document</em></strong> 矩阵，<span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 是按照以下方式构建：遍历数亿的文档和当词 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 出现在文档 <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span>，我们对 <span><span class="MathJax_Preview">X_{ij}</span><script type="math/tex">X_{ij}</script></span> 加一。这显然是一个很大的矩阵 <span><span class="MathJax_Preview">\mathbb{R}^{|V|\times M}</span><script type="math/tex">\mathbb{R}^{|V|\times M}</script></span>，它的规模是和文档数量 <span><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 成正比关系。因此我们可以尝试更好的方法。</p>
<p><strong>Window based Co-occurrence Matrix</strong></p>
<p>同样的逻辑也适用于这里，但是矩阵 <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 存储单词的共现，从而成为一个关联矩阵。在此方法中，我们计算每个单词在特定大小的窗口中出现的次数。我们按照这个方法对语料库中的所有单词进行统计。</p>
<ul>
<li>生成维度为 <span><span class="MathJax_Preview">|V| \times|V|</span><script type="math/tex">|V| \times|V|</script></span> 的共现矩阵<span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
<li>在 <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 上应用 <strong>SVD</strong> 从而得到 <span><span class="MathJax_Preview">X = {USV}^T</span><script type="math/tex">X = {USV}^T</script></span> </li>
<li>选择 <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> 前 <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> 行 得到 <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> 维的词向量</li>
<li><span><span class="MathJax_Preview">\frac{\sum_{i=1}^{k} \sigma_{i}}{\sum_{i=1}^{|V|} \sigma_{i}}</span><script type="math/tex">\frac{\sum_{i=1}^{k} \sigma_{i}}{\sum_{i=1}^{|V|} \sigma_{i}}</script></span> 表示第一个k维捕获的方差量</li>
</ul>
<p><strong>Applying SVD to the cooccurrence matrix</strong></p>
<p>我们对矩阵 <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 使用 SVD，观察奇异值（矩阵 S 上对角线上元素），根据期望的捕获方差百分比截断，留下前 k 个元素：</p>
<p>然后取子矩阵 <span><span class="MathJax_Preview">U_{1:|V|, 1:k}</span><script type="math/tex">U_{1:|V|, 1:k}</script></span> 作为词嵌入矩阵。这就给出了词汇表中每个词的 k 维表示</p>
<p>对矩阵 <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 使用SVD</p>
<p><img alt="1560082218689" src="../imgs/1560082218689.png" /></p>
<p>通过选择前 k 个奇异向量来降低维度</p>
<p><img alt="1560082288451" src="../imgs/1560082288451.png" /></p>
<p>这两种方法都给我们提供了足够的词向量来编码语义和句法(part of speech)信息，但伴随许多其他问题</p>
<ul>
<li>矩阵的维度会经常发生改变（经常增加新的单词和语料库的大小会改变）。</li>
<li>矩阵会非常的稀疏，因为很多词不会共现。</li>
<li>矩阵维度一般会非常高 <span><span class="MathJax_Preview">\approx 10^{6}\times 10^{6}</span><script type="math/tex">\approx 10^{6}\times 10^{6}</script></span></li>
<li>基于 SVD 的方法的计算复杂度很高 ( <span><span class="MathJax_Preview">m×n</span><script type="math/tex">m×n</script></span> 矩阵的计算成本是 <span><span class="MathJax_Preview">O({mn}^2)</span><script type="math/tex">O({mn}^2)</script></span> )，并且很难合并新单词或文档</li>
<li>需要在 X 上加入一些技巧处理来解决词频的极剧的不平衡</li>
</ul>
<p>然而，基于计数的方法可以有效地利用统计量</p>
<p>对上述讨论中存在的问题存在以下的解决方法：</p>
<ul>
<li>忽略功能词，例如 “the”，“he”，“has” 等等。</li>
<li>使用 ramp window，即根据文档中单词之间的距离对共现计数进行加权</li>
<li>使用皮尔逊相关系数并将负计数设置为0，而不是只使用原始计数</li>
</ul>
<p>正如我们在下一节中看到的，基于迭代的方法以一种优雅得多的方式解决了大部分上述问题。</p>
<h3 id="iteration-based-methods-word2vec">Iteration Based Methods - Word2vec<a class="headerlink" href="#iteration-based-methods-word2vec" title="Permanent link">&para;</a></h3>
<p>这里我们尝试一个新的方法。我们可以尝试创建一个模型，该模型能够一次学习一个迭代，并最终能够对给定上下文的单词的概率进行编码，而不是计算和存储一些大型数据集(可能是数十亿个句子)的全局信息。</p>
<p>这个想法是设计一个模型，该模型的参数就是词向量。然后根据一个目标函数训练模型，在每次模型的迭代计算误差，并遵循一些更新规则，该规则具有惩罚造成错误的模型参数的作用，从而可以学习到词向量。这个方法可以追溯到 1986年，我们称这个方法为“反向传播”，模型和任务越简单，训练它的速度就越快。</p>
<ul>
<li>基于迭代的方法一次捕获一个单词的共现情况，而不是像SVD方法那样直接捕获所有的共现计数。</li>
</ul>
<p>已经很多人按照这个思路测试了不同的方法。[Collobert et al., 2011] 设计的模型首先将每个单词转换为向量。对每个特定的任务（命名实体识别、词性标注等等），他们不仅训练模型的参数，同时也训练单词向量，计算出了非常好的词向量的同时取得了很好的性能。</p>
<p>在这里，我们介绍一个非常有效的概率模型：Word2vec。Word2vec 是一个软件包实际上包含：</p>
<ul>
<li><strong>两个算法</strong>：continuous bag-of-words（CBOW）和 skip-gram。CBOW 是根据中心词周围的上下文单词来预测该词的词向量。skip-gram 则相反，是根据中心词预测周围上下文的词的概率分布。</li>
<li><strong>两个训练方法</strong>：negative sampling 和 hierarchical softmax。Negative sampling 通过抽取负样本来定义目标，hierarchical softmax 通过使用一个有效的树结构来计算所有词的概率来定义目标。</li>
</ul>
<p><strong>Language Models (Unigrams, Bigrams, etc.)</strong></p>
<p>首先，我们需要创建一个模型来为一系列的单词分配概率。我们从一个例子开始：</p>
<p>“The cat jumped over the puddle”</p>
<p>一个好的语言模型会给这个句子很高的概率，因为在句法和语义上这是一个完全有效的句子。相似地，句子“stock boil fish is toy”会得到一个很低的概率，因为这是一个无意义的句子。在数学上，我们可以称为对给定 n 个词的序列的概率是：
$$
P(w_{1}, w_{2}, \ldots, w_{n})
$$
我们可以采用一元语言模型方法(<strong>Unigram model</strong>)，假设单词的出现是完全独立的，从而分解概率
$$
P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=1}^{n} P\left(w_{i}\right)
$$
但是我们知道这是不大合理的，因为下一个单词是高度依赖于前面的单词序列的。如果使用上述的语言模型，可能会让一个无意义的句子具有很高的概率。所以我们让序列的概率取决于序列中的单词和其旁边的单词的成对概率。我们称之为 bigram 模型：
$$
P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=2}^{n} P\left(w_{i} | w_{i-1}\right)
$$
但是，这个方法还是有点简单，因为我们只关心一对邻近的单词，而不是针对整个句子来考虑。但是我们将看到，这个方法会有显著的提升。考虑在词-词共现矩阵中，共现窗口为 1，我们基本上能得到这样的成对的概率。但是，这又需要计算和存储大量数据集的全局信息。</p>
<p>既然我们已经理解了如何考虑具有概率的单词序列，那么让我们观察一些能够学习这些概率的示例模型。</p>
<p><strong>Continuous Bag of Words Model (CBOW)</strong></p>
<p>这一方法是把 {"The","cat","over","the","puddle"} 作为上下文，希望从这些词中能够预测或者生成中心词“jumped”。这样的模型我们称之为 continuous bag-of-words（CBOW）模型。</p>
<p>它是从上下文中预测中心词的方法，在这个模型中的每个单词，我们希望学习两个向量</p>
<ul>
<li><span><span class="MathJax_Preview">v</span><script type="math/tex">v</script></span> (输入向量) 当词在上下文中</li>
<li><span><span class="MathJax_Preview">u</span><script type="math/tex">u</script></span> (输出向量) 当词是中心词</li>
</ul>
<p>首先我们设定已知参数。令我们模型的已知参数是 one-hot 形式的词向量表示。输入的 one-hot 向量或者上下文我们用 <span><span class="MathJax_Preview">x^{(c)}</span><script type="math/tex">x^{(c)}</script></span> 表示，输出用 <span><span class="MathJax_Preview">y^{(c)}</span><script type="math/tex">y^{(c)}</script></span> 表示。在 CBOW 模型中，因为我们只有一个输出，因此我们把 y 称为是已知中心词的的 one-hot 向量。现在让我们定义模型的未知参数。</p>
<p>首先我们对 CBOW 模型作出以下定义</p>
<ul>
<li><span><span class="MathJax_Preview">w_{i}</span><script type="math/tex">w_{i}</script></span> ：词汇表 <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> 中的单词 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span></li>
<li><span><span class="MathJax_Preview">\mathcal{V}\in \mathbb{R}^{n\times |V|}</span><script type="math/tex">\mathcal{V}\in \mathbb{R}^{n\times |V|}</script></span> ：输入词矩阵</li>
<li><span><span class="MathJax_Preview">v_{i} ： \mathcal{V}</span><script type="math/tex">v_{i} ： \mathcal{V}</script></span> 的第 i 列，单词 <span><span class="MathJax_Preview">w_{i}</span><script type="math/tex">w_{i}</script></span> 的输入向量表示</li>
<li>
<p><span><span class="MathJax_Preview">\mathcal{U}\in \mathbb{R}^{|V|\times n}</span><script type="math/tex">\mathcal{U}\in \mathbb{R}^{|V|\times n}</script></span> ：输出词矩阵</p>
</li>
<li><span><span class="MathJax_Preview">u_{i} ： \mathcal{U}</span><script type="math/tex">u_{i} ： \mathcal{U}</script></span> 的第 i 行，单词 <span><span class="MathJax_Preview">w_{i}</span><script type="math/tex">w_{i}</script></span> 的输出向量表示</li>
</ul>
<p>我们创建两个矩阵，<span><span class="MathJax_Preview">\mathcal{V}\in \mathbb{R}^{n\times |V|}</span><script type="math/tex">\mathcal{V}\in \mathbb{R}^{n\times |V|}</script></span> 和 <span><span class="MathJax_Preview">\mathcal{U}\in \mathbb{R}^{|V|\times n}</span><script type="math/tex">\mathcal{U}\in \mathbb{R}^{|V|\times n}</script></span> 。其中 n 是嵌入空间的任意维度大小。 <span><span class="MathJax_Preview">\mathcal{V}</span><script type="math/tex">\mathcal{V}</script></span> 是输入词矩阵，使得当其为模型的输入时，<span><span class="MathJax_Preview">\mathcal{V}</span><script type="math/tex">\mathcal{V}</script></span> 的第 i 列是词 <span><span class="MathJax_Preview">w_{i}</span><script type="math/tex">w_{i}</script></span> 的 n 维嵌入向量。我们定义这个 <span><span class="MathJax_Preview">n \times 1</span><script type="math/tex">n \times 1</script></span> 的向量为 <span><span class="MathJax_Preview">v_{i}</span><script type="math/tex">v_{i}</script></span> 。相似地， <span><span class="MathJax_Preview">\mathcal{U}</span><script type="math/tex">\mathcal{U}</script></span> 是输出词矩阵。当其为模型的输入时， <span><span class="MathJax_Preview">\mathcal{U}</span><script type="math/tex">\mathcal{U}</script></span> 的第 j 行是词 <span><span class="MathJax_Preview">w_{j}</span><script type="math/tex">w_{j}</script></span> 的 n 维嵌入向量。我们定义 <span><span class="MathJax_Preview">\mathcal{U}</span><script type="math/tex">\mathcal{U}</script></span> 的这行为 <span><span class="MathJax_Preview">u_{j}</span><script type="math/tex">u_{j}</script></span> 。注意实际上对每个词 <span><span class="MathJax_Preview">w_{i}</span><script type="math/tex">w_{i}</script></span> 我们需要学习两个词向量（即输入词向量 <span><span class="MathJax_Preview">v_{i}</span><script type="math/tex">v_{i}</script></span> 和输出词向量 <span><span class="MathJax_Preview">u_{i}</span><script type="math/tex">u_{i}</script></span> ）。</p>
<p>我们将这个模型分解为以下步骤</p>
<ul>
<li>我们为大小为 m 的输入上下文，生成 one-hot 词向量 <span><span class="MathJax_Preview"><span><span class="MathJax_Preview">(x^{(c-m)},...,x^{(c-1)},x^{(c+1)},...,x^{(c+m)}\in \mathbb{R}^{|V|})</span><script type="math/tex">(x^{(c-m)},...,x^{(c-1)},x^{(c+1)},...,x^{(c+m)}\in \mathbb{R}^{|V|})</script></span></span><script type="math/tex"><span><span class="MathJax_Preview">(x^{(c-m)},...,x^{(c-1)},x^{(c+1)},...,x^{(c+m)}\in \mathbb{R}^{|V|})</span><script type="math/tex">(x^{(c-m)},...,x^{(c-1)},x^{(c+1)},...,x^{(c+m)}\in \mathbb{R}^{|V|})</script></span></script></span>  </li>
<li>我们从上下文  <span><span class="MathJax_Preview">(v_{c-m}=\mathcal{V}x^{(c-m)},v_{c-m+1}=\mathcal{V}x^{(c-m+1)},...,v_{c+m}=\mathcal{V}x^{(c+m)}\in \mathbb{R}^{n})</span><script type="math/tex">(v_{c-m}=\mathcal{V}x^{(c-m)},v_{c-m+1}=\mathcal{V}x^{(c-m+1)},...,v_{c+m}=\mathcal{V}x^{(c+m)}\in \mathbb{R}^{n})</script></span> 得到嵌入词向量。</li>
<li>对上述的向量求平均值 <span><span class="MathJax_Preview">\widehat{v}=\frac{v_{c-m}+v_{c-m+1+...+v_{c+m}}}{2m}\in \mathbb{R}^{n}</span><script type="math/tex">\widehat{v}=\frac{v_{c-m}+v_{c-m+1+...+v_{c+m}}}{2m}\in \mathbb{R}^{n}</script></span> 。</li>
<li>生成一个分数向量 <span><span class="MathJax_Preview">z = \mathcal{U}\widehat{v}\in \mathbb{R}^{|V|}</span><script type="math/tex">z = \mathcal{U}\widehat{v}\in \mathbb{R}^{|V|}</script></span> 。当相似向量的点积越高，就会令到相似的词更为靠近，从而获得更高的分数。将分数转换为概率 <span><span class="MathJax_Preview">\widehat{y}=softmax(z)\in \mathbb{R}^{|V|}</span><script type="math/tex">\widehat{y}=softmax(z)\in \mathbb{R}^{|V|}</script></span> 。<ul>
<li>这里 softmax 是一个常用的函数。它将一个向量转换为另外一个向量，其中转换后的向量的第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 个元素是 <span><span class="MathJax_Preview">\frac{e^{\widehat{y}_i}}{\sum_{k=1}^{|V|}e^{\widehat{y}_k}}</span><script type="math/tex">\frac{e^{\widehat{y}_i}}{\sum_{k=1}^{|V|}e^{\widehat{y}_k}}</script></span> 。因为该函数是一个指数函数，所以值一定为正数；通过除以 <span><span class="MathJax_Preview">\sum_{k=1}^{|V|}e^{\widehat{y}_k}</span><script type="math/tex">\sum_{k=1}^{|V|}e^{\widehat{y}_k}</script></span> 来归一化向量（使得 <span><span class="MathJax_Preview">\sum_{k=1}^{|V|}\widehat{y}_k=1</span><script type="math/tex">\sum_{k=1}^{|V|}\widehat{y}_k=1</script></span>）得到概率。</li>
</ul>
</li>
<li>我们希望生成的概率 <span><span class="MathJax_Preview">\widehat{y} \in \mathbb{R}^{|V|}</span><script type="math/tex">\widehat{y} \in \mathbb{R}^{|V|}</script></span> 与实际的概率 <span><span class="MathJax_Preview">y \in \mathbb{R}^{|V|}</span><script type="math/tex">y \in \mathbb{R}^{|V|}</script></span> 匹配。使得其刚好是实际的词就是这个 one-hot 向量。</li>
</ul>
<p>下图是 CBOW 模型的计算图示</p>
<p><img alt="1560088203499" src="../imgs/1560088203499.png" /></p>
<p>如果有 <span><span class="MathJax_Preview">\mathcal{V}</span><script type="math/tex">\mathcal{V}</script></span> 和 <span><span class="MathJax_Preview">\mathcal{U}</span><script type="math/tex">\mathcal{U}</script></span> ，我们知道这个模型是如何工作的，那我们如何学习这两个矩阵呢？这需要创建一个目标函数。一般我们想从一些真实的概率中学习一个概率，信息论提供了一个 <strong>度量两个概率分布的距离</strong> 的方法。这里我们采用一个常见的距离/损失方法，交叉熵 <span><span class="MathJax_Preview">H(\widehat{y}, y)</span><script type="math/tex">H(\widehat{y}, y)</script></span> 。</p>
<p>在离散情况下使用交叉熵可以直观地得出损失函数的公式</p>
<p>$$
H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_{j} \log \left(\hat{y}_{j}\right)
$$
上面的公式中，y 是 one-hot 向量。因此上面的损失函数可以简化为：</p>
<p>$$
H(\widehat{y}, y)= - y_{j}\,log(\widehat{y}_{j})
$$
c 是正确词的 one-hot 向量的索引。我们现在可以考虑我们的预测是完美并且 <span><span class="MathJax_Preview">\widehat{y}_{c}=1</span><script type="math/tex">\widehat{y}_{c}=1</script></span> 的情况。然后我们可以计算 <span><span class="MathJax_Preview">H(\widehat{y}, y)=-1\,log(1)=0</span><script type="math/tex">H(\widehat{y}, y)=-1\,log(1)=0</script></span> 。因此，对一个完美的预测，我们不会面临任何惩罚或者损失。现在我们考虑一个相反的情况，预测非常差并且 <span><span class="MathJax_Preview">\widehat{y}_{c}=0.01</span><script type="math/tex">\widehat{y}_{c}=0.01</script></span> 。和前面类似，我们可以计算损失 <span><span class="MathJax_Preview">H(\widehat{y}, y)=-1\,log(0.01)=4.605</span><script type="math/tex">H(\widehat{y}, y)=-1\,log(0.01)=4.605</script></span> 。因此，我们可以看到，对于概率分布，交叉熵为我们提供了一个很好的距离度量。因此我们的优化目标函数公式为：</p>
<p>$$
\begin{aligned} \text { minimize } J &amp;=-\log P\left(w_{c} | w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \ &amp;=-\log P\left(u_{c} | \hat{v}\right) \ &amp;=-\log \frac{\exp \left(u_{c}^{T} \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right)} \ &amp;=-u_{c}^{T} \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_{j}^{T} \hat{v}\right) \end{aligned}
$$
我们使用 SGD 来更新所有相关的词向量 <span><span class="MathJax_Preview">u_{c}</span><script type="math/tex">u_{c}</script></span> 和 <span><span class="MathJax_Preview">v_{j}</span><script type="math/tex">v_{j}</script></span> 。SGD 对一个窗口计算梯度和更新参数：</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}{\mathcal{U}_{\text {new}} \leftarrow \mathcal{U}_{\text {old}}-\alpha \nabla_{\mathcal{U}} J} \\ {\mathcal{V}_{\text {old}} \leftarrow \mathcal{V}_{\text {old}}-\alpha \nabla_{\mathcal{V}} J}\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}{\mathcal{U}_{\text {new}} \leftarrow \mathcal{U}_{\text {old}}-\alpha \nabla_{\mathcal{U}} J} \\ {\mathcal{V}_{\text {old}} \leftarrow \mathcal{V}_{\text {old}}-\alpha \nabla_{\mathcal{V}} J}\end{array}
</script>
</div>
<p><strong>Skip-Gram Model</strong></p>
<p>Skip-Gram模型与CBOW大体相同，但是交换了我们的 x 和 y，即 CBOW 中的 x 现在是 y，y 现在是 x。输入的 one-hot 向量（中心词）我们表示为 x，输出向量为 <span><span class="MathJax_Preview">y^{(j)}</span><script type="math/tex">y^{(j)}</script></span> 。我们定义的 <span><span class="MathJax_Preview">\mathcal{V}</span><script type="math/tex">\mathcal{V}</script></span> 和 <span><span class="MathJax_Preview">\mathcal{U}</span><script type="math/tex">\mathcal{U}</script></span> 是和 CBOW 一样的。</p>
<ul>
<li>生成中心词的 one-hot 向量 <span><span class="MathJax_Preview">x\in \mathbb{R}^{|V|}</span><script type="math/tex">x\in \mathbb{R}^{|V|}</script></span> </li>
<li>我们对中心词 <span><span class="MathJax_Preview">v_{c}=\mathcal{V}x\in \mathbb{R}^{|V|}</span><script type="math/tex">v_{c}=\mathcal{V}x\in \mathbb{R}^{|V|}</script></span> 得到词嵌入向量</li>
<li>生成分数向量 <span><span class="MathJax_Preview">z = \mathcal{U}v_{c}</span><script type="math/tex">z = \mathcal{U}v_{c}</script></span> </li>
<li>将分数向量转化为概率， <span><span class="MathJax_Preview">\widehat{y}=softmax(z)</span><script type="math/tex">\widehat{y}=softmax(z)</script></span> 注意 <span><span class="MathJax_Preview">\widehat{y}_{c-m},...,\widehat{y}_{c-1},\widehat{y}_{c+1},...,\widehat{y}_{c+m}</span><script type="math/tex">\widehat{y}_{c-m},...,\widehat{y}_{c-1},\widehat{y}_{c+1},...,\widehat{y}_{c+m}</script></span> 是每个上下文词观察到的概率</li>
<li>我们希望我们生成的概率向量匹配真实概率 <span><span class="MathJax_Preview">y^{(c-m)},...,y^{(c-1)},y^{(c+1)},...,y^{(c+m)}</span><script type="math/tex">y^{(c-m)},...,y^{(c-1)},y^{(c+1)},...,y^{(c+m)}</script></span> ，one-hot 向量是实际的输出。</li>
</ul>
<p>下图是 Skip-Gram 模型的计算图示</p>
<p><img alt="1560089015401" src="../imgs/1560089015401.png" /></p>
<p>和 CBOW 模型一样，我们需要生成一个目标函数来评估这个模型。与 CBOW 模型的一个主要的不同是我们引用了一个朴素的贝叶斯假设来拆分概率。这是一个很强（朴素）的条件独立假设。换而言之，<strong>给定中心词，所有输出的词是完全独立的</strong>。(即公式1至2行)
$$
\begin{aligned} \text { minimize } J &amp;=-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} | w_{c}\right) \ &amp;=-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} | w_{c}\right) \ &amp;=-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} | v_{c}\right) \ &amp;=-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^{T} v_{c}\right)}{\sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right)} \ &amp;=-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^{T} v_{c}+2 m \log \sum_{k=1}^{|V|} \exp \left(u_{k}^{T} v_{c}\right) \end{aligned}
$$
通过这个目标函数，我们可以计算出与未知参数相关的梯度，并且在每次迭代中通过 SGD 来更新它们。</p>
<p>注意
$$
\begin{aligned} J &amp;=-\sum_{j=0, j \neq m}^{2 m} \log P\left(u_{c-m+j} | v_{c}\right) \ &amp;=\sum_{j=0, j \neq m}^{2 m} H\left(\hat{y}, y_{c-m+j}\right) \end{aligned}
$$
其中 <span><span class="MathJax_Preview">H(\widehat{y},y_{c-m+j})</span><script type="math/tex">H(\widehat{y},y_{c-m+j})</script></span> 是向量 <span><span class="MathJax_Preview">\widehat{y}</span><script type="math/tex">\widehat{y}</script></span> 的概率和 one-hot 向量 <span><span class="MathJax_Preview">y_{c-m+j}</span><script type="math/tex">y_{c-m+j}</script></span> 之间的交叉熵。</p>
<blockquote>
<p>只有一个概率向量 <span><span class="MathJax_Preview">\hat{y}</span><script type="math/tex">\hat{y}</script></span> 是被计算的。Skip-Gram 对每个上下文单词一视同仁：该模型计算每个单词在上下文中出现的概率，而与它到中心单词的距离无关。</p>
</blockquote>
<p><strong>Negative Sampling</strong></p>
<p>让我们再回到目标函数上。注意对 <span><span class="MathJax_Preview">|V|</span><script type="math/tex">|V|</script></span> 的求和计算量是非常大的。任何的更新或者对目标函数的评估都要花费 <span><span class="MathJax_Preview">O(|V|)</span><script type="math/tex">O(|V|)</script></span> 的时间复杂度。一个简单的想法是不去直接计算，而是去求近似值。</p>
<p>在每一个训练的时间步，我们不去遍历整个词汇表，而仅仅是抽取一些负样例。我们对噪声分布 <span><span class="MathJax_Preview">P_{n}(w)</span><script type="math/tex">P_{n}(w)</script></span>  “抽样”，这个概率是和词频的排序相匹配的。为加强对问题的表述以纳入负抽样，我们只需更新其</p>
<ul>
<li>目标函数</li>
<li>梯度</li>
<li>更新规则</li>
</ul>
<p>Mikolov 在论文《Distributed Representations of Words and Phrases and their Compositionality.》中提出了负采样。虽然负采样是基于 Skip-Gram 模型，但实际上是对一个不同的目标函数进行优化。</p>
<p>考虑一对中心词和上下文词 <span><span class="MathJax_Preview">(w,c)</span><script type="math/tex">(w,c)</script></span> 。这词对是来自训练数据集吗？我们通过 <span><span class="MathJax_Preview">P(D=1\mid w,c)</span><script type="math/tex">P(D=1\mid w,c)</script></span> 表示 <span><span class="MathJax_Preview">(w,c)</span><script type="math/tex">(w,c)</script></span> 是来自语料库。相应地， <span><span class="MathJax_Preview">P(D=0\mid w,c)</span><script type="math/tex">P(D=0\mid w,c)</script></span> 表示 <span><span class="MathJax_Preview">(w,c)</span><script type="math/tex">(w,c)</script></span> 不是来自语料库。</p>
<p>首先，我们对 <span><span class="MathJax_Preview">P(D=1\mid w,c)</span><script type="math/tex">P(D=1\mid w,c)</script></span> 用 sigmoid 函数建模：</p>
<div>
<div class="MathJax_Preview">
P(D=1 | w, c, \theta)=\sigma\left(v_{c}^{T} v_{w}\right)=\frac{1}{1+e^{\left(-v_{c}^{T} v_{w}\right)}}
</div>
<script type="math/tex; mode=display">
P(D=1 | w, c, \theta)=\sigma\left(v_{c}^{T} v_{w}\right)=\frac{1}{1+e^{\left(-v_{c}^{T} v_{w}\right)}}
</script>
</div>
<p>现在，我们建立一个新的目标函数，如果中心词和上下文词确实在语料库中，就最大化概率 <span><span class="MathJax_Preview">P(D=1\mid w,c)</span><script type="math/tex">P(D=1\mid w,c)</script></span> ，如果中心词和上下文词确实不在语料库中，就最大化概率 <span><span class="MathJax_Preview">P(D=0\mid w,c)</span><script type="math/tex">P(D=0\mid w,c)</script></span> 。我们对这两个概率采用一个简单的极大似然估计的方法（这里我们把 <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 作为模型的参数，在我们的例子是 <span><span class="MathJax_Preview">\mathcal{V}</span><script type="math/tex">\mathcal{V}</script></span> 和 <span><span class="MathJax_Preview">\mathcal{U}</span><script type="math/tex">\mathcal{U}</script></span> ）</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \theta &amp;=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \widetilde{D}} P(D=0 | w, c, \theta) \\ &amp;=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \widetilde{D}}(1-P(D=1 | w, c, \theta)) \\ &amp;=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 | w, c, \theta)+\sum_{(w, c) \in \widetilde{D}} \log (1-P(D=1 | w, c, \theta)) \\ &amp;=\arg \max _{\theta} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \widetilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right) \\ &amp;=\arg \max _{\theta} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \widetilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \theta &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \widetilde{D}} P(D=0 | w, c, \theta) \\ &=\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 | w, c, \theta) \prod_{(w, c) \in \widetilde{D}}(1-P(D=1 | w, c, \theta)) \\ &=\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 | w, c, \theta)+\sum_{(w, c) \in \widetilde{D}} \log (1-P(D=1 | w, c, \theta)) \\ &=\arg \max _{\theta} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \widetilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}\right) \\ &=\arg \max _{\theta} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}+\sum_{(w, c) \in \widetilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right) \end{aligned}
</script>
</div>
<p>注意最大化似然函数等同于最小化负对数似然：</p>
<div>
<div class="MathJax_Preview">
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in \widetilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)
</div>
<script type="math/tex; mode=display">
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_{w}^{T} v_{c}\right)}-\sum_{(w, c) \in \widetilde{D}} \log \left(\frac{1}{1+\exp \left(u_{w}^{T} v_{c}\right)}\right)
</script>
</div>
<p>注意 <span><span class="MathJax_Preview">\widetilde{D}</span><script type="math/tex">\widetilde{D}</script></span> 是“假的”或者“负的”语料。例如我们有句子类似“stock boil fish is toy”，这种无意义的句子出现时会得到一个很低的概率。我们可以从语料库中随机抽样出负样例 <span><span class="MathJax_Preview">\widetilde{D}</span><script type="math/tex">\widetilde{D}</script></span> 。</p>
<p>对于 Skip-Gram 模型，我们对给定中心词 <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> 来观察的上下文单词 <span><span class="MathJax_Preview">c-m+j</span><script type="math/tex">c-m+j</script></span> 的新目标函数为</p>
<div>
<div class="MathJax_Preview">
-\log \sigma\left(u_{c-m+j}^{T} \cdot v_{c}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot v_{c}\right)
</div>
<script type="math/tex; mode=display">
-\log \sigma\left(u_{c-m+j}^{T} \cdot v_{c}\right)-\sum_{k=1}^{K} \log \sigma\left(-\tilde{u}_{k}^{T} \cdot v_{c}\right)
</script>
</div>
<p>对 CBOW 模型，我们对给定上下文向量 <span><span class="MathJax_Preview">\widehat{v}=\frac{v_{c-m}+v_{c-m+1}+...+v_{c+m}}{2m}</span><script type="math/tex">\widehat{v}=\frac{v_{c-m}+v_{c-m+1}+...+v_{c+m}}{2m}</script></span> 来观察中心词 <span><span class="MathJax_Preview">u_{c}</span><script type="math/tex">u_{c}</script></span> 的新的目标函数为</p>
<div>
<div class="MathJax_Preview">
-log\,\sigma(u_{c}^{T}\cdot \widehat{v})-\sum_{k=1}^{K}log\,\sigma(-\widetilde{u}_{k}^{T}\cdot \widehat{v})
</div>
<script type="math/tex; mode=display">
-log\,\sigma(u_{c}^{T}\cdot \widehat{v})-\sum_{k=1}^{K}log\,\sigma(-\widetilde{u}_{k}^{T}\cdot \widehat{v})
</script>
</div>
<p>在上面的公式中，<span><span class="MathJax_Preview">\{\widetilde{u}_{k}\mid k=1...K\}</span><script type="math/tex">\{\widetilde{u}_{k}\mid k=1...K\}</script></span> 是从 <span><span class="MathJax_Preview">P_{n}(w)</span><script type="math/tex">P_{n}(w)</script></span> 中抽样。有很多关于如何得到最好近似的讨论，从实际效果看来最好的是指数为 &frac34; 的 Unigram 模型。那么为什么是 &frac34;？下面有一些例如可能让你有一些直观的了解：</p>
<div>
<div class="MathJax_Preview">
\begin{eqnarray}  is: 0.9^{3/4} &amp;=&amp; 0.92 \nonumber \\ Constitution: 0.09^{3/4}&amp;=&amp; 0.16 \nonumber \\ bombastic:0.01^{3/4}&amp;=&amp; 0.032 \nonumber \end{eqnarray}
</div>
<script type="math/tex; mode=display">
\begin{eqnarray}  is: 0.9^{3/4} &=& 0.92 \nonumber \\ Constitution: 0.09^{3/4}&=& 0.16 \nonumber \\ bombastic:0.01^{3/4}&=& 0.032 \nonumber \end{eqnarray}
</script>
</div>
<p>“Bombastic”现在被抽样的概率是之前的三倍，而“is”只比之前的才提高了一点点。</p>
<p><strong>Hierarchical Softmax</strong></p>
<p>Mikolov 在论文《Distributed Representations of Words and Phrases and their Compositionality.》中提出了 hierarchical softmax，相比普通的 softmax 这是一种更有效的替代方法。<strong>在实际中，hierarchical softmax 对低频词往往表现得更好，负采样对高频词和较低维度向量表现得更好</strong>。</p>
<p>Hierarchical softmax 使用一个二叉树来表示词表中的所有词。树中的每个叶结点都是一个单词，而且只有一条路径从根结点到叶结点。在这个模型中，没有词的输出表示。相反，图的每个节点（根节点和叶结点除外）与模型要学习的向量相关联。单词作为输出单词的概率定义为从根随机游走到单词所对应的叶的概率。计算成本变为 <span><span class="MathJax_Preview">O(log (|V|))</span><script type="math/tex">O(log (|V|))</script></span> 而不是 <span><span class="MathJax_Preview">O(|V|)</span><script type="math/tex">O(|V|)</script></span> 。</p>
<p>在这个模型中，给定一个向量 <span><span class="MathJax_Preview">w_{i}</span><script type="math/tex">w_{i}</script></span> 的下的单词 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 的概率 <span><span class="MathJax_Preview">p(w\mid w_{i})</span><script type="math/tex">p(w\mid w_{i})</script></span> ，等于从根结点开始到对应 w 的叶结点结束的随机漫步概率。这个方法最大的优势是计算概率的时间复杂度仅仅是 <span><span class="MathJax_Preview">O(log(|V|))</span><script type="math/tex">O(log(|V|))</script></span> ，对应着路径的长度。</p>
<p>下图是 Hierarchical softmax 的二叉树示意图</p>
<p><img alt="1560093818125" src="../imgs/1560093818125.png" /></p>
<p>让我们引入一些概念。令 <span><span class="MathJax_Preview">L(w)</span><script type="math/tex">L(w)</script></span> 为从根结点到叶结点 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 的路径中节点数目。例如，上图中的 <span><span class="MathJax_Preview">L(w_{2})</span><script type="math/tex">L(w_{2})</script></span> 为 3。我们定义 <span><span class="MathJax_Preview">n(w,i)</span><script type="math/tex">n(w,i)</script></span> 为与向量 <span><span class="MathJax_Preview">v_{n(w,i)}</span><script type="math/tex">v_{n(w,i)}</script></span> 相关的路径上第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 个结点。因此 <span><span class="MathJax_Preview">n(w,1)</span><script type="math/tex">n(w,1)</script></span> 是根结点，而 <span><span class="MathJax_Preview">n(w,L(w))</span><script type="math/tex">n(w,L(w))</script></span> 是 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 的父节点。现在对每个内部节点 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>，我们任意选取一个它的子节点，定义为 <span><span class="MathJax_Preview">ch(n)</span><script type="math/tex">ch(n)</script></span> （一般是左节点）。然后，我们可以计算概率为</p>
<div>
<div class="MathJax_Preview">
p\left(w | w_{i}\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^{T} v_{w_{i}}\right)
</div>
<script type="math/tex; mode=display">
p\left(w | w_{i}\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^{T} v_{w_{i}}\right)
</script>
</div>
<p>其中</p>
<div>
<div class="MathJax_Preview">
[x]=\left\{\begin{array}{ll}{1} &amp; {\text { if } x \text { is true }} \\ {-1} &amp; {\text { otherwise }}\end{array}\right.
</div>
<script type="math/tex; mode=display">
[x]=\left\{\begin{array}{ll}{1} & {\text { if } x \text { is true }} \\ {-1} & {\text { otherwise }}\end{array}\right.
</script>
</div>
<p>这个公式看起来非常复杂，让我们细细梳理一下。</p>
<p>首先，我们将根据从根节点 <span><span class="MathJax_Preview">(n(w,1))</span><script type="math/tex">(n(w,1))</script></span> 到叶节点 <span><span class="MathJax_Preview">(w)</span><script type="math/tex">(w)</script></span> 的路径的形状来计算相乘的项。如果我们假设 <span><span class="MathJax_Preview">ch(n)</span><script type="math/tex">ch(n)</script></span> 一直都是 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 的左节点，然后当路径往左时 <span><span class="MathJax_Preview">[n(w,j+1)=ch(n(w,j))]</span><script type="math/tex">[n(w,j+1)=ch(n(w,j))]</script></span> 的值返回 1，往右则返回 0。</p>
<p>此外，<span><span class="MathJax_Preview">[n(w,j+1)=ch(n(w,j))]</span><script type="math/tex">[n(w,j+1)=ch(n(w,j))]</script></span> 提供了归一化的作用。在节点 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 处，如果我们将去往左和右节点的概率相加，对于 <span><span class="MathJax_Preview">v_{n}^{T}v_{w_{i}}</span><script type="math/tex">v_{n}^{T}v_{w_{i}}</script></span> 的任何值则可以检查，</p>
<div>
<div class="MathJax_Preview">
\sigma\left(v_{n}^{T} v_{w_{i}}\right)+\sigma\left(-v_{n}^{T} v_{w_{i}}\right)=1
</div>
<script type="math/tex; mode=display">
\sigma\left(v_{n}^{T} v_{w_{i}}\right)+\sigma\left(-v_{n}^{T} v_{w_{i}}\right)=1
</script>
</div>
<p>归一化也保证了 <span><span class="MathJax_Preview">\sum_{w=1}^{|V|}P(w\mid w_{i})=1</span><script type="math/tex">\sum_{w=1}^{|V|}P(w\mid w_{i})=1</script></span> ，和在普通的 softmax 是一样的。</p>
<p>最后我们计算点积来比较输入向量 <span><span class="MathJax_Preview">v_{w_{i}}</span><script type="math/tex">v_{w_{i}}</script></span> 对每个内部节点向量 <span><span class="MathJax_Preview">v_{n(w,j)}^{T}</span><script type="math/tex">v_{n(w,j)}^{T}</script></span> 的相似度。下面我们给出一个例子。以上图中的 <span><span class="MathJax_Preview">w_{2}</span><script type="math/tex">w_{2}</script></span> 为例，从根节点要经过两次左边的边和一次右边的边才到达 <span><span class="MathJax_Preview">w_{2}</span><script type="math/tex">w_{2}</script></span> ，因此</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} p\left(w_{2} | w_{i}\right) &amp;=p\left(n\left(w_{2}, 1\right), \text {left}\right) \cdot p\left(n\left(w_{2}, 2\right), \text {left}\right) \cdot p\left(n\left(w_{2}, 3\right), \text { right }\right) \\ &amp;=\sigma\left(v_{n\left(w_{2}, 1\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(v_{n\left(w_{2}, 2\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(-v_{n\left(w_{2}, 3\right)}^{T} v_{w_{i}}\right) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} p\left(w_{2} | w_{i}\right) &=p\left(n\left(w_{2}, 1\right), \text {left}\right) \cdot p\left(n\left(w_{2}, 2\right), \text {left}\right) \cdot p\left(n\left(w_{2}, 3\right), \text { right }\right) \\ &=\sigma\left(v_{n\left(w_{2}, 1\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(v_{n\left(w_{2}, 2\right)}^{T} v_{w_{i}}\right) \cdot \sigma\left(-v_{n\left(w_{2}, 3\right)}^{T} v_{w_{i}}\right) \end{aligned}
</script>
</div>
<p>我们训练模型的目标是最小化负的对数似然 <span><span class="MathJax_Preview">-log\,P(w\mid w_{i})</span><script type="math/tex">-log\,P(w\mid w_{i})</script></span> 。不是更新每个词的输出向量，而是更新更新二叉树中从根结点到叶结点的路径上的节点的向量。</p>
<p>该方法的速度由构建二叉树的方式确定，并将词分配给叶节点。Mikolov 在论文《Distributed Representations of Words and Phrases and their Compositionality.》中使用的是哈夫曼树，在树中分配高频词到较短的路径。</p>
<h2 id="gensim-word-vectors-example">Gensim word vectors example<a class="headerlink" href="#gensim-word-vectors-example" title="Permanent link">&para;</a></h2>
<p>Gensim提供了将 <code>Glove</code> 转化为Word2Vec格式的API，并且提供了 <code>most_similar</code>,  <code>doesnt_match</code>等API。我们可以对<code>most_similar</code>进行封装，输出三元组的类比结果</p>
<div class="highlight"><pre><span></span><code><span class="n">model</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">word2vec_glove_file</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="s1">&#39;banana&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">analogy</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">y1</span><span class="p">):</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="n">y1</span><span class="p">,</span> <span class="n">x2</span><span class="p">],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="n">x1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">analogy</span><span class="p">(</span><span class="s1">&#39;japan&#39;</span><span class="p">,</span> <span class="s1">&#39;japanese&#39;</span><span class="p">,</span> <span class="s1">&#39;australia&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">doesnt_match</span><span class="p">(</span><span class="s2">&quot;breakfast cereal dinner lunch&quot;</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
</code></pre></div>
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>var disqus_config=function(){this.page.url="None",this.page.identifier="None"};!function(){var e=document,i=e.createElement("script");i.src="//https-looperxx-github-io-my-wiki.disqus.com/embed.js",i.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(i)}()</script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="">
        
          <a href="../CS224n-2019-Assignment/" title="CS224n-2019作业笔记" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                CS224n-2019作业笔记
              </div>
            </div>
          </a>
        
        
          <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" title="02 Word Vectors 2 and Word Senses" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  下一页
                </span>
                02 Word Vectors 2 and Word Senses
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 - 2020 Xiao Xu
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="https://github.com/looperXX" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
      </a>
    
      
      
      <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.c51dfa35.min.js"></script>
      <script src="../assets/javascripts/bundle.eaaa3931.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "", "search.config.separator": "[\\uff0c\\u3002]+", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.58d22e8e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
    
  </body>
</html>