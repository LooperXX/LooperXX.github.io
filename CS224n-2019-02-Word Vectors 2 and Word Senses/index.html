<!DOCTYPE html><html class=no-js lang=zh> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="Xiao Xu - Homepage" name=description><meta content="Xiao Xu" name=author><link href=https://looperxx.github.io/CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/ rel=canonical><link href=../CS224n-2019-01-Introduction%20and%20Word%20Vectors/ rel=prev><link href=../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/ rel=next><link href=../assets/images/favicon.png rel=icon><meta content="mkdocs-1.4.2, mkdocs-material-9.1.6" name=generator><title>02 Word Vectors 2 and Word Senses - The Sun Also Rises.</title><link href=../assets/stylesheets/main.ded33207.min.css rel=stylesheet><link href=../assets/stylesheets/palette.a0c5b2b5.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-CV5JZHXZY8"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-CV5JZHXZY8",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-CV5JZHXZY8",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src=../assets/javascripts/glightbox.min.js></script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#lecture-02-word-vectors-and-word-senses> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav aria-label=页眉 class="md-header__inner md-grid"> <a aria-label="The Sun Also Rises." class="md-header__button md-logo" data-md-component=logo href=.. title="The Sun Also Rises."> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> The Sun Also Rises. </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 02 Word Vectors 2 and Word Senses </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=indigo data-md-color-media data-md-color-primary=indigo data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg> </label> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media data-md-color-primary=indigo data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=搜索 autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=搜索 required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg> </label> <nav aria-label=查找 class=md-search__options> <a aria-label=分享 class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=分享> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"></path></svg> </a> <button aria-label=清空当前内容 class="md-search__icon md-icon" tabindex=-1 title=清空当前内容 type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/LooperXX/LooperXX.github.io title=前往仓库> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg> </div> <div class=md-source__repository> LooperXX/LooperXX.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav aria-label=标签 class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a class=md-tabs__link href=..> Xiao Xu @ HIT-SCIR </a> </li> <li class=md-tabs__item> <a href=../blog/ManagerTower/ class=md-tabs__link> Blogs </a> </li> <li class=md-tabs__item> <a href=../Normalization/ class=md-tabs__link> Notes </a> </li> <li class=md-tabs__item> <a href=../CS224n-2019-00-Info/ class="md-tabs__link md-tabs__link--active"> Notes on CS224n-2019 </a> </li> <li class=md-tabs__item> <a href=../MkDocs_demo/ class=md-tabs__link> Notes on MkDocs </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=导航栏 class="md-nav md-nav--primary md-nav--lifted" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="The Sun Also Rises." class="md-nav__button md-logo" data-md-component=logo href=.. title="The Sun Also Rises."> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg> </a> The Sun Also Rises. </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/LooperXX/LooperXX.github.io title=前往仓库> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg> </div> <div class=md-source__repository> LooperXX/LooperXX.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=..> Xiao Xu @ HIT-SCIR </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_2 type=checkbox> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> Blogs <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_2_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blogs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../blog/ManagerTower/ class=md-nav__link> ACL 2023 Oral Paper | ManagerTower: 自适应融合单模态专家见解的视觉语言表示学习方法 </a> </li> <li class=md-nav__item> <a href=../blog/BridgeTower/ class=md-nav__link> AAAI 2023 Oral Paper | BridgeTower: 在视觉语言表示学习中建立编码器间的桥梁 </a> </li> <li class=md-nav__item> <a href=../blog/Profile%20SLU/ class=md-nav__link> AAAI 2022 Oral Paper | Profile SLU: 基于Profile信息的口语语言理解基准 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_3 type=checkbox> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> Notes <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_3_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Normalization/ class=md-nav__link> Normalization </a> </li> <li class=md-nav__item> <a href=../Transfer%20Learning/ class=md-nav__link> Transfer Learning </a> </li> <li class=md-nav__item> <a href=../Attention/ class=md-nav__link> Attention </a> </li> <li class=md-nav__item> <a href=../Neural%20Reading%20Comprehension%20and%20beyond/ class=md-nav__link> Machine Reading Comprehension </a> </li> <li class=md-nav__item> <a href=../Notes%20on%20NCRF%2B%2B/ class=md-nav__link> NCRF++ </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_4 type=checkbox> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> Notes on CS224n-2019 <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_4_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Notes on CS224n-2019 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../CS224n-2019-00-Info/ class=md-nav__link> CS224n-2019 Introduction </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-Assignment/ class=md-nav__link> CS224n-2019 Assignment </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-01-Introduction%20and%20Word%20Vectors/ class=md-nav__link> 01 Introduction and Word Vectors </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> 02 Word Vectors 2 and Word Senses <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> 02 Word Vectors 2 and Word Senses </a> <nav aria-label=目录 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#lecture-02-word-vectors-and-word-senses> Lecture 02 Word Vectors and Word Senses </a> <nav aria-label="Lecture 02 Word Vectors and Word Senses" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#review-main-idea-of-word2vec> Review: Main idea of word2vec </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#optimization-gradient-descent> Optimization: Gradient Descent </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#but-why-not-capture-co-occurrence-counts-directly> But why not capture co-occurrence counts directly? </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#encoding-meaning-in-vector-differences> Encoding meaning in vector differences </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#how-to-evaluate-word-vectors> How to evaluate word vectors? </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#word-senses-and-word-sense-ambiguity> Word senses and word sense ambiguity </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#notes-02-glove-evaluation-and-training> Notes 02 GloVe, Evaluation and Training </a> <nav aria-label="Notes 02  GloVe, Evaluation and Training" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#1-global-vectors-for-word-representation-glove> 1 Global Vectors for Word Representation (GloVe) </a> <nav aria-label="1 Global Vectors for Word Representation (GloVe)" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#11-comparison-with-previous-methods> 1.1 Comparison with Previous Methods </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#12-co-occurrence-matrix> 1.2 Co-occurrence Matrix </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#13-least-squares-objective> 1.3 Least Squares Objective </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#14-conclusion> 1.4 Conclusion </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#2-evaluation-of-word-vectors> 2 Evaluation of Word Vectors </a> <nav aria-label="2 Evaluation of Word Vectors" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#21-intrinsic-evaluation> 2.1 Intrinsic Evaluation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#22-extrinsic-evaluation> 2.2 Extrinsic Evaluation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#23-intrinsic-evaluation-example-word-vector-analogies> 2.3 Intrinsic Evaluation Example: Word Vector Analogies </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#24-intrinsic-evaluation-tuning-example-analogy-evaluations> 2.4 Intrinsic Evaluation Tuning Example: Analogy Evaluations </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#25-intrinsic-evaluation-example-correlation-evaluation> 2.5 Intrinsic Evaluation Example: Correlation Evaluation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#26-further-reading-dealing-with-ambiguity> 2.6 Further Reading: Dealing With Ambiguity </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#3-training-for-extrinsic-tasks> 3 Training for Extrinsic Tasks </a> <nav aria-label="3 Training for Extrinsic Tasks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#31-problem-formulation> 3.1 Problem Formulation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#32-retraining-word-vectors> 3.2 Retraining Word Vectors </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#33-softmax-classification-and-regularization> 3.3 Softmax Classiﬁcation and Regularization </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#34-window-classification> 3.4 Window Classiﬁcation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#35-non-linear-classifiers> 3.5 Non-linear Classiﬁers </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#suggested-readings> Suggested Readings </a> <nav aria-label="Suggested Readings" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#glove-global-vectors-for-word-representation> GloVe Global Vectors for Word Representation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#improving-distributional-similarity-with-lessons-learned-from-word-embeddings> Improving Distributional Similarity with Lessons Learned from Word Embeddings </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#evaluation-methods-for-unsupervised-word-embeddings> Evaluation methods for unsupervised word embeddings </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#reference> Reference </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/ class=md-nav__link> 03 Word Window Classification,Neural Networks, and Matrix Calculus </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/ class=md-nav__link> 04 Backpropagation and Computation Graphs </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/ class=md-nav__link> 05 Linguistic Structure Dependency Parsing </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/ class=md-nav__link> 06 The probability of a sentence Recurrent Neural Networks and Language Models </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/ class=md-nav__link> 07 Vanishing Gradients and Fancy RNNs </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/ class=md-nav__link> 08 Machine Translation, Sequence-to-sequence and Attention </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/ class=md-nav__link> 09 Practical Tips for Final Projects </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/ class=md-nav__link> 10 Question Answering and the Default Final Project </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-11-ConvNets%20for%20NLP/ class=md-nav__link> 11 ConvNets for NLP </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/ class=md-nav__link> 12 Information from parts of words Subword Models </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/ class=md-nav__link> 13 Modeling contexts of use Contextual Representations and Pretraining </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/ class=md-nav__link> 14 Transformers and Self-Attention For Generative Models </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-15-Natural%20Language%20Generation/ class=md-nav__link> 15 Natural Language Generation </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-16-Coreference%20Resolution/ class=md-nav__link> 16 Coreference Resolution </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-17-Multitask%20Learning/ class=md-nav__link> 17 Multitask Learning </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/ class=md-nav__link> 18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/ class=md-nav__link> 19 Safety, Bias, and Fairness </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/ class=md-nav__link> 20 The Future of NLP + Deep Learning </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_5 type=checkbox> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> Notes on MkDocs <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_5_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Notes on MkDocs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MkDocs_demo/ class=md-nav__link> Demo </a> </li> <li class=md-nav__item> <a href=../Material%20Theme%20Tutorial/ class=md-nav__link> Material Theme Tutorial </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=目录 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#lecture-02-word-vectors-and-word-senses> Lecture 02 Word Vectors and Word Senses </a> <nav aria-label="Lecture 02 Word Vectors and Word Senses" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#review-main-idea-of-word2vec> Review: Main idea of word2vec </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#optimization-gradient-descent> Optimization: Gradient Descent </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#but-why-not-capture-co-occurrence-counts-directly> But why not capture co-occurrence counts directly? </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#encoding-meaning-in-vector-differences> Encoding meaning in vector differences </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#how-to-evaluate-word-vectors> How to evaluate word vectors? </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#word-senses-and-word-sense-ambiguity> Word senses and word sense ambiguity </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#notes-02-glove-evaluation-and-training> Notes 02 GloVe, Evaluation and Training </a> <nav aria-label="Notes 02  GloVe, Evaluation and Training" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#1-global-vectors-for-word-representation-glove> 1 Global Vectors for Word Representation (GloVe) </a> <nav aria-label="1 Global Vectors for Word Representation (GloVe)" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#11-comparison-with-previous-methods> 1.1 Comparison with Previous Methods </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#12-co-occurrence-matrix> 1.2 Co-occurrence Matrix </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#13-least-squares-objective> 1.3 Least Squares Objective </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#14-conclusion> 1.4 Conclusion </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#2-evaluation-of-word-vectors> 2 Evaluation of Word Vectors </a> <nav aria-label="2 Evaluation of Word Vectors" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#21-intrinsic-evaluation> 2.1 Intrinsic Evaluation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#22-extrinsic-evaluation> 2.2 Extrinsic Evaluation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#23-intrinsic-evaluation-example-word-vector-analogies> 2.3 Intrinsic Evaluation Example: Word Vector Analogies </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#24-intrinsic-evaluation-tuning-example-analogy-evaluations> 2.4 Intrinsic Evaluation Tuning Example: Analogy Evaluations </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#25-intrinsic-evaluation-example-correlation-evaluation> 2.5 Intrinsic Evaluation Example: Correlation Evaluation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#26-further-reading-dealing-with-ambiguity> 2.6 Further Reading: Dealing With Ambiguity </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#3-training-for-extrinsic-tasks> 3 Training for Extrinsic Tasks </a> <nav aria-label="3 Training for Extrinsic Tasks" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#31-problem-formulation> 3.1 Problem Formulation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#32-retraining-word-vectors> 3.2 Retraining Word Vectors </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#33-softmax-classification-and-regularization> 3.3 Softmax Classiﬁcation and Regularization </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#34-window-classification> 3.4 Window Classiﬁcation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#35-non-linear-classifiers> 3.5 Non-linear Classiﬁers </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#suggested-readings> Suggested Readings </a> <nav aria-label="Suggested Readings" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#glove-global-vectors-for-word-representation> GloVe Global Vectors for Word Representation </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#improving-distributional-similarity-with-lessons-learned-from-word-embeddings> Improving Distributional Similarity with Lessons Learned from Word Embeddings </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#evaluation-methods-for-unsupervised-word-embeddings> Evaluation methods for unsupervised word embeddings </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#reference> Reference </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>02 Word Vectors 2 and Word Senses</h1> <h2 id=lecture-02-word-vectors-and-word-senses>Lecture 02 Word Vectors and Word Senses<a class=headerlink href=#lecture-02-word-vectors-and-word-senses title="Permanent link">¶</a></h2> <p><strong>Lecture Plan</strong></p> <ul> <li>Finish looking at word vectors and word2vec</li> <li>Optimization basics</li> <li>Can we capture this essence more effectively by counting?</li> <li>The GloVe model of word vectors</li> <li>Evaluating word vectors </li> <li>Word senses</li> <li>The course</li> </ul> <p><strong>Goal</strong>: be able to read word embeddings papers by the end of class.</p> <h3 id=review-main-idea-of-word2vec>Review: Main idea of word2vec<a class=headerlink href=#review-main-idea-of-word2vec title="Permanent link">¶</a></h3> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560257599521.png><img alt=1560257599521 src=../imgs/1560257599521.png></a></p> <div class=arithmatex>\[ P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)} \]</div> <ul> <li>遍历整个语料库中的每个单词</li> <li>使用单词向量预测周围的单词</li> <li>更新向量以便更好地预测</li> </ul> <p><strong>Word2vec parameters and computations</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560257888161.png><img alt=1560257888161 src=../imgs/1560257888161.png></a></p> <blockquote> <p><span class=arithmatex>\(n \times d \cdot d \to 1 \rightarrow n \times 1 \overset{softmax} \to n \times 1\)</span> </p> </blockquote> <ul> <li>每行代表一个单词的词向量，点乘后得到的分数通过softmax映射为概率分布，并且我们得到的概率分布是对于该中心词而言的上下文中单词的概率分布，该分布于上下文所在的具体位置无关，所以在每个位置的预测都是一样的</li> <li>我们希望模型对上下文中(相当频繁)出现的所有单词给出一个合理的高概率估计</li> <li>the, and, that, of 这样的停用词，是每个单词点乘后得到的较大概率的单词<ul> <li>去掉这一部分可以使词向量效果更好</li> </ul> </li> </ul> <h3 id=optimization-gradient-descent>Optimization: Gradient Descent<a class=headerlink href=#optimization-gradient-descent title="Permanent link">¶</a></h3> <p>Gradient Descent 每次使用全部样本进行更新</p> <p>Stochastic Gradient Descent 每次只是用单个样本进行更新</p> <p>Mini-batch具有以下优点</p> <ul> <li>通过平均值，减少梯度估计的噪音</li> <li>在GPU上并行化运算，加快运算速度</li> </ul> <p><strong>Stochastic gradients with word vectors</strong></p> <p><span class=arithmatex>\(\nabla_{\theta} J_{t}(\theta)\)</span> 将会非常稀疏，所以我们可能只更新实际出现的向量</p> <p>解决方案</p> <ul> <li>需要稀疏矩阵更新操作来只更新矩阵U和V中的特定行</li> <li>需要保留单词向量的散列</li> </ul> <p>如果有数百万个单词向量，并且进行分布式计算，那么重要的是不必到处发送巨大的更新</p> <p><strong>Word2vec: More details</strong></p> <p>为什么两个向量？</p> <ul> <li>更容易优化，最后都取平均值</li> <li>可以每个单词只用一个向量</li> </ul> <p>两个模型变体</p> <ul> <li>Skip-grams (SG)<ul> <li>输入中心词并预测上下文中的单词</li> </ul> </li> <li>Continuous Bag of Words (CBOW)<ul> <li>输入上下文中的单词并预测中心词</li> </ul> </li> </ul> <p>之前一直使用naive的softmax(简单但代价很高的训练方法)，接下来使用负采样方法加快训练速率</p> <p><strong>The skip-gram model with negative sampling (HW2)</strong></p> <p>softmax中用于归一化的分母的计算代价太高 $$ P(o | c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp \left(u_{w}^{T} v_{c}\right)} $$ 我们将在作业2中实现使用 <strong>negative sampling</strong> 负采样方法的 <code>skip-gram</code> 模型</p> <ul> <li>使用一个 true pair (中心词及其上下文窗口中的词)与几个 noise pair (中心词与随机词搭配) 形成的样本，训练二元逻辑回归</li> </ul> <p>原文中的(最大化)目标函数是</p> <div class=arithmatex>\[ J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J_{t}(\theta) \]</div> <div class=arithmatex>\[ J_{t}(\theta)=\log \sigma\left(u_{o}^{T} v_{c}\right)+\sum_{i=1}^{k} \mathbb{E}_{j \sim P(w)}\left[\log \sigma\left(-u_{j}^{T} v_{c}\right)\right] \]</div> <p>本课以及作业中的目标函数是</p> <div class=arithmatex>\[ J_{\text {neg-sample}}\left(\boldsymbol{o}, \boldsymbol{v}_{c}, \boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\right) \]</div> <ul> <li>我们希望中心词与真实上下文单词的向量点积更大，中心词与随机单词的点积更小</li> <li>k是我们负采样的样本数目</li> </ul> <div class=arithmatex>\[ P(w)=U(w)^{3 / 4} / Z \]</div> <p>使用上式作为抽样的分布，<span class=arithmatex>\(U(w)\)</span> 是 unigram 分布，通过 <span class=arithmatex>\(\frac{3}{4}\)</span> 次方，相对减少常见单词的频率，增大稀有词的概率。 <span class=arithmatex>\(Z\)</span> 用于生成概率分布。</p> <h3 id=but-why-not-capture-co-occurrence-counts-directly>But why not capture co-occurrence counts directly?<a class=headerlink href=#but-why-not-capture-co-occurrence-counts-directly title="Permanent link">¶</a></h3> <p>共现矩阵 X </p> <ul> <li>两个选项：windows vs. full document</li> <li>Window ：与word2vec类似，在每个单词周围都使用Window，包括语法(POS)和语义信息</li> <li>Word-document 共现矩阵的基本假设是在同一篇文章中出现的单词更有可能相互关联。假设单词 <span class=arithmatex>\(i\)</span> 出现在文章 <span class=arithmatex>\(j\)</span> 中，则矩阵元素 <span class=arithmatex>\(X_{ij}\)</span> 加一，当我们处理完数据库中的所有文章后，就得到了矩阵 X ，其大小为 <span class=arithmatex>\(|V|\times M\)</span> ，其中 <span class=arithmatex>\(|V|\)</span> 为词汇量，而 <span class=arithmatex>\(M\)</span> 为文章数。这一构建单词文章co-occurrence matrix的方法也是经典的Latent Semantic Analysis所采用的。<span class="critic comment">潜在语义分析</span></li> </ul> <p>利用某个定长窗口中单词与单词同时出现的次数来产生window-based (word-word) co-occurrence matrix。下面以窗口长度为1来举例，假设我们的数据包含以下几个句子：</p> <ul> <li>I like deep learning.</li> <li>I like NLP.</li> <li>I enjoy flying.</li> </ul> <p>则我们可以得到如下的word-word co-occurrence matrix:</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560263567193.png><img alt=1560263567193 src=../imgs/1560263567193.png></a></p> <p>使用共现次数衡量单词的相似性，但是会随着词汇量的增加而增大矩阵的大小，并且需要很多空间来存储这一高维矩阵，后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，使得效果不佳。我们需要对这一矩阵进行降维，获得低维(25-1000)的稠密向量。</p> <p><strong>Method 1: Dimensionality Reduction on X (HW1)</strong></p> <p>使用SVD方法将共现矩阵 X 分解为 <span class=arithmatex>\(U \Sigma V^{\top}\)</span> ，<span class=arithmatex>\(\sum\)</span> 是对角线矩阵，对角线上的值是矩阵的奇异值。 <span class=arithmatex>\(U,V\)</span> 是对应于行和列的正交基。</p> <p>为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的k个值，并将矩阵 <span class=arithmatex>\(U,V\)</span> 的相应的行列保留。这是经典的线性代数算法，对于大型矩阵而言，计算代价昂贵。</p> <p><strong>Hacks to X (several used in Rohde et al. 2005)</strong></p> <p>按比例调整 counts 会很有效</p> <ul> <li>对高频词进行缩放(语法有太多的影响)<ul> <li>使用log进行缩放</li> <li><span class=arithmatex>\(min(X, t), t \approx 100\)</span></li> <li>直接全部忽视</li> </ul> </li> <li>在基于window的计数中，提高更加接近的单词的计数</li> <li>使用Pearson相关系数</li> </ul> <blockquote> <p>Conclusion：对计数进行处理是可以得到有效的词向量的</p> </blockquote> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560264868083.png><img alt=1560264868083 src=../imgs/1560264868083.png></a></p> <p><span class=arithmatex>\(drive \to driver , swim \to swimmer, teach \to teacher\)</span></p> <p>在向量中出现的有趣的句法模式：语义向量基本上是线性组件，虽然有一些摆动，但是基本是存在动词和动词实施者的方向。</p> <p>基于计数：使用整个矩阵的全局统计数据来直接估计</p> <ul> <li>优点<ul> <li>训练快速</li> <li>统计数据高效利用</li> </ul> </li> <li>缺点<ul> <li>主要用于捕捉单词相似性</li> <li>对大量数据给予比例失调的重视</li> </ul> </li> </ul> <p>转换计数：定义概率分布并试图预测单词</p> <ul> <li>优点<ul> <li>提高其他任务的性能</li> <li>能捕获除了单词相似性以外的复杂的模式</li> </ul> </li> <li>缺点<ul> <li>与语料库大小有关的量表</li> <li>统计数据的低效使用（<strong>采样是对统计数据的低效使用</strong>）</li> </ul> </li> </ul> <h3 id=encoding-meaning-in-vector-differences>Encoding meaning in vector differences<a class=headerlink href=#encoding-meaning-in-vector-differences title="Permanent link">¶</a></h3> <p>将两个流派的想法结合起来，在神经网络中使用计数矩阵</p> <blockquote> <p>关于Glove的理论分析需要阅读原文，也可以阅读<a href=https://zhuanlan.zhihu.com/p/60208480>CS224N笔记(二)：GloVe</a> </p> </blockquote> <p><strong>关键思想</strong>：共现概率的比值可以对meaning component进行编码</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560266202421.png><img alt=1560266202421 src=../imgs/1560266202421.png></a></p> <p>重点不是单一的概率大小，重点是他们之间的比值，其中蕴含着meaning component。</p> <blockquote> <p>例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，它们之间的关系可通过与不同的单词 x 的co-occurrence probability 的比值来描述。</p> <p>例如对于solid固态，虽然 <span class=arithmatex>\(P(solid|ice)\)</span> 与 <span class=arithmatex>\(P(solid|steam)\)</span> 本身很小，不能透露有效的信息，但是它们的比值 <span class=arithmatex>\(\frac{P(solid|ice)}{P(solid|steam)}\)</span> 却较大，因为solid更常用来描述ice的状态而不是steam的状态，所以在ice的上下文中出现几率较大</p> <p>对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的co-occurrence probability，实际上co-occurrence probability的相对比值更有意义</p> </blockquote> <p>我们如何在词向量空间中以线性meaning component的形式捕获共现概率的比值？</p> <p>log-bilinear 模型 : <span class=arithmatex>\(w_{i} \cdot w_{j}=\log P(i | j)\)</span></p> <p>向量差异 : <span class=arithmatex>\(w_{x} \cdot\left(w_{a}-w_{b}\right)=\log \frac{P(x | a)}{P(x | b)}\)</span></p> <ul> <li>如果使向量点积等于共现概率的对数，那么向量差异变成了共现概率的比率</li> </ul> <div class=arithmatex>\[ J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T} \tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2} \]</div> <ul> <li>使用平方误差促使点积尽可能得接近共现概率的对数</li> <li>使用 <span class=arithmatex>\(f(x)\)</span> 对常见单词进行限制</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560267029780.png><img alt=1560267029780 src=../imgs/1560267029780.png></a></p> <ul> <li>优点<ul> <li>训练快速</li> <li>可以扩展到大型语料库</li> <li>即使是小语料库和小向量，性能也很好</li> </ul> </li> </ul> <h3 id=how-to-evaluate-word-vectors>How to evaluate word vectors?<a class=headerlink href=#how-to-evaluate-word-vectors title="Permanent link">¶</a></h3> <p>与NLP的一般评估相关：内在与外在</p> <ul> <li>内在<ul> <li>对特定/中间子任务进行评估</li> <li>计算速度快</li> <li>有助于理解这个系统</li> <li>不清楚是否真的有用，除非与实际任务建立了相关性</li> </ul> </li> <li>外在<ul> <li>对真实任务的评估</li> <li>计算精确度可能需要很长时间</li> <li>不清楚子系统是问题所在，是交互问题，还是其他子系统</li> <li>如果用另一个子系统替换一个子系统可以提高精确度</li> </ul> </li> </ul> <p><strong>Intrinsic word vector evaluation</strong></p> <p>词向量类比 <code>a:b :: c:?</code></p> <div class=arithmatex>\[ d=\arg \max _{i} \frac{\left(x_{b}-x_{a}+x_{c}\right)^{T} x_{i}}{\left\|x_{b}-x_{a}+x_{c}\right\|} \]</div> <ul> <li>通过加法后的余弦距离是否能很好地捕捉到直观的语义和句法类比问题来评估单词向量</li> <li>从搜索中丢弃输入的单词</li> <li>问题:如果有信息但不是线性的怎么办？</li> </ul> <p>Glove可视化效果</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560267650486.png><img alt=1560267650486 src=../imgs/1560267650486.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560267659926.png><img alt=1560267659926 src=../imgs/1560267659926.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560267671216.png><img alt=1560267671216 src=../imgs/1560267671216.png></a></p> <blockquote> <p>可以使用数据集评估语法和语义上的效果</p> </blockquote> <p><strong>Analogy evaluation and hyperparameters</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560267825912.png><img alt=1560267825912 src=../imgs/1560267825912.png></a></p> <ul> <li>300是一个很好的词向量维度</li> <li>不对称上下文(只使用单侧的单词)不是很好，但是这在下游任务重可能不同</li> <li> <p>window size 设为 8 对 Glove向量来说比较好</p> </li> <li> <p>分析</p> <ul> <li>window size设为2的时候实际上有效的，并且对于句法分析是更好的，因为句法效果非常局部</li> </ul> </li> </ul> <p><strong>On the Dimensionality of Word Embedding</strong></p> <p>利用矩阵摄动理论，揭示了词嵌入维数选择的基本的偏差与方法的权衡</p> <p>当持续增大词向量维度的时候，词向量的效果不会一直变差并且会保持平稳</p> <p><strong>Analogy evaluation and hyperparameters</strong></p> <ul> <li>训练时间越长越好</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560268494152.png><img alt=1560268494152 src=../imgs/1560268494152.png></a></p> <ul> <li>数据集越大越好，并且维基百科数据集比新闻文本数据集要好<ul> <li>因为维基百科就是在解释概念以及他们之间的相互关联，更多的说明性文本显示了事物之间的所有联系</li> <li>而新闻并不去解释，而只是去阐述一些事件</li> </ul> </li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560268567031.png><img alt=1560268567031 src=../imgs/1560268567031.png></a></p> <p><strong>Another intrinsic word vector evaluation</strong></p> <p>使用 cosine similarity 衡量词向量之间的相似程度</p> <h3 id=word-senses-and-word-sense-ambiguity>Word senses and word sense ambiguity<a class=headerlink href=#word-senses-and-word-sense-ambiguity title="Permanent link">¶</a></h3> <p>大多数单词都是多义的</p> <ul> <li>特别是常见单词</li> <li>特别是存在已久的单词</li> </ul> <p><strong>Improving Word Representations Via Global Context</strong> <strong>And Multiple Word Prototypes (Huang et al. 2012)</strong></p> <p>将常用词的所有上下文进行聚类，通过该词得到一些清晰的簇，从而将这个常用词分解为多个单词，例如<code>bank_1, bank_2, bank_3</code></p> <p>虽然这很粗糙，并且有时sensors之间的划分也不是很明确甚至相互重叠</p> <p><strong>Linear Algebraic Structure of Word Senses, with</strong> <strong>Applications to Polysemy、</strong></p> <ul> <li>单词在标准单词嵌入(如word2vec)中的不同含义以线性叠加(加权和)的形式存在，<span class=arithmatex>\(f\)</span> 指频率</li> </ul> <div class=arithmatex>\[ v_{\text { pike }}=\alpha_{1} v_{\text { pike }_{1}}+\alpha_{2} v_{\text { pike }_{2}}+\alpha_{3} v_{\text { pike }_{3}} \\ \alpha_{1}=\frac{f_{1}}{f_{1}+f_{2}+f_{3}} \]</div> <p>令人惊讶的结果，只是加权平均值就已经可以获得很好的效果</p> <ul> <li>由于从稀疏编码中得到的概念，你实际上可以将感官分离出来(前提是它们相对比较常见)</li> <li>可以理解为由于单词存在于高维的向量空间之中，不同的纬度所包含的含义是不同的，所以加权平均值并不会损害单词在不同含义所属的纬度上存储的信息</li> </ul> <p><strong>Extrinsic word vector evaluation</strong></p> <p>单词向量的外部评估：词向量可以应用于NLP的很多任务</p> <h2 id=notes-02-glove-evaluation-and-training>Notes 02 GloVe, Evaluation and Training<a class=headerlink href=#notes-02-glove-evaluation-and-training title="Permanent link">¶</a></h2> <p><strong>Keyphrases</strong>: Global Vectors for Word Representation (GloVe). Intrinsic and extrinsic evaluations. Effect of hyperparameters on analogy evaluation tasks. Correlation of human judgment with word vector distances. Dealing with ambiguity in word using contexts. Window classification.</p> <p><strong>概述</strong>：这组笔记首先介绍了训练词向量的Glove模型。然后，它扩展了我们对词向量(或称词嵌入)的讨论，看看它们是如何通过内部和外部来评估的。我们讨论了以词类比作为一种内在评价技术的例子，以及它如何被用来调整词嵌入技术。然后我们讨论了训练模型的权重/参数和词向量的外部任务。最后，我们将人工神经网络作为一种自然语言处理任务的模型。</p> <h3 id=1-global-vectors-for-word-representation-glove>1 Global Vectors for Word Representation (GloVe)<a class=headerlink href=#1-global-vectors-for-word-representation-glove title="Permanent link">¶</a></h3> <h4 id=11-comparison-with-previous-methods>1.1 Comparison with Previous Methods<a class=headerlink href=#11-comparison-with-previous-methods title="Permanent link">¶</a></h4> <p>到目前为止，我们已经研究了两类主要的词嵌入方法。第一类是基于统计并且依赖矩阵分解（例如LSA，HAL）。虽然这类方法有效地利用了<strong>全局的信息</strong>，它们主要用于捕获单词的<strong>相似性</strong>，但是对例如单词类比的任务上表现不好。另外一类方法是基于浅层窗口（例如，Skip-Gram 和 CBOW 模型），这类模型通过在局部上下文窗口通过预测来学习词向量。这些模型除了在单词相似性任务上表现良好外，还展示了捕获<strong>复杂语言模式</strong>能力，但未能利用到全局共现统计数据。</p> <p>相比之下，GloVe 由一个加权最小二乘模型组成，基于全局word-word共现计数进行训练，从而有效地利用全局统计数据。模型生成了包含有意义的子结构的单词向量空间，在词类比任务上表现非常出色。</p> <blockquote> <p>Glove 利用全局统计量，以最小二乘为目标，预测单词 <span class=arithmatex>\(j\)</span> 出现在单词 <span class=arithmatex>\(i\)</span> 上下文中的概率。</p> </blockquote> <h4 id=12-co-occurrence-matrix>1.2 Co-occurrence Matrix<a class=headerlink href=#12-co-occurrence-matrix title="Permanent link">¶</a></h4> <p><span class=arithmatex>\(X\)</span> 表示 word-word 共现矩阵，其中 <span class=arithmatex>\(X_{ij}\)</span> 表示词 <span class=arithmatex>\(j\)</span> 出现在词 <span class=arithmatex>\(i\)</span> 的上下文的次数。令 <span class=arithmatex>\(X_{i}=\sum_{k}X_{ik}\)</span> 为任意词 <span class=arithmatex>\(k\)</span> 出现在词 <span class=arithmatex>\(i\)</span> 的上下文的次数。最后，令 <span class=arithmatex>\(P_{ij}=P(w_{j}\mid w_{i})=\frac{X_{ij}}{X_{i}}\)</span> 是词 <span class=arithmatex>\(j\)</span> 出现在词 <span class=arithmatex>\(i\)</span> 的上下文的概率。 </p> <p>计算这个矩阵需要遍历一次整个语料库获得统计信息。对庞大的语料库，这样的遍历会产生非常大的计算量，但是这只是一次性的前期投入成本。</p> <h4 id=13-least-squares-objective>1.3 Least Squares Objective<a class=headerlink href=#13-least-squares-objective title="Permanent link">¶</a></h4> <p>回想一下 Skip-Gram 模型，我们使用 softmax 来计算词 <span class=arithmatex>\(j\)</span> 出现在词 <span class=arithmatex>\(i\)</span> 的上下文的概率。</p> <div class=arithmatex>\[ Q_{ij}=\frac{exp(u_{j}^{T}v_{i})}{\sum_{w=1}^{W}exp(u_{w}^{T}v_{i})} \]</div> <p>训练时以在线随机的方式进行，但是隐含的全局交叉熵损失可以如下计算：</p> <div class=arithmatex>\[ J=-\sum_{i\in corpus} \sum_{j\in context(i)}log\;Q_{ij} \]</div> <p>同样的单词 <span class=arithmatex>\(i\)</span> 和 <span class=arithmatex>\(j\)</span> 可能在语料库中出现多次，因此首先将 <span class=arithmatex>\(i\)</span> 和 <span class=arithmatex>\(j\)</span> 相同的值组合起来更有效：</p> <div class=arithmatex>\[ J=-\sum_{i=1}^{W}\sum_{j=1}^{W}X_{ij}log\;Q_{ij} \]</div> <p>其中，共现频率的值是通过共现矩阵 <span class=arithmatex>\(X\)</span> 给定。</p> <p>交叉熵损失的一个显着缺点是要求分布 <span class=arithmatex>\(Q\)</span> 被正确归一化，因为对整个词汇的求和的计算量是非常大的。因此，我们使用一个最小二乘的目标函数，其中 <span class=arithmatex>\(P\)</span> 和 <span class=arithmatex>\(Q\)</span> 的归一化因子被丢弃了：</p> <div class=arithmatex>\[ \widehat{J}=\sum_{i=1}^{W}\sum_{j=1}^{W}X_{i}(\widehat{P}_{ij}-\widehat{Q}_{ij})^{2} \]</div> <p>其中 <span class=arithmatex>\(\hat{P}_{i j}=X_{i j} \text { and } \hat{Q}_{i j}=\exp \left(\vec{u}_{j}^{T} \vec{v}_{i}\right)\)</span> 是未归一化分布。这个公式带来了一个新的问题， <span class=arithmatex>\(X_{ij}\)</span> 经常会是很大的值，从而难以优化。一个有效的改变是最小化 <span class=arithmatex>\(\widehat{P}\)</span> 和 <span class=arithmatex>\(\widehat{Q}\)</span> 对数的平方误差：</p> <div class=arithmatex>\[ \begin{eqnarray} \widehat{J} &amp;=&amp;\sum_{i=1}^{W}\sum_{j=1}^{W}X_{i}(log(\widehat{P}_{ij})-log(\widehat{Q}_{ij}))^{2} \nonumber \\ &amp;=&amp; \sum_{i=1}^{W}\sum_{j=1}^{W}X_{i}(u_{j}^{T}v_{i}-log\; X_{ij})^{2} \nonumber \end{eqnarray} \]</div> <p>另外一个问题是权值因子 <span class=arithmatex>\(X_{i}\)</span> 不能保证是最优的。因此，我们引入更一般化的权值函数，我们可以自由地依赖于上下文单词：</p> <div class=arithmatex>\[ \widehat{J}=\sum_{i=1}^{W}\sum_{j=1}^{W}f(X_{ij})(u_{j}^{T}v_{i}-log\; X_{ij})^{2} \]</div> <h4 id=14-conclusion>1.4 Conclusion<a class=headerlink href=#14-conclusion title="Permanent link">¶</a></h4> <p>总而言之，GloVe 模型仅对单词共现矩阵中的非零元素训练，从而有效地利用全局统计信息，并生成具有有意义的子结构向量空间。给出相同的语料库，词汇，窗口大小和训练时间，它的表现都优于 word2vec，它可以更快地实现更好的效果，并且无论速度如何，都能获得最佳效果。</p> <h3 id=2-evaluation-of-word-vectors>2 Evaluation of Word Vectors<a class=headerlink href=#2-evaluation-of-word-vectors title="Permanent link">¶</a></h3> <p>到目前为止，我们已经讨论了诸如 Word2Vec 和 GloVe 来训练和发现语义空间中的自然语言词语的潜在向量表示。在这部分，我们讨论如何量化评估词向量的质量。</p> <h4 id=21-intrinsic-evaluation>2.1 Intrinsic Evaluation<a class=headerlink href=#21-intrinsic-evaluation title="Permanent link">¶</a></h4> <p>词向量的内部评估是对一组由如 Word2Vec 或 GloVe 生成的词向量在特定的中间子任务（如词类比）上的评估。这些子任务通常简单而且计算速度快，从而能够帮助我们理解生成的的词向量。内部评估通常应该返回给我们一个数值，来表示这些词向量在评估子任务上的表现。</p> <ul> <li>对特定的中间任务进行评估</li> <li>可以很快的计算性能</li> <li>帮助理解子系统</li> <li>需要和真实的任务正相关来确定有用性</li> </ul> <p>在下图中，左子系统（红色）训练的计算量大，因此更改为一个简单的子系统（绿色）作内部评估。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560309179181.png><img alt=1560309179181 src=../imgs/1560309179181.png></a></p> <p><strong>动机：</strong>我们考虑创建一个问答系统，其中使用词向量作为输入的例子。一个方法是训练一个机器学习系统：</p> <ol> <li>输入词语</li> <li>将输入词语转换为词向量</li> <li>对一个复杂的机器学习系统，使用词向量作为输入</li> <li>将输出的词向量通过系统映射到自然语言词语上。</li> <li>生成词语作为答案</li> </ol> <p>当然，在训练这样的一个问答系统的过程中，因为它们被用在下游子系统（例如深度神经网络），我们需要创建最优的词向量表示。在实际操作中，我们需要对 Word2Vec 子系统中的许多超参数进行调整（例如词向量的维度）。</p> <p>虽然最理想的方法是在 Word2Vec 子系统中的任何参数改变后都重新训练，但从工程角度来看是不实际的，因为机器学习系统（在第3步）通常是一个深层神经网络，网络中的数百万个参数需要很长的时间训练。</p> <p>在这样的情况下，我们希望能有一个简单的内部评估技术来度量词向量子系统的好坏。显然的要求是内部评价与最终任务的表现有正相关关系。</p> <h4 id=22-extrinsic-evaluation>2.2 Extrinsic Evaluation<a class=headerlink href=#22-extrinsic-evaluation title="Permanent link">¶</a></h4> <p>词向量的外部评估是对一组在实际任务中生成的词向量的评估。这些任务通常复杂而且计算速度慢。对我们上面的例子，允许对问题答案进行评估的系统是外部评估系统。通常，优化表现不佳的外部评估系统我们难以确定哪个特定子系统存在错误，这就需要进一步的内部评估。 </p> <ul> <li>对真实任务的评估</li> <li>计算性能可能很慢</li> <li>不清楚是子系统出了问题，还是其他子系统出了问题，还是内部交互出了问题</li> <li>如果替换子系统提高了性能，那么更改可能是好的</li> </ul> <h4 id=23-intrinsic-evaluation-example-word-vector-analogies>2.3 Intrinsic Evaluation Example: Word Vector Analogies<a class=headerlink href=#23-intrinsic-evaluation-example-word-vector-analogies title="Permanent link">¶</a></h4> <p>一个比较常用的内部评估的方法是词向量的类比。在词向量类比中，给定以下形式的不完整类比：</p> <div class=arithmatex>\[ a:b::c:? \]</div> <p>然后内部评估系统计算词向量的最大余弦相似度：</p> <div class=arithmatex>\[ d=arg\max_{i}\frac{(x_{b}-x_{a}+x_{c})^{T}x_{i}}{|x_{b}-x_{a}+x_{c}|} \]</div> <p>这个指标有直观的解释。理想的情况下，我们希望 <span class=arithmatex>\(x_{b}-x_{a}=x_{d}-x_{c}\)</span> （例如，queen-king=actress-actor）。这就暗含着我们希望 <span class=arithmatex>\(x_{b}-x_{a}+x_{c}=x_{d}\)</span> 。因此，我们确定可以最大化两个词向量之间的归一化点积的向量 <span class=arithmatex>\(x_{d}\)</span> 即可（即余弦相似度）。使用诸如词向量类比的内部评估技术应该小心处理（要考虑到预训练的语料库的各个方面）。例如，考虑以下的类比形式：</p> <p><span class=arithmatex>\(City\;1:State\;containing\;City\;1:\;:City\;2:State\;containing\;City\;2\)</span> </p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310091559.png><img alt=1560310091559 src=../imgs/1560310091559.png></a></p> <p>上图是可能受到具有相同名称的不同城市的语义词向量类比（内在评估）。在上面很多的例子，美国有很多同名的城市／城镇／村庄。因此，很多州都符合正确的答案。例如，在美国至少有 10 个地方的名称是 Phoenix，所以 Arizona 不是唯一的正确答案。在考虑以下的类比形式：</p> <p><span class=arithmatex>\(Capital\;City\;1:\;Country\;1:\;:Capital\;City\;2:\;Country\;2\)</span></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310315647.png><img alt=1560310315647 src=../imgs/1560310315647.png></a></p> <p>上图是可能在不同时间点有不同首都的国家的<strong>语义</strong>词向量类比（内在评估）。上面很多的例子，这个任务中生成的城市仅仅是近期的国家首都，可能会受到不同国家在不同时间点拥有不同首都的影响。例如，1997 年之前 Kazakhstan 的首都是 Almaty。因此，如果我们的语料库过时就会出现问题。</p> <p>之前的两个例子说明如何使用词向量进行语义测试。我们也可以使用词向量类似进行<strong>语法</strong>测试。下面是测试形容词最高级概念的句法词向量类比(内在评价)，如下图所示：</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310392566.png><img alt=1560310392566 src=../imgs/1560310392566.png></a></p> <p>类似地，下图的内部评估展示了测试词向量捕获过去时态概念的能力</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310409068.png><img alt=1560310409068 src=../imgs/1560310409068.png></a></p> <h4 id=24-intrinsic-evaluation-tuning-example-analogy-evaluations>2.4 Intrinsic Evaluation Tuning Example: Analogy Evaluations<a class=headerlink href=#24-intrinsic-evaluation-tuning-example-analogy-evaluations title="Permanent link">¶</a></h4> <p>我们现在探讨使用内在评估系统（如类比系统）来调整的词向量嵌入技术（如 Word2Vec 和 GloVe）中的超参数。我们首先来看看在类比评估任务中，在相同的超参数下，由不同方法创建的词向量表现效果：</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310476307.png><img alt=1560310476307 src=../imgs/1560310476307.png></a></p> <p>根据上表，我们可以看到 3 点：</p> <ul> <li> <p><strong>模型的表现高度依赖模型所使用的词向量的模型：</strong></p> <ul> <li>这点是可以预料到的，因为不同的生成词向量方法是基于不同的<strong>特性</strong>的（例如共现计数，奇异向量等等）。</li> </ul> </li> <li> <p><strong>语料库更大模型的表现更好：</strong></p> <ul> <li>这是因为模型训练的语料越大，模型的表现就会更好。例如，如果训练的时候没有包含测试的词语，那么词类比会产生错误的结果。</li> </ul> </li> <li> <p><strong>对于极高或者极低维度的词向量，模型的表现较差：</strong></p> <ul> <li>低维度的词向量不能捕获在语料库中不同词语的意义。这可以被看作是我们的模型复杂度太低的高偏差问题。例如，我们考虑单词“king”、“queen”、“man”、“woman”。直观上，我们需要使用例如“性别”和“领导”两个维度来将它们编码成 2 字节的词向量。维度较低的词向量不会捕获四个单词之间的语义差异，而过高的维度的可能捕获语料库中无助于泛化的噪声-即所谓的高方差问题。</li> </ul> </li> </ul> <blockquote> <p>GloVe 中使用中心词左右的窗口大小为 8 时模型表现较好。</p> </blockquote> <p>下图可以看到训练时间对模型表现的影响</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310743759.png><img alt=1560310743759 src=../imgs/1560310743759.png></a></p> <p>下图可以看到增加语料库规模对模型准确度的影响</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310781746.png><img alt=1560310781746 src=../imgs/1560310781746.png></a></p> <p>下图可以看到不同超参数对 GloVe 模型准确度的影响</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310803261.png><img alt=1560310803261 src=../imgs/1560310803261.png></a></p> <blockquote> <p>超高维向量：</p> <p>直观地说，这些向量似乎会在语料库中捕获不允许泛化的噪声，即导致高方差。</p> <p>但是Yin等人在On the Dimensionality of Word Embedding上表明，skip-gram 和 Glove 对这种过拟合具有鲁棒性。</p> </blockquote> <h4 id=25-intrinsic-evaluation-example-correlation-evaluation>2.5 Intrinsic Evaluation Example: Correlation Evaluation<a class=headerlink href=#25-intrinsic-evaluation-example-correlation-evaluation title="Permanent link">¶</a></h4> <p>另外一个评估词向量质量的简单方法是，让人去给两个词的相似度在一个固定的范围内（例如 0-10）评分，然后将其与对应词向量的余弦相似度进行对比。这已经在包含人为评估的各种数据集上尝试过。</p> <p>下图是使用不同的词嵌入技术与不同的人类判断数据集的词向量相似性之间的相关性</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560310931521.png><img alt=1560310931521 src=../imgs/1560310931521.png></a></p> <h4 id=26-further-reading-dealing-with-ambiguity>2.6 Further Reading: Dealing With Ambiguity<a class=headerlink href=#26-further-reading-dealing-with-ambiguity title="Permanent link">¶</a></h4> <p>我们想知道如何处理在不同的自然语言处理使用场景下，用不同的的词向量来捕获同一个单词在不同场景下的不同用法。例如，“run”是一个名词也是一个动词，在不同的语境中它的词性也会不同。论文 <strong><em>Improving Word Representations Via Global Context And Multiple Word Prototypes</em></strong> 提出上述问题的的解决方法。该方法的本质如下：</p> <ol> <li>收集所有出现的单词的固定大小的上下文窗口(例如前 5 个和后 5 个)。</li> <li>每个上下文使用上下文词向量的加权平均值来表示(使用idf加权)。</li> <li>用 <code>spherical k-means</code> 对这些上下文表示进行聚类。</li> <li>最后，每个单词的出现都重新标签为其相关联的类，同时对这个类，来训练对应的词向量。</li> </ol> <p>要对这个问题进行更严谨的处理，可以参考原文。</p> <h3 id=3-training-for-extrinsic-tasks>3 Training for Extrinsic Tasks<a class=headerlink href=#3-training-for-extrinsic-tasks title="Permanent link">¶</a></h3> <p>到目前为止，我们一直都关注于内在任务，并强调其在开发良好的词向量技术中的重要性。但是大多数实际问题的最终目标是将词向量结果用于其他的外部任务。接下来会讨论处理外部任务的方法。</p> <h4 id=31-problem-formulation>3.1 Problem Formulation<a class=headerlink href=#31-problem-formulation title="Permanent link">¶</a></h4> <p>很多 NLP 的外部任务都可以表述为分类任务。例如，给定一个句子，我们可以对这个句子做情感分类，判断其情感类别为正面，负面还是中性。相似地，在命名实体识别(NER)，给定一个上下文和一个中心词，我们想将中心词分类为许多类别之一。对输入，“Jim bought 300 shares of Acme Corp. in 2006”，我们希望有这样的一个分类结果：<span class=arithmatex>\([Jim]_{Person} \ bought \ 300 \ shares \ of [Acme\;Corp.]_{Organization} \ in \ [2006]_{Time}\)</span> .</p> <p>对这类问题，我们一般有以下形式的训练集：</p> <div class=arithmatex>\[ \{x^{(i)},y^{(i)}\}_{1}^{N} \]</div> <p>其中 <span class=arithmatex>\(x^{(i)}\)</span> 是一个 d 维的词向量， <span class=arithmatex>\(y^{(i)}\)</span> 是一个 C 维的 one-hot 向量，表示我们希望最终预测的标签（情感，其他词，专有名词，买／卖决策等）。</p> <p>我们可以使用诸如逻辑回归和 SVM 之类的算法对 2-D 词向量来进行分类，如下图所示</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560316538065.png><img alt=1560316538065 src=../imgs/1560316538065.png></a></p> <p>在一般的机器学习任务中，我们通常固定输入数据和目标标签，然后使用优化算法来训练权重（例如梯度下降，L-BFGS，牛顿法等等）。然而在 NLP 应用中，我们引入一个新的思想：在训练外部任务时对输入字向量进行再训练。下面我们讨论何时使用以及为什么要这样做。</p> <blockquote> <p>对于大型训练数据集，应考虑字向量再训练。对于小数据集，重新训练单词向量可能会降低性能。</p> </blockquote> <h4 id=32-retraining-word-vectors>3.2 Retraining Word Vectors<a class=headerlink href=#32-retraining-word-vectors title="Permanent link">¶</a></h4> <p>正如我们迄今所讨论的那样，我们用于外部评估的词向量是通过一个简单的内部评估来进行优化并初始化。在许多情况下，这些预训练的词向量在外部评估中表现良好。但是，这些预训练的词向量在外部评估中的表现仍然有提高的可能。然而，重新训练存在着一定的风险。</p> <p>如果我们在外部评估中重新训练词向量，这就需要保证训练集足够大并能覆盖词汇表中大部分的单词。这是因为Word2Vec或GloVe会生成语义相关的单词，这些单词位于单词空间的同一部分。</p> <p>假设预训练向量位于二维空间中，如下图所示。在这里，我们看到在一些外部分类任务中，单词向量被正确分类。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560317319052.png><img alt=1560317319052 src=../imgs/1560317319052.png></a></p> <p>现在，如果我们因为有限的训练集大小而只对其中两个向量进行再训练，那么我们在下图中可以看到，其中一个单词被错误分类了，因为单词向量更新导致边界移动。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560317310030.png><img alt=1560317310030 src=../imgs/1560317310030.png></a></p> <p>因此，如果训练数据集很小，就不应该对单词向量进行再训练。如果培训集很大，再培训可以提高性能。</p> <h4 id=33-softmax-classification-and-regularization>3.3 Softmax Classiﬁcation and Regularization<a class=headerlink href=#33-softmax-classification-and-regularization title="Permanent link">¶</a></h4> <p>我们考虑使用 Softmax 分类函数，函数形式如下所示：</p> <div class=arithmatex>\[ p(y_{j}=1\mid x)=\frac{exp(W_{j.}x)}{\sum_{c=1}^{C}exp(W_{c.}x)} \]</div> <p>这里我们计算词向量 <span class=arithmatex>\(x\)</span> 是类别 <span class=arithmatex>\(j\)</span> 的概率。使用交叉熵损失函数计算一个样本的损失如下所示：</p> <div class=arithmatex>\[ -\sum_{j=1}^{C}y_{j}\,log(p(y_{j}=1\mid x))=-\sum_{j=1}^{C}y_{j}\,log\bigg(\frac{exp(W_{j.}x)}{\sum_{c=1}^{C}exp(W_{c.}x)}\bigg) \]</div> <p>当然，上述求和是对 <span class=arithmatex>\((C-1)\)</span> 个零值求和，因为 <span class=arithmatex>\(y_{j}\)</span> 仅在单个索引为 1，这意味着 <span class=arithmatex>\(x\)</span> 仅属于 1 个正确的类别。现在我们定义 <span class=arithmatex>\(k\)</span> 为正确类别的索引。因此，我们现在可以简化损失函数：</p> <div class=arithmatex>\[ -log\bigg(\frac{exp(W_{k.}x)}{\sum_{c=1}^{C}exp(W_{c.}x)}\bigg) \]</div> <p>然后我们可以扩展为有 <span class=arithmatex>\(N\)</span> 个单词的损失函数：</p> <div class=arithmatex>\[ -\sum_{i=1}^{N}log\bigg(\frac{exp(W_{k(i).}x^{(i)})}{\sum_{c=1}^{C}exp(W_{c.}x^{i})}\bigg) \]</div> <p>上面公式的唯一不同是 <span class=arithmatex>\(k(i)\)</span> 现在一个函数，返回 <span class=arithmatex>\(x^{(i)}\)</span> 对应的每正确的类的索引。</p> <p>现在我们来估计一下同时训练模型的权值 <span class=arithmatex>\((W)\)</span> 和词向量 <span class=arithmatex>\((x)\)</span> 时需要更新的参数的数量。我们知道一个简单的线性决策模型至少需要一个 <span class=arithmatex>\(d\)</span> 维的词向量输入和生成一个 <span class=arithmatex>\(C\)</span> 个类别的分布。因此更新模型的权值，我们需要 <span class=arithmatex>\(C \cdot d\)</span> 个参数。如果我们也对词汇表 <span class=arithmatex>\(V\)</span> 中的每个单词都更新词向量，那么就要更新 <span class=arithmatex>\(|V|\)</span> 个词向量，每一个的维度是 <span class=arithmatex>\(d\)</span> 维。因此对一个简单的线性分类模型，总共的参数数目是 <span class=arithmatex>\(C \cdot d + |V|\)</span> ：</p> <div class=arithmatex>\[ \begin{eqnarray} \nabla_{\theta}J(\theta) = \begin{bmatrix} \nabla_{W_{.1}} \\ \vdots \\ \nabla_{W_{.d}} \\ \nabla_{aardvark} \\ \vdots \\ \nabla_{zebra} \end{bmatrix} \nonumber \end{eqnarray} \\ \]</div> <p>对于一个简单的模型来说，这是相当大的参数量——这样的参数量很可能会出现过拟合的问题。</p> <p>为了降低过拟合的风险，我们引入一个正则项，从贝叶斯派的思想看，这个正则项是对模型的参数加上一个先验分布，让参数变小（即接近于 0）：</p> <div class=arithmatex>\[ -\sum_{i=1}^{N}log\bigg(\frac{exp(W_{k(i).}x^{(i)})}{\sum_{c=1}^{C}exp(W_{c.}x^{i})}\bigg)+\lambda\sum_{k=1}^{C \cdot d + |V|\cdot d} \theta_{k}^{2} \]</div> <p>如果调整好目标权重 <span class=arithmatex>\(\lambda\)</span> 的值，最小化上面的函数将会降低出现很大的参数值的可能性，同时也提高模型的泛化能力。在我们使用更多参数更复杂的模型（例如神经网络）时，就更加需要正则化的思想。</p> <h4 id=34-window-classification>3.4 Window Classiﬁcation<a class=headerlink href=#34-window-classification title="Permanent link">¶</a></h4> <p>下图是我们有一个中心词和一个长度为 2 的对称窗口。这样的上下文可以帮助分辨 Paris 是一个地点还是一个名字。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560319005694.png><img alt=1560319005694 src=../imgs/1560319005694.png></a></p> <p>目前为止，我们主要探讨了使用单个单词向量 <span class=arithmatex>\(x\)</span> 预测的外部评估任务。在现实中，因为自然语言处理的性质，这几乎不会有这样的任务。在自然语言处理中，常常存在着一词多义的情况，我们一般要利用词的上下文来判断其不同的意义。例如，如果你要某人解释“to sanction”是什么意思，你会马上意识到根据“to sanction”的上下文其意思可能是“to permit”或者“to punish”。在更多的情况下，我们使用一个单词序列作为模型的输入。这个序列是由中心词向量和上下文词向量组成。上下文中的单词数量也被称为上下文窗口大小，并根据解决的问题而变化。<strong>一般来说,较窄的窗口大小会导致在句法测试中更好的性能,而更宽的窗口会导致在语义测试中更好的性能</strong>。</p> <p>为了将之前讨论的 Softmax 模型修改为使用单词的窗口来进行分类，我们只需要按照下面形式将 <span class=arithmatex>\(x^{(i)}\)</span> 替换为 <span class=arithmatex>\(x_{window}^{(i)}\)</span> ：</p> <div class=arithmatex>\[ \begin{eqnarray} x_{window}^{(i)} = \begin{bmatrix} x^{(i-2)} \\ x^{(i-1)} \\ x^{(i)} \\ x^{(i+1)} \\ x^{(i+2)} \end{bmatrix} \nonumber \end{eqnarray} \]</div> <p>因此，当我们计算单词的损失梯度如下所示，当然需要分配梯度来更新相应的词向量：</p> <div class=arithmatex>\[ \begin{eqnarray} x_{window}^{(i)} = \begin{bmatrix} \nabla_{x^{(i-2)}} \\ \nabla_{x^{(i-1)}} \\ \nabla_{x^{(i)}} \\ \nabla_{x^{(i+1)}} \\ \nabla_{x^{(i+2)}} \end{bmatrix} \nonumber \end{eqnarray} \]</div> <p>当然，梯度将需要分发来更新相应的词向量。</p> <h4 id=35-non-linear-classifiers>3.5 Non-linear Classiﬁers<a class=headerlink href=#35-non-linear-classifiers title="Permanent link">¶</a></h4> <p>我们现在介绍非线性分类模型，如神经网络。我们看到即使是最优的线性分类平面，也有许多样例都被错误的分类。这是因为线性模型在这个数据集上的分类能力有限。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560320056279.png><img alt=1560320056279 src=../imgs/1560320056279.png></a></p> <p>在下图中，我们看到非线性分类模型可以对上面的数据集的样例有着更好的分类结果，这个简答的例子可以初步的说明我们为什么需要非线性模型。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560320060184.png><img alt=1560320060184 src=../imgs/1560320060184.png></a></p> <h2 id=suggested-readings>Suggested Readings<a class=headerlink href=#suggested-readings title="Permanent link">¶</a></h2> <h3 id=glove-global-vectors-for-word-representation>GloVe Global Vectors for Word Representation<a class=headerlink href=#glove-global-vectors-for-word-representation title="Permanent link">¶</a></h3> <p>此为 Glove 原文，已在 Lecture 和 Note 中详细记录。</p> <h3 id=improving-distributional-similarity-with-lessons-learned-from-word-embeddings>Improving Distributional Similarity with Lessons Learned from Word Embeddings<a class=headerlink href=#improving-distributional-similarity-with-lessons-learned-from-word-embeddings title="Permanent link">¶</a></h3> <p>实际应用中如何获得更好的词向量</p> <ul> <li>Abstract<ul> <li>近年来的研究趋势表明，基于神经网络的词嵌入模型在词相似性和相似性检测任务上优于传统的基于计数的分布模型。我们发现，词嵌入的性能提高在很大程度上是由于某些系统设计选择和超参数优化，而不是嵌入算法本身。此外，我们证明这些修改可以转移到传统的分配模型，产生类似的收益。与以前的报告相比，我们观察到的方法之间的性能差异主要是局部的或不显著的，没有任何一种方法比其他方法具有全局优势。</li> </ul> </li> <li>Introduction<ul> <li>然而，最先进的嵌入方法都是基于相同的 bag-of-contexts 的单词。此外，分析表明，word2vec的 SGNS 隐含地分解了单词上下文的PMI矩阵。也就是说，其数学目标和可用的信息来源实际上与传统方法所使用的非常相似。那么其优势来源于什么呢？</li> <li>虽然模型和优化的目标函数是主要因素，但是其他因素也会影响结果<ul> <li>超参数优化：负采样的样本个数，平滑的负采样分布，动态大小的上下文窗口</li> </ul> </li> <li>在这项工作中，我们将这些超参数显式化，并展示了如何将它们应用到传统的基于计数器的方法中。为了评估每个超参数对算法性能的影响，我们进行了实验，比较了四种不同的表示方法，同时控制了不同的超参数</li> </ul> </li> <li>Practical Recommendations<ul> <li>通常建议为手头的任务调优所有超参数，以及特定于算法的超参数。然而，这可能是计算昂贵的。因此我们提供一些“经验法则”，我们发现它们在我们的设置中工作得很好：<ul> <li>始终使用上下文分布平滑(cds = 0.75)来修改PMI，并且适用于 PPMI,SVD 和 SGNS，不断提高性能</li> <li>不要使用 SVD “correctly” (eig = 1) 。使用对称变体之一</li> <li>SGNS是健壮的基线。虽然它可能不是每个任务的最佳方法，但它在任何情况下都不会表现得很差。此外，SGNS是最快的训练方法，而且在磁盘空间和内存消耗方面(到目前为止)也是最便宜的。</li> <li>SGNS适合更多的负样本</li> <li>对于SGNS和GloVe而言，值得对 <span class=arithmatex>\(\vec{w}+\vec{c}\)</span> 做实验，因为这是容易应用（不需要重新训练）并且带来可观收益（以及可观损失）的。</li> </ul> </li> </ul> </li> <li>Conclusion<ul> <li>最近的嵌入方法引入了大量的网络结构以外的设计选择和优化算法。我们揭示了这些看似微小的变化对单词表示方法的成功有很大的影响。通过展示如何在传统方法中适应和调整这些超参数,我们对表示进行适当的比较,并从词嵌入文献中挑战各种优势。</li> <li>本研究还揭示了对更多控制变量实验的需要,并将“变量”的概念从明显的任务、数据和方法扩展到经常忽略的预处理步骤和超参数设置。我们还强调了需要进行透明和可重复的实验,并赞扬诸如Mikolov、Pennington等作者,以及其他人公开提供他们的代码。本着这种精神,我们也公布我们的代码</li> </ul> </li> </ul> <h3 id=evaluation-methods-for-unsupervised-word-embeddings>Evaluation methods for unsupervised word embeddings<a class=headerlink href=#evaluation-methods-for-unsupervised-word-embeddings title="Permanent link">¶</a></h3> <ul> <li> <p>Abstract</p> <ul> <li>我们介绍了一种无监督嵌入技术的评估方法，该方法可以从文本中获取有意义的表示。嵌入方法的顺序不同，评价结果也不同，这就对通常认为只有一个最优向量表示的假设提出了质疑。我们提供了一种新的评估技术，可以直接通过特定查询比较词嵌入。这些方法减少了偏差，提供了更大的洞察力，并允许我们通过众包快速准确地征求数据驱动的相关性判断。</li> </ul> </li> <li> <p>Discussion</p> <ul> <li>超参数优化会导致明显的性能差异。</li> <li>实际上，不同的算法编码的信息出奇地不同，这些信息可能与我们想要的用例一致，也可能与我们想要的用例不一致。</li> <li>例如，我们发现词嵌入将关于词频的信息的编码程度不同，即使在长度归一化后也是如此。</li> <li>这个结果令人惊讶<ul> <li>首先，许多算法保留了不同的拦截参数来吸收基于频率的效果。</li> <li>其次，我们希望嵌入空间的几何形状主要由语义驱动：频率相对较小的词应该均匀地分布在空间中，而大量罕见的、特定的单词应该围绕相关但更频繁的单词聚集。</li> </ul> </li> <li>我们训练了一个逻辑回归模型来预测基于词向量的词频类别。训练线性分类器将单词分类为常见或罕见类别，阈值从100到50000不等。在每个阈值频率下，我们对训练集进行采样以确保标签分布在所有频率上的一致性平衡。我们使用了长度归一化的嵌入，因为罕见的单词在训练期间更新较少，可能具有更短的向量(Turian et al., 2010)。在下图中，我们报告了在每个阈值频率下使用五倍交叉验证的平均准确度和标准偏差(1<span class=arithmatex>\(\sigma\)</span>)。</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-type=image data-width=100% href=../imgs/1560844518041.png><img alt=1560844518041 src=../imgs/1560844518041.png></a></p> <ul> <li>所有单词嵌入都比随机的好,这表明它们包含一些频率信息。GloVe和TSCCA在接近1000的词频上达到近100%的准确性。与其他所有嵌入不同，C&amp;W嵌入的更大的词频的准确性增加了。进一步的调查显示，权重矩阵的方向是随词频阈值改变的，这表明词频似乎在嵌入空间中以平滑的方式被编码。</li> <li>虽然GloVe和CBOW是内在任务上最佳的两种嵌入，但它们在其编码的频率信息量上有很大的差异。因此,我们可以断定，不同的频率预测并不是因为自然语言的内在性质：并不是说频繁的单词自然只有频繁的邻居。</li> <li>嵌入空间中的词频信息也会影响词的相似性。对于WordSim-353数据集，我们查询了k = 1000 个最近邻居。然后，我们查询其在训练集语料库中频率的排名，平均了所有的查询词。在我们的实验中，我们发现<strong>一个单词的频率和它在最近邻中的排名位置有很强的相关性。</strong>下图显示了C&amp;W词嵌入中一个词的最近邻排名（关于一次查询）和其词频在训练集语料库中的排名之间的幂律关系 (nn-rank ∼ 1000 · <span class=arithmatex>\(\text{corpus-rank}^{0.17}\)</span>)。这是一个值得关注的问题：语言中单词的频率在人类的文字处理过程中也起着至关重要的作用(Cattell, 1886)。因此，在实验设计中，我们需要明确地把词频作为一个因素来考虑。同时，上述结果也表明，常用的余弦相似度在嵌入空间内的固有任务时，会受到频率效应的影响。我们认为，进一步的研究应该解决如何更好地衡量词与嵌入空间之间的语言关系的问题，例如通过学习自定义度量。</li> </ul> </li> <li> <p>Related Work</p> <ul> <li>Mikolov et al. (2013b) 说明嵌入空间存在特定的语言规律。通过在嵌入空间中进行简单的向量运算，可以解决各种句法和语义类比问题。这与之前的工作不同，之前的工作将类比任务描述为一个分类问题(Turney, 2008)。令人惊讶的是，词嵌入似乎捕捉到了更复杂的语言特性。Chen等人(2013)的研究表明，单词嵌入甚至包含了区域拼写(英式与美式)、名词性别和情感极性等信息。</li> <li>以往的词嵌入评价工作可分为内部评价和外部评价。内在评价通过直接测量语义关联和几何关联之间的相关性来衡量词向量的质量，通常通过查询术语的的库存来实现。Baroni等人(2014)以内在度量为重点，在各种查询清单和任务上比较词嵌入和分布词向量。Faruqui and Dyer (2014)提供了一个网站，该网站允许对一些查询清单的嵌入进行自动评估。Gaoetal.(2014)发表了一份改进的类比推理任务查询清单。最后，Tsvetkov等人(2015)提出了一种新的内在度量方法，该方法可以更好地关联外部效果。然而，所有这些评估都是在预先收集的清单上进行的，并且大多局限于本地指标，如相关性。</li> <li>外部评估使用嵌入作为其他任务模型中的特征，例如语义角色标记或词性标记(Collobert etal., 2011)，并提高现有系统的性能(Turianetal.，2010)。然而，他们在其他任务上，如解析，则不太成功(Andreas和Klein, 2014)。</li> <li>在主题模型的上下文中，无监督语义建模方面做了更多的工作。一个例子是单词入侵任务(Chang et al.， 2009)，其中注释器被要求识别插入到给定主题的一组高概率单词中的随机单词。词嵌入不产生可解释的维度，因此我们不能直接使用这个方法，但是我们提出了一个基于最近邻居的相关任务。手工评估是昂贵和耗时的，但其他研究表明，自动化评估可以紧密地模拟人类的直觉(Newman et al.， 2010)。</li> </ul> </li> <li> <p>Conclusion</p> <ul> <li>影响嵌入质量的因素很多。标准的综合评价虽然有用，但不能提供完整或一致的情况。词汇频率等因素在其中扮演着重要的角色，而这在以前是不为人知的。词频也会干扰常用的余弦相似性度量。我们提出了一个新的评估框架，该框架基于嵌入之间的直接比较，为这些嵌入提供了更精细的分析，并支持简单的众包相关性判断。我们还提出了一个新的一致性任务，它测量了我们的直觉，即嵌入空间中的邻域应该在语义或语法上相关。我们发现，外部评估虽然有助于突出嵌入性能的特定方面，但不应该用作通用质量的代理。</li> </ul> </li> </ul> <h2 id=reference>Reference<a class=headerlink href=#reference title="Permanent link">¶</a></h2> <p>以下是学习本课程时的可用参考书籍：</p> <p><a href=https://item.jd.com/12355569.html>《基于深度学习的自然语言处理》</a> <a href=https://nndl.github.io/ >《神经网络与深度学习》</a></p> <p>以下是整理笔记的过程中参考的博客：</p> <p><a href=https://zhuanlan.zhihu.com/p/59011576>斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p> <p><a href=https://zhuanlan.zhihu.com/p/31977759>斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> (这是针对note部分的翻译)</p> </article> </div> </div> <button class="md-top md-icon" data-md-component=top hidden type=button> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg> 回到页面顶部 </button> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2019 - 2022; Xiao Xu </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material for MkDocs </a> </div> <div class=md-social> <a class=md-social__link href=https://github.com/looperXX rel=noopener target=_blank title=github.com> <svg viewbox="0 0 480 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg> </a> <a class=md-social__link href=https://twitter.com/looperxx_nlp rel=noopener target=_blank title=twitter.com> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg> </a> <a class=md-social__link href=https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/ rel=noopener target=_blank title=www.linkedin.com> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.annotate", "content.tooltips", "navigation.indexes", "navigation.tracking", "navigation.sections", "navigation.tabs", "navigation.top", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../assets/javascripts/bundle.51198bba.min.js></script> <script src=../javascripts/baidu-tongji.js></script> <script src=../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body> </html>