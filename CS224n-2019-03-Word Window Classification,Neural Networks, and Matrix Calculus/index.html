


<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LooperXX's homepage">
      
      
      
        <meta name="author" content="Looper - Xiao Xu">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.1">
    
    
      
        <title>03 Word Window Classification,Neural Networks, and Matrix Calculus - Science is interesting.</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.a676eddb.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-164217558-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    <body dir="">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="">
    <a href=".." title="Science is interesting." class="md-header-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Science is interesting.
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              03 Word Window Classification,Neural Networks, and Matrix Calculus
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
          

  

<nav class="md-tabs md-tabs--active" aria-label="" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." class="md-tabs__link">
        Home
      </a>
    
  </li>

      
        
  
  
    
    
  
  
    <li class="md-tabs__item">
      
        <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" class="md-tabs__link">
          NLP
        </a>
      
    </li>
  

  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../Normalization/" class="md-tabs__link">
          ML & DL
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-tabs__link md-tabs__link--active">
          CS224n学习笔记
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../MkDocs_demo/" class="md-tabs__link">
          For MkDocs
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Science is interesting." class="md-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    Science is interesting.
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      NLP
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="NLP" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        NLP
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-1" type="checkbox" id="nav-2-1">
    
    <label class="md-nav__link" for="nav-2-1">
      理论笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="理论笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-1">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        理论笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" title="自然语言处理简介" class="md-nav__link">
      自然语言处理简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%E7%9A%84%E5%B7%A8%E4%BA%BA%E8%82%A9%E8%86%80/" title="NLP的巨人肩膀" class="md-nav__link">
      NLP的巨人肩膀
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Attention/" title="Attention" class="md-nav__link">
      Attention
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2">
    
    <label class="md-nav__link" for="nav-2-2">
      代码学习
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="代码学习" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        代码学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../NCRF%2B%2B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="NCRF++" class="md-nav__link">
      NCRF++
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-3" type="checkbox" id="nav-2-3">
    
    <label class="md-nav__link" for="nav-2-3">
      书籍笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="书籍笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        书籍笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" title="Machine Reading Comprehension" class="md-nav__link">
      Machine Reading Comprehension
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%20Concepts/" title="Some Concepts" class="md-nav__link">
      Some Concepts
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NNDL%20%E4%B9%A0%E9%A2%98/" title="NNDL 习题" class="md-nav__link">
      NNDL 习题
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      ML & DL
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="ML & DL" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        ML & DL
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Normalization/" title="Normalization" class="md-nav__link">
      Normalization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%8A%B1%E4%B9%A6%E7%BB%8F%E9%AA%8C%E6%B3%95%E5%88%99/" title="花书经验法则" class="md-nav__link">
      花书经验法则
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      CS224n学习笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="CS224n学习笔记" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        CS224n学习笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" title="CS224n-2019简介" class="md-nav__link">
      CS224n-2019简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-Assignment/" title="CS224n-2019作业笔记" class="md-nav__link">
      CS224n-2019作业笔记
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-01-Introduction%20and%20Word%20Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">
      01 Introduction and Word Vectors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">
      02 Word Vectors 2 and Word Senses
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        03 Word Window Classification,Neural Networks, and Matrix Calculus
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link md-nav__link--active">
      03 Word Window Classification,Neural Networks, and Matrix Calculus
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" class="md-nav__link">
    Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus
  </a>
  
    <nav class="md-nav" aria-label="Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-setup-and-notation" class="md-nav__link">
    Classification setup and notation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-network-classifiers" class="md-nav__link">
    Neural Network Classifiers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named-entity-recognition-ner" class="md-nav__link">
    Named Entity Recognition (NER)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#binary-word-window-classification" class="md-nav__link">
    Binary word window classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradients" class="md-nav__link">
    Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-03-neural-networks-backpropagation" class="md-nav__link">
    Notes 03 Neural Networks, Backpropagation
  </a>
  
    <nav class="md-nav" aria-label="Notes 03 Neural Networks, Backpropagation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-neural-networks-foundations" class="md-nav__link">
    1 Neural Networks: Foundations
  </a>
  
    <nav class="md-nav" aria-label="1 Neural Networks: Foundations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-a-neuron" class="md-nav__link">
    1.1 A Neuron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-a-single-layer-of-neurons" class="md-nav__link">
    1.2 A Single Layer of Neurons
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-feed-forward-computation" class="md-nav__link">
    1.3 Feed-forward Computation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-maximum-margin-objective-function" class="md-nav__link">
    1.4 Maximum Margin Objective Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-training-with-backpropagation-elemental" class="md-nav__link">
    1.5 Training with Backpropagation – Elemental
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-training-with-backpropagation-vectorized" class="md-nav__link">
    1.6 Training with Backpropagation – Vectorized
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-neural-networks-tips-and-tricks" class="md-nav__link">
    2 Neural Networks: Tips and Tricks
  </a>
  
    <nav class="md-nav" aria-label="2 Neural Networks: Tips and Tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-gradient-check" class="md-nav__link">
    2.1 Gradient Check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-regularization" class="md-nav__link">
    2.2 Regularization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-dropout" class="md-nav__link">
    2.3 Dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-neuron-units" class="md-nav__link">
    2.4 Neuron Units
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-data-preprocessing" class="md-nav__link">
    2.5 Data Preprocessing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26-parameter-initialization" class="md-nav__link">
    2.6 Parameter Initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27-learning-strategies" class="md-nav__link">
    2.7 Learning Strategies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#28-momentum-updates" class="md-nav__link">
    2.8 Momentum Updates
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#29-adaptive-optimization-methods" class="md-nav__link">
    2.9 Adaptive Optimization Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#210-more-reference" class="md-nav__link">
    2.10 More reference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-neural-network-gradients" class="md-nav__link">
    Computing Neural Network Gradients
  </a>
  
    <nav class="md-nav" aria-label="Computing Neural Network Gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1 Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-vectorized-gradients" class="md-nav__link">
    2 Vectorized Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">
      04 Backpropagation and Computation Graphs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-nav__link">
      05 Linguistic Structure Dependency Parsing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" title="06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">
      06 The probability of a sentence Recurrent Neural Networks and Language Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" title="07 Vanishing Gradients and Fancy RNNs" class="md-nav__link">
      07 Vanishing Gradients and Fancy RNNs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/" title="08 Machine Translation, Sequence-to-sequence and Attention" class="md-nav__link">
      08 Machine Translation, Sequence-to-sequence and Attention
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" title="09 Practical Tips for Final Projects" class="md-nav__link">
      09 Practical Tips for Final Projects
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/" title="10 Question Answering and the Default Final Project" class="md-nav__link">
      10 Question Answering and the Default Final Project
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" title="11 ConvNets for NLP" class="md-nav__link">
      11 ConvNets for NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/" title="12 Information from parts of words Subword Models" class="md-nav__link">
      12 Information from parts of words Subword Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/" title="13 Modeling contexts of use Contextual Representations and Pretraining" class="md-nav__link">
      13 Modeling contexts of use Contextual Representations and Pretraining
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/" title="14 Transformers and Self-Attention For Generative Models" class="md-nav__link">
      14 Transformers and Self-Attention For Generative Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-15-Natural%20Language%20Generation/" title="15 Natural Language Generation" class="md-nav__link">
      15 Natural Language Generation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-16-Coreference%20Resolution/" title="16 Coreference Resolution" class="md-nav__link">
      16 Coreference Resolution
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-17-Multitask%20Learning/" title="17 Multitask Learning" class="md-nav__link">
      17 Multitask Learning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/" title="18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment" class="md-nav__link">
      18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/" title="19 Safety, Bias, and Fairness" class="md-nav__link">
      19 Safety, Bias, and Fairness
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/" title="20 The Future of NLP + Deep Learning" class="md-nav__link">
      20 The Future of NLP + Deep Learning
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      For MkDocs
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="For MkDocs" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        For MkDocs
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../MkDocs_demo/" title="Demo" class="md-nav__link">
      Demo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Material%20Theme%20Tutorial/" title="Material Theme Tutorial" class="md-nav__link">
      Material Theme Tutorial
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" class="md-nav__link">
    Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus
  </a>
  
    <nav class="md-nav" aria-label="Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-setup-and-notation" class="md-nav__link">
    Classification setup and notation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-network-classifiers" class="md-nav__link">
    Neural Network Classifiers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named-entity-recognition-ner" class="md-nav__link">
    Named Entity Recognition (NER)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#binary-word-window-classification" class="md-nav__link">
    Binary word window classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradients" class="md-nav__link">
    Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-03-neural-networks-backpropagation" class="md-nav__link">
    Notes 03 Neural Networks, Backpropagation
  </a>
  
    <nav class="md-nav" aria-label="Notes 03 Neural Networks, Backpropagation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-neural-networks-foundations" class="md-nav__link">
    1 Neural Networks: Foundations
  </a>
  
    <nav class="md-nav" aria-label="1 Neural Networks: Foundations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-a-neuron" class="md-nav__link">
    1.1 A Neuron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-a-single-layer-of-neurons" class="md-nav__link">
    1.2 A Single Layer of Neurons
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-feed-forward-computation" class="md-nav__link">
    1.3 Feed-forward Computation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-maximum-margin-objective-function" class="md-nav__link">
    1.4 Maximum Margin Objective Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-training-with-backpropagation-elemental" class="md-nav__link">
    1.5 Training with Backpropagation – Elemental
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-training-with-backpropagation-vectorized" class="md-nav__link">
    1.6 Training with Backpropagation – Vectorized
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-neural-networks-tips-and-tricks" class="md-nav__link">
    2 Neural Networks: Tips and Tricks
  </a>
  
    <nav class="md-nav" aria-label="2 Neural Networks: Tips and Tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-gradient-check" class="md-nav__link">
    2.1 Gradient Check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-regularization" class="md-nav__link">
    2.2 Regularization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-dropout" class="md-nav__link">
    2.3 Dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-neuron-units" class="md-nav__link">
    2.4 Neuron Units
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-data-preprocessing" class="md-nav__link">
    2.5 Data Preprocessing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26-parameter-initialization" class="md-nav__link">
    2.6 Parameter Initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27-learning-strategies" class="md-nav__link">
    2.7 Learning Strategies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#28-momentum-updates" class="md-nav__link">
    2.8 Momentum Updates
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#29-adaptive-optimization-methods" class="md-nav__link">
    2.9 Adaptive Optimization Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#210-more-reference" class="md-nav__link">
    2.10 More reference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-neural-network-gradients" class="md-nav__link">
    Computing Neural Network Gradients
  </a>
  
    <nav class="md-nav" aria-label="Computing Neural Network Gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1 Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-vectorized-gradients" class="md-nav__link">
    2 Vectorized Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/LooperXX/LooperXX.github.io/edit/master/docs/CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus.md" title="编辑此页" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71,7.04C21.1,6.65 21.1,6 20.71,5.63L18.37,3.29C18,2.9 17.35,2.9 16.96,3.29L15.12,5.12L18.87,8.87M3,17.25V21H6.75L17.81,9.93L14.06,6.18L3,17.25Z" /></svg>
                  </a>
                
                
                  
                
                
                  <h1>03 Word Window Classification,Neural Networks, and Matrix Calculus</h1>
                
                <h2 id="lecture-03-word-window-classificationneural-networks-and-matrix-calculus">Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus<a class="headerlink" href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" title="Permanent link">&para;</a></h2>
<p><strong>Lecture Plan</strong></p>
<ul>
<li>Classification review/introduction</li>
<li>Neural networks introduction</li>
<li>Named Entity Recognition</li>
<li>Binary true vs. corrupted word window classification</li>
<li>Matrix calculus introduction</li>
</ul>
<p><strong>提示</strong>：这对一些人而言将是困难的一周，课后需要阅读提供的资料。</p>
<h3 id="classification-setup-and-notation">Classification setup and notation<a class="headerlink" href="#classification-setup-and-notation" title="Permanent link">&para;</a></h3>
<p>通常我们有由样本组成的训练数据集</p>
<div>
<div class="MathJax_Preview">
\left\{x_{i}, y_{i}\right\}_{i=1}^{N}
</div>
<script type="math/tex; mode=display">
\left\{x_{i}, y_{i}\right\}_{i=1}^{N}
</script>
</div>
<p><span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 是输入，例如单词（索引或是向量），句子，文档等等，维度为 <span><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span></p>
<p><span><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> 是我们尝试预测的标签（ <span><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> 个类别中的一个），例如：</p>
<ul>
<li>类别：感情，命名实体，购买/售出的决定</li>
<li>其他单词</li>
<li>之后：多词序列的</li>
</ul>
<p><strong>Classification intuition</strong></p>
<p><img alt="1560343189656" src="../imgs/1560343189656.png" /></p>
<p>训练数据： <span><span class="MathJax_Preview">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</span><script type="math/tex">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</script></span></p>
<p>简单的说明情况</p>
<ul>
<li>固定的二维单词向量分类</li>
<li>使用softmax/logistic回归</li>
<li>线性决策边界</li>
</ul>
<p><strong>传统的机器学习/统计学方法</strong>：假设 <span><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 是固定的，训练 softmax/logistic 回归的权重 <span><span class="MathJax_Preview">W \in \mathbb{R}^{C \times d}</span><script type="math/tex">W \in \mathbb{R}^{C \times d}</script></span> 来决定决定边界(超平面)</p>
<p><strong>方法</strong>：对每个 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> ，预测
$$
p(y | x)=\frac{\exp \left(W_{y} . x\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}
$$
我们可以将预测函数分为两个步骤：</p>
<ol>
<li>
<p>将 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 的 <span><span class="MathJax_Preview">y^{th}</span><script type="math/tex">y^{th}</script></span> 行和 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 中的对应行相乘得到分数
    $$
    W_{y} \cdot x=\sum_{i=1}^{d} W_{y i} x_{i}=f_{y}
    $$</p>
<p>计算所有的 <span><span class="MathJax_Preview">f_c, for \ c=1,\dots,C</span><script type="math/tex">f_c, for \ c=1,\dots,C</script></span></p>
</li>
<li>
<p>使用softmax函数获得归一化的概率</p>
</li>
</ol>
<div>
<div class="MathJax_Preview">
p(y | x)=\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}=\operatorname{softmax}\left(f_{y}\right)
</div>
<script type="math/tex; mode=display">
p(y | x)=\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}=\operatorname{softmax}\left(f_{y}\right)
</script>
</div>
<p><strong>Training with softmax and cross-entropy loss</strong></p>
<p>对于每个训练样本 <span><span class="MathJax_Preview">(x,y)</span><script type="math/tex">(x,y)</script></span> ，我们的目标是最大化正确类 <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 的概率，或者我们可以最小化该类的负对数概率
$$
-\log p(y | x)=-\log \left(\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}\right)
$$
<strong>Background: What is “cross entropy” loss/error?</strong></p>
<ul>
<li>交叉熵”的概念来源于信息论，衡量两个分布之间的差异</li>
<li>令真实概率分布为 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></li>
<li>令我们计算的模型概率为 <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span></li>
<li>交叉熵为</li>
</ul>
<div>
<div class="MathJax_Preview">
H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)
</div>
<script type="math/tex; mode=display">
H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)
</script>
</div>
<ul>
<li>假设 groud truth (or true or gold or target)的概率分布在正确的类上为1，在其他任何地方为0：<span><span class="MathJax_Preview">p = [0,…,0,1,0,…0]</span><script type="math/tex">p = [0,…,0,1,0,…0]</script></span> </li>
<li>因为 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 是独热向量，所以唯一剩下的项是真实类的负对数概率</li>
</ul>
<p><strong>Classification over a full dataset</strong></p>
<p>在整个数据集 <span><span class="MathJax_Preview">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</span><script type="math/tex">\left\{x_{i}, y_{i}\right\}_{i=1}^{N}</script></span> 上的交叉熵损失函数，是所有样本的交叉熵的均值</p>
<div>
<div class="MathJax_Preview">
J(\theta)=\frac{1}{N} \sum_{i=1}^{N}-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{c=1}^{C} e^{f_{c}}}\right)
</div>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{N} \sum_{i=1}^{N}-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{c=1}^{C} e^{f_{c}}}\right)
</script>
</div>
<p>我们不使用</p>
<div>
<div class="MathJax_Preview">
f_{y}=f_{y}(x)=W_{y} \cdot x=\sum_{j=1}^{d} W_{y j} x_{j}
</div>
<script type="math/tex; mode=display">
f_{y}=f_{y}(x)=W_{y} \cdot x=\sum_{j=1}^{d} W_{y j} x_{j}
</script>
</div>
<p>我们使用矩阵来表示 <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span></p>
<div>
<div class="MathJax_Preview">
f = Wx
</div>
<script type="math/tex; mode=display">
f = Wx
</script>
</div>
<p><strong>Traditional ML optimization</strong></p>
<ul>
<li>一般机器学习的参数 <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 通常只由W的列组成</li>
</ul>
<div>
<div class="MathJax_Preview">
\theta=\left[\begin{array}{c}{W_{\cdot 1}} \\ {\vdots} \\ {W_{\cdot d}}\end{array}\right]=W( :) \in \mathbb{R}^{C d}
</div>
<script type="math/tex; mode=display">
\theta=\left[\begin{array}{c}{W_{\cdot 1}} \\ {\vdots} \\ {W_{\cdot d}}\end{array}\right]=W( :) \in \mathbb{R}^{C d}
</script>
</div>
<ul>
<li>因此，我们只通过以下方式更新决策边界</li>
</ul>
<div>
<div class="MathJax_Preview">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d}}}\end{array}\right] \in \mathbb{R}^{C d}
</div>
<script type="math/tex; mode=display">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d}}}\end{array}\right] \in \mathbb{R}^{C d}
</script>
</div>
<h3 id="neural-network-classifiers">Neural Network Classifiers<a class="headerlink" href="#neural-network-classifiers" title="Permanent link">&para;</a></h3>
<p><img alt="1560345898614" src="../imgs/1560345898614.png" /></p>
<ul>
<li>单独使用Softmax(≈logistic回归)并不十分强大</li>
<li>Softmax只给出线性决策边界<ul>
<li>这可能是相当有限的，当问题很复杂时是无用的</li>
<li>纠正这些错误不是很酷吗?</li>
</ul>
</li>
</ul>
<p><strong>Neural Nets for the Win!</strong></p>
<p>神经网络可以学习更复杂的函数和非线性决策边界</p>
<p><img alt="1560346033994" src="../imgs/1560346033994.png" /></p>
<p>更高级的分类需要</p>
<ul>
<li>词向量</li>
<li>更深层次的深层神经网络</li>
</ul>
<p><strong>Classification difference with word vectors</strong></p>
<p>一般在NLP深度学习中</p>
<ul>
<li>我们学习了矩阵 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 和词向量 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></li>
<li>我们学习传统参数和表示</li>
<li>词向量是对独热向量的重新表示——在中间层向量空间中移动它们——以便使用(线性)softmax分类器通过 x = Le 层进行分类<ul>
<li>即将词向量理解为一层神经网络，输入单词的独热向量并获得单词的词向量表示，并且我们需要对其进行更新。其中，<span><span class="MathJax_Preview">Vd</span><script type="math/tex">Vd</script></span> 是数量很大的参数</li>
</ul>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d a r d v a r k}}} \\ {\vdots} \\ {\nabla_{x_{z e b r a}}}\end{array}\right] \in \mathbb{R}^{C d + V d}
</div>
<script type="math/tex; mode=display">
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d a r d v a r k}}} \\ {\vdots} \\ {\nabla_{x_{z e b r a}}}\end{array}\right] \in \mathbb{R}^{C d + V d}
</script>
</div>
<p><strong>Neural computation</strong></p>
<p><img alt="1560346664232" src="../imgs/1560346664232.png" /></p>
<p><strong>An artificial neuron</strong></p>
<ul>
<li>神经网络有自己的术语包</li>
<li>但如果你了解 softmax 模型是如何工作的，那么你就可以很容易地理解神经元的操作</li>
</ul>
<p><img alt="1560346716435" src="../imgs/1560346716435.png" /></p>
<p><strong>A neuron can be a binary logistic regression unit</strong></p>
<p><span><span class="MathJax_Preview">f = \text{nonlinear activation fct. (e.g. sigmoid)} , w =\text{weights}, b =\text{bias}, h =\text{hidden}, x = \text{inputs}</span><script type="math/tex">f = \text{nonlinear activation fct. (e.g. sigmoid)} , w =\text{weights}, b =\text{bias}, h =\text{hidden}, x = \text{inputs}</script></span>
$$
\begin{array}{l}{h_{w, b}(x)=f\left(w^{\top} x+b\right)} \ {f(z)=\frac{1}{1+e^{-z}}}\end{array}
$$
<span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> : 我们可以有一个“总是打开”的特性，它给出一个先验类，或者将它作为一个偏向项分离出来</p>
<p><span><span class="MathJax_Preview">w,b</span><script type="math/tex">w,b</script></span> 是神经元的参数</p>
<p><strong>A neural network</strong>
<strong>= running several logistic regressions at the same time</strong></p>
<p><img alt="1560347357837" src="../imgs/1560347357837.png" /></p>
<p>如果我们输入一个向量通过一系列逻辑回归函数，那么我们得到一个输出向量，但是我们不需要提前决定这些逻辑回归试图预测的变量是什么。</p>
<p><img alt="1560347481494" src="../imgs/1560347481494.png" /></p>
<p>我们可以输入另一个logistic回归函数。损失函数将指导中间隐藏变量应该是什么，以便更好地预测下一层的目标。我们当然可以使用更多层的神经网络。</p>
<p><strong>Matrix notation for a layer</strong></p>
<p><img alt="1560347762809" src="../imgs/1560347762809.png" />
$$
\begin{array}{l}{a_{1}=f\left(W_{11} x_{1}+W_{12} x_{2}+W_{13} x_{3}+b_{1}\right)} \ {a_{2}=f\left(W_{21} x_{1}+W_{22} x_{2}+W_{23} x_{3}+b_{2}\right)}\ {z=W x+b} \ {a=f(z)} \ f\left(\left[z_{1}, z_{2}, z_{3}\right]\right)=\left[f\left(z_{1}\right), f\left(z_{2}\right), f\left(z_{3}\right)\right] \end{array}
$$</p>
<ul>
<li><span><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 在运算时是 element-wise 逐元素的</li>
</ul>
<p><strong>Non-linearities (aka “f ”): Why they’re needed</strong></p>
<p>例如：函数近似，如回归或分类</p>
<ul>
<li>没有非线性，深度神经网络只能做线性变换</li>
<li>多个线性变换可以组成一个的线性变换 <span><span class="MathJax_Preview">W_1 W_2 x = Wx</span><script type="math/tex">W_1 W_2 x = Wx</script></span> <ul>
<li>因为线性变换是以某种方式旋转和拉伸空间，多次的旋转和拉伸可以融合为一次线性变换</li>
</ul>
</li>
<li>对于非线性函数而言，使用更多的层，他们可以近似更复杂的函数</li>
</ul>
<h3 id="named-entity-recognition-ner">Named Entity Recognition (NER)<a class="headerlink" href="#named-entity-recognition-ner" title="Permanent link">&para;</a></h3>
<ul>
<li>任务：例如，查找和分类文本中的名称</li>
</ul>
<p><img alt="1560359392887" src="../imgs/1560359392887.png" /></p>
<ul>
<li>可能的用途<ul>
<li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li>
<li>对于问题回答，答案通常是命名实体</li>
<li>许多需要的信息实际上是命名实体之间的关联</li>
<li>同样的技术可以扩展到其他 slot-filling 槽填充 分类</li>
</ul>
</li>
<li>通常后面是命名实体链接/规范化到知识库</li>
</ul>
<p><strong>Named Entity Recognition on word sequences</strong></p>
<p><img alt="1560359650543" src="../imgs/1560359650543.png" /></p>
<p>我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体</p>
<p><strong>Why might NER be hard?</strong></p>
<ul>
<li>很难计算出实体的边界<ul>
<li><img alt="1560359674788" src="../imgs/1560359674788.png" /></li>
<li>第一个实体是 “First National Bank” 还是 “National Bank”</li>
</ul>
</li>
<li>很难知道某物是否是一个实体<ul>
<li>是一所名为“Future School” 的学校，还是这是一所未来的学校？</li>
</ul>
</li>
<li>很难知道未知/新奇实体的类别<ul>
<li><img alt="1560359774508" src="../imgs/1560359774508.png" /></li>
<li>“Zig Ziglar” ?  一个人</li>
</ul>
</li>
<li>实体类是模糊的，依赖于上下文<ul>
<li><img alt="1560359806734" src="../imgs/1560359806734.png" /></li>
<li>这里的“Charles Schwab”  是 PER
    不是 ORG</li>
</ul>
</li>
</ul>
<h3 id="binary-word-window-classification">Binary word window classification<a class="headerlink" href="#binary-word-window-classification" title="Permanent link">&para;</a></h3>
<p>为在上下文中的语言构建分类器</p>
<ul>
<li>一般来说，很少对单个单词进行分类</li>
<li>有趣的问题，如上下文歧义出现</li>
<li>例子：auto-antonyms<ul>
<li>"To sanction" can mean "to permit" or "to punish”</li>
<li>"To seed" can mean "to place seeds" or "to remove seeds"</li>
</ul>
</li>
<li>例子：解决模糊命名实体的链接<ul>
<li>Paris → Paris, France vs. Paris Hilton vs. Paris, Texas</li>
<li>Hathaway → Berkshire Hathaway vs. Anne Hathaway</li>
</ul>
</li>
</ul>
<p><strong>Window classification</strong></p>
<ul>
<li>思想：在**相邻词的上下文窗口**中对一个词进行分类</li>
<li>例如，上下文中一个单词的命名实体分类<ul>
<li>人、地点、组织、没有</li>
</ul>
</li>
<li>在上下文中对单词进行分类的一个简单方法可能是对窗口中的单词向量进行**平均**，并对平均向量进行分类<ul>
<li>问题：<strong>这会丢失位置信息</strong></li>
</ul>
</li>
</ul>
<p><strong>Window classification: Softmax</strong></p>
<ul>
<li>训练softmax分类器对中心词进行分类，方法是在一个窗口内**将中心词周围的词向量串联起来**</li>
<li>例子：在这句话的上下文中对“Paris”进行分类，窗口长度为2</li>
</ul>
<p><img alt="1560360448681" src="../imgs/1560360448681.png" /></p>
<ul>
<li>结果向量 <span><span class="MathJax_Preview">x_{window} = x \in R^{5d}</span><script type="math/tex">x_{window} = x \in R^{5d}</script></span>  是一个列向量</li>
</ul>
<p><strong>Simplest window classifier: Softmax</strong></p>
<p>对于 <span><span class="MathJax_Preview">x = x_{window}</span><script type="math/tex">x = x_{window}</script></span> ，我们可以使用与之前相同的softmax分类器</p>
<p><img alt="1560360599779" src="../imgs/1560360599779.png" /></p>
<ul>
<li>如何更新向量？</li>
<li>简而言之：就像上周那样，求导和优化</li>
</ul>
<p><strong>Binary classification with unnormalized scores</strong></p>
<ul>
<li>
<p>之前的例子：<span><span class="MathJax_Preview">X_{\text { window }}=[\begin{array}{ccc}{\mathrm{X}_{\text { museums }}} &amp; {\mathrm{X}_{\text { in }}} &amp; {\mathrm{X}_{\text { paris }} \quad \mathrm{X}_{\text { are }} \quad \mathrm{X}_{\text { amazing }} ]}\end{array}</span><script type="math/tex">X_{\text { window }}=[\begin{array}{ccc}{\mathrm{X}_{\text { museums }}} & {\mathrm{X}_{\text { in }}} & {\mathrm{X}_{\text { paris }} \quad \mathrm{X}_{\text { are }} \quad \mathrm{X}_{\text { amazing }} ]}\end{array}</script></span></p>
</li>
<li>
<p>假设我们要对中心词是否为一个地点，进行分类</p>
</li>
<li>与word2vec类似，我们将遍历语料库中的所有位置。但这一次，它将受到监督，只有一些位置能够得到高分。</li>
<li>例如，在他们的中心有一个实际的NER Location的位置是“真实的”位置会获得高分</li>
</ul>
<p><strong>Binary classification for NER Location</strong></p>
<ul>
<li>
<p>例子：Not all museums in Paris are amazing</p>
</li>
<li>
<p>这里：一个真正的窗口，以Paris为中心的窗口和所有其他窗口都“损坏”了，因为它们的中心没有指定的实体位置。</p>
<ul>
<li>museums in Paris are amazing</li>
</ul>
</li>
<li>
<p>“损坏”窗口很容易找到，而且有很多：任何中心词没有在我们的语料库中明确标记为NER位置的窗口</p>
<ul>
<li>Not all museums in Paris</li>
</ul>
</li>
</ul>
<p><strong>Neural Network Feed-forward Computation</strong></p>
<p>使用神经激活 <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 简单地给出一个非标准化的分数
$$
score(x)=U^{T} a \in \mathbb{R}
$$
我们用一个三层神经网络计算一个窗口的得分</p>
<ul>
<li><span><span class="MathJax_Preview">s = score("museums  \ in \ Paris \ are \ amazing”)</span><script type="math/tex">s = score("museums  \ in \ Paris \ are \ amazing”)</script></span></li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{array}{l}{s=U^{T} f(W x+b)} \\ {x \in \mathbb{R}^{20 \times 1}, W \in \mathbb{R}^{8 \times 20}, U \in \mathbb{R}^{8 \times 1}}\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}{s=U^{T} f(W x+b)} \\ {x \in \mathbb{R}^{20 \times 1}, W \in \mathbb{R}^{8 \times 20}, U \in \mathbb{R}^{8 \times 1}}\end{array}
</script>
</div>
<p><img alt="1560361207976" src="../imgs/1560361207976.png" /></p>
<p><strong>Main intuition for extra layer</strong></p>
<p>中间层学习输入词向量之间的**非线性交互**</p>
<p>例如：只有当“museum”是第一个向量时，“in”放在第二个位置才重要</p>
<p><strong>The max-margin loss</strong></p>
<p><img alt="1560361550807" src="../imgs/1560361550807.png" /></p>
<ul>
<li>关于训练目标的想法：让真实窗口的得分更高，而破坏窗口的得分更低(直到足够好为止)</li>
<li><span><span class="MathJax_Preview">s = score("museums  \ in \ Paris \ are \ amazing”)</span><script type="math/tex">s = score("museums  \ in \ Paris \ are \ amazing”)</script></span></li>
<li><span><span class="MathJax_Preview">s_c = score("Not \ all \ museums  \ in \ Paris)</span><script type="math/tex">s_c = score("Not \ all \ museums  \ in \ Paris)</script></span></li>
<li>最小化 <span><span class="MathJax_Preview">J=\max \left(0,1-s+s_{c}\right)</span><script type="math/tex">J=\max \left(0,1-s+s_{c}\right)</script></span></li>
<li>
<p>这是不可微的，但它是连续的→我们可以用SGD。</p>
</li>
<li>
<p>每个选项都是连续的</p>
</li>
<li>
<p>单窗口的目标函数为</p>
</li>
</ul>
<div>
<div class="MathJax_Preview">
J=\max \left(0,1-s+s_{c}\right)
</div>
<script type="math/tex; mode=display">
J=\max \left(0,1-s+s_{c}\right)
</script>
</div>
<ul>
<li>每个中心有NER位置的窗口的得分应该比中心没有位置的窗口高1分</li>
</ul>
<p><img alt="1560361673756" src="../imgs/1560361673756.png" /></p>
<ul>
<li>要获得完整的目标函数：为每个真窗口采样几个损坏的窗口。对所有培训窗口求和</li>
<li>
<p>类似于word2vec中的负抽样</p>
</li>
<li>
<p>使用SGD更新参数</p>
<ul>
<li><span><span class="MathJax_Preview">\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)</span><script type="math/tex">\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)</script></span></li>
<li><span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 是 步长或是学习率</li>
</ul>
</li>
<li>如何计算 <span><span class="MathJax_Preview">\nabla_{\theta} J(\theta)</span><script type="math/tex">\nabla_{\theta} J(\theta)</script></span> ？<ul>
<li>手工计算（本课）</li>
<li>算法：反向传播（下一课）</li>
</ul>
</li>
</ul>
<p><strong>Computing Gradients by Hand</strong></p>
<ul>
<li>回顾多元导数</li>
<li>矩阵微积分：完全矢量化的梯度<ul>
<li>比非矢量梯度快得多，也更有用</li>
<li>但做一个非矢量梯度可以是一个很好的实践；以上周的讲座为例</li>
<li><strong>notes</strong> 更详细地涵盖了这些材料</li>
</ul>
</li>
</ul>
<h3 id="gradients">Gradients<a class="headerlink" href="#gradients" title="Permanent link">&para;</a></h3>
<ul>
<li>给定一个函数，有1个输出和1个输入</li>
</ul>
<div>
<div class="MathJax_Preview">
f(x) = x^3
</div>
<script type="math/tex; mode=display">
f(x) = x^3
</script>
</div>
<ul>
<li>斜率是它的导数</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{d f}{d x}=3 x^{2}
</div>
<script type="math/tex; mode=display">
\frac{d f}{d x}=3 x^{2}
</script>
</div>
<ul>
<li>给定一个函数，有1个输出和 n 个输入</li>
</ul>
<div>
<div class="MathJax_Preview">
f(\boldsymbol{x})=f\left(x_{1}, x_{2}, \ldots, x_{n}\right)
</div>
<script type="math/tex; mode=display">
f(\boldsymbol{x})=f\left(x_{1}, x_{2}, \ldots, x_{n}\right)
</script>
</div>
<ul>
<li>梯度是关于每个输入的偏导数的向量</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{\partial f}{\partial \boldsymbol{x}}=\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{n}}\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial \boldsymbol{x}}=\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{n}}\right]
</script>
</div>
<p><strong>Jacobian Matrix: Generalization of the Gradient</strong></p>
<ul>
<li>给定一个函数，有 m 个输出和 n 个输入</li>
</ul>
<div>
<div class="MathJax_Preview">
\boldsymbol{f}(\boldsymbol{x})=\left[f_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right), \ldots, f_{m}\left(x_{1}, x_{2}, \ldots, x_{n}\right)\right]
</div>
<script type="math/tex; mode=display">
\boldsymbol{f}(\boldsymbol{x})=\left[f_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right), \ldots, f_{m}\left(x_{1}, x_{2}, \ldots, x_{n}\right)\right]
</script>
</div>
<ul>
<li>其雅可比矩阵是一个<span><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span>的偏导矩阵</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}=\left[\begin{array}{ccc}{\frac{\partial f_{1}}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{1}}{\partial x_{n}}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{m}}{\partial x_{n}}}\end{array}\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}=\left[\begin{array}{ccc}{\frac{\partial f_{1}}{\partial x_{1}}} & {\cdots} & {\frac{\partial f_{1}}{\partial x_{n}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{1}}} & {\cdots} & {\frac{\partial f_{m}}{\partial x_{n}}}\end{array}\right]
</script>
</div>
<div>
<div class="MathJax_Preview">
\left(\frac{\partial f}{\partial x}\right)_{i j}=\frac{\partial f_{i}}{\partial x_{j}}
</div>
<script type="math/tex; mode=display">
\left(\frac{\partial f}{\partial x}\right)_{i j}=\frac{\partial f_{i}}{\partial x_{j}}
</script>
</div>
<p><strong>Chain Rule</strong></p>
<p>对于单变量函数：乘以导数
$$
\begin{array}{l}{z=3 y} \ {y=x^{2}} \ {\frac{d z}{d x}=\frac{d z}{d y} \frac{d y}{d x}=(3)(2 x)=6 x}\end{array}
$$
对于一次处理多个变量：乘以雅可比矩阵
$$
\begin{array}{l}{\textbf{h}=f(\textbf{z})} \ {\textbf{z}=\textbf{W} \textbf{x}+\textbf{b}} \ {\frac{\partial \textbf{h}}{\partial \textbf{x}}=\frac{\partial \textbf{h}}{\partial \textbf{z}} \frac{\partial \textbf{z}}{\partial \textbf{x}}=\dots}\end{array}
$$
<strong>Example Jacobian: Elementwise activation Function</strong></p>
<p><span><span class="MathJax_Preview">h=f(z)</span><script type="math/tex">h=f(z)</script></span> , <span><span class="MathJax_Preview">\frac{\partial \textbf{h}}{\partial \textbf{z}} = ?, \textbf{h},\textbf{z} \in \mathbb{R}^{n}</span><script type="math/tex">\frac{\partial \textbf{h}}{\partial \textbf{z}} = ?, \textbf{h},\textbf{z} \in \mathbb{R}^{n}</script></span>  </p>
<p>由于使用的是 element-wise，所以 <span><span class="MathJax_Preview">h_{i}=f\left(z_{i}\right)</span><script type="math/tex">h_{i}=f\left(z_{i}\right)</script></span></p>
<p>函数有n个输出和n个输入 → n×n 的雅可比矩阵</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}\left(\frac{\partial h}{\partial z}\right)_{i j} &amp;=\frac{\partial h_{i}}{\partial z_{j}}=\frac{\partial}{\partial z_{j}} f\left(z_{i}\right), \text{definition of Jacobian} \\ &amp;=\left\{\begin{array}{ll}{f^{\prime}\left(z_{i}\right)} &amp; {\text { if } i=j} \\ {0} &amp; {\text { if otherwise }} , \text{regular 1-variable derivative} \end{array}\right.\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}\left(\frac{\partial h}{\partial z}\right)_{i j} &=\frac{\partial h_{i}}{\partial z_{j}}=\frac{\partial}{\partial z_{j}} f\left(z_{i}\right), \text{definition of Jacobian} \\ &=\left\{\begin{array}{ll}{f^{\prime}\left(z_{i}\right)} & {\text { if } i=j} \\ {0} & {\text { if otherwise }} , \text{regular 1-variable derivative} \end{array}\right.\end{aligned}
</script>
</div>
<div>
<div class="MathJax_Preview">
\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}= \left(\begin{array}{ccc}{f^{\prime}\left(z_{1}\right)} &amp; { } &amp; {0} \\ {} &amp; {\ddots} &amp; { } \\ {0} &amp; { } &amp; {f^{\prime}\left(z_{n}\right)}\end{array}\right)=\operatorname{diag}\left(\boldsymbol{f}^{\prime}(\boldsymbol{z})\right)
</div>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}= \left(\begin{array}{ccc}{f^{\prime}\left(z_{1}\right)} & { } & {0} \\ {} & {\ddots} & { } \\ {0} & { } & {f^{\prime}\left(z_{n}\right)}\end{array}\right)=\operatorname{diag}\left(\boldsymbol{f}^{\prime}(\boldsymbol{z})\right)
</script>
</div>
<p><strong>Other Jacobians</strong>
$$
\begin{array}{l}{\frac{\partial}{\partial \boldsymbol{x}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{W}} \ {\frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{I} \text { (Identity matrix) }} \ \frac{\partial}{\partial \boldsymbol{u}}\left(\boldsymbol{u}^{T} \boldsymbol{h}\right)=\boldsymbol{h}^{\boldsymbol{T}} \end{array}
$$
这是正确的雅可比矩阵。稍后我们将讨论“形状约定”；用它则答案是 <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> 。</p>
<p><strong>Back to our Neural Net!</strong></p>
<p><img alt="1560363626037" src="../imgs/1560363626037.png" /></p>
<p>如何计算 <span><span class="MathJax_Preview">\frac{\partial s}{\partial b}</span><script type="math/tex">\frac{\partial s}{\partial b}</script></span> ？</p>
<p>实际上，我们关心的是损失的梯度，但是为了简单起见，我们将计算分数的梯度</p>
<p><strong>Break up equations into simple pieces</strong></p>
<p><img alt="1560363713598" src="../imgs/1560363713598.png" /></p>
<p><strong>Apply the chain rule</strong>
$$
\begin{array}{l}{s=\boldsymbol{u}^{T} \boldsymbol{h}} \ {\boldsymbol{h}=f(\boldsymbol{z})} \ {\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}} \ {\boldsymbol{x}} \text{ (input) }\end{array}
$$</p>
<div>
<div class="MathJax_Preview">
\frac{\partial s}{\partial \boldsymbol{b}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}
</div>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial \boldsymbol{b}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}
</script>
</div>
<p><img alt="1560363929934" src="../imgs/1560363929934.png" /></p>
<p>如何计算 <span><span class="MathJax_Preview">\frac{\partial s}{\partial \textbf{W}}</span><script type="math/tex">\frac{\partial s}{\partial \textbf{W}}</script></span> ？
$$
\begin{aligned} \frac{\partial s}{\partial \boldsymbol{W}} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \ \frac{\partial s}{\partial \boldsymbol{b}} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} \end{aligned}
$$
前两项是重复的，无须重复计算
$$
\begin{aligned} \frac{\partial s}{\partial \boldsymbol{W}} &amp;=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \ \frac{\partial s}{\partial \boldsymbol{b}} &amp;=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}=\boldsymbol{\delta} \ \boldsymbol{\delta} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}=\boldsymbol{u}^{T} \circ f^{\prime}(\boldsymbol{z}) \end{aligned}
$$
其中，<span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> 是局部误差符号</p>
<p><strong>Derivative with respect to Matrix: Output shape</strong></p>
<ul>
<li><span><span class="MathJax_Preview">\boldsymbol{W} \in \mathbb{R}^{n \times m}</span><script type="math/tex">\boldsymbol{W} \in \mathbb{R}^{n \times m}</script></span> ，<span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}</script></span> 的形状是</li>
<li>1个输出，<span><span class="MathJax_Preview">n \times m</span><script type="math/tex">n \times m</script></span> 个输入：1 × nm 的雅可比矩阵？<ul>
<li>不方便更新参数 <span><span class="MathJax_Preview">\theta^{n e w}=\theta^{o l d}-\alpha \nabla_{\theta} J(\theta)</span><script type="math/tex">\theta^{n e w}=\theta^{o l d}-\alpha \nabla_{\theta} J(\theta)</script></span></li>
</ul>
</li>
<li>而是遵循惯例：导数的形状是参数的形状 （形状约定）<ul>
<li><span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}</script></span> 的形状是 <span><span class="MathJax_Preview">n \times m</span><script type="math/tex">n \times m</script></span> </li>
</ul>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\left[\begin{array}{ccc}{\frac{\partial s}{\partial W_{11}}} &amp; {\cdots} &amp; {\frac{\partial s}{\partial W_{1 m}}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial s}{\partial W_{n 1}}} &amp; {\cdots} &amp; {\frac{\partial s}{\partial W_{n m}}}\end{array}\right]
</div>
<script type="math/tex; mode=display">
\left[\begin{array}{ccc}{\frac{\partial s}{\partial W_{11}}} & {\cdots} & {\frac{\partial s}{\partial W_{1 m}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial s}{\partial W_{n 1}}} & {\cdots} & {\frac{\partial s}{\partial W_{n m}}}\end{array}\right]
</script>
</div>
<p><strong>Derivative with respect to Matrix</strong></p>
<ul>
<li><span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}</script></span><ul>
<li><span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> 将出现在我们的答案中</li>
<li>另一项应该是 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> ，因为 <span><span class="MathJax_Preview">\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}</span><script type="math/tex">\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}</script></span> </li>
</ul>
</li>
<li>这表明 <span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}</script></span></li>
</ul>
<p><img alt="1560364755148" src="../imgs/1560364755148.png" /></p>
<p><strong>Why the Transposes?</strong>
$$
\begin{array}{l}{\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \quad \boldsymbol{x}^{T}} \ {[n \times m]} {[n \times 1]} {[1 \times m]}\end{array}
$$</p>
<ul>
<li>粗糙的回答是：这样就可以解决尺寸问题了<ul>
<li>检查工作的有用技巧</li>
</ul>
</li>
<li>课堂讲稿中有完整的解释<ul>
<li>每个输入到每个输出——你得到的是外部积</li>
</ul>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}=\left[\begin{array}{c}{\delta_{1}} \\ {\vdots} \\ {\delta_{n}}\end{array}\right]\left[x_{1}, \ldots, x_{m}\right]=\left[\begin{array}{ccc}{\delta_{1} x_{1}} &amp; {\dots} &amp; {\delta_{1} x_{m}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\delta_{n} x_{1}} &amp; {\dots} &amp; {\delta_{n} x_{m}}\end{array}\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}=\left[\begin{array}{c}{\delta_{1}} \\ {\vdots} \\ {\delta_{n}}\end{array}\right]\left[x_{1}, \ldots, x_{m}\right]=\left[\begin{array}{ccc}{\delta_{1} x_{1}} & {\dots} & {\delta_{1} x_{m}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\delta_{n} x_{1}} & {\dots} & {\delta_{n} x_{m}}\end{array}\right]
</script>
</div>
<p><strong>What shape should derivatives be?</strong></p>
<ul>
<li><span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{b}}=\boldsymbol{h}^{T} \circ f^{\prime}(\boldsymbol{z})</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{b}}=\boldsymbol{h}^{T} \circ f^{\prime}(\boldsymbol{z})</script></span> 是行向量<ul>
<li>但是习惯上说梯度应该是一个列向量因为 <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 是一个列向量</li>
</ul>
</li>
<li>
<p>雅可比矩阵形式(这使得链式法则很容易)和形状约定(这使得SGD很容易实现)之间的分歧</p>
<ul>
<li>我们希望答案遵循形状约定</li>
<li>但是雅可比矩阵形式对于计算答案很有用</li>
</ul>
</li>
<li>
<p>两个选择</p>
<ul>
<li>尽量使用雅可比矩阵形式，最后按照约定进行整形<ul>
<li>我们刚刚做的。但最后转置 <span><span class="MathJax_Preview">\frac{\partial s}{\partial \boldsymbol{b}}</span><script type="math/tex">\frac{\partial s}{\partial \boldsymbol{b}}</script></span> 使导数成为列向量，得到 <span><span class="MathJax_Preview">\delta ^ T</span><script type="math/tex">\delta ^ T</script></span></li>
</ul>
</li>
<li>始终遵循惯例<ul>
<li>查看维度，找出何时转置 和/或 重新排序项。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>反向传播</p>
<ul>
<li>算法高效地计算梯度</li>
<li>将我们刚刚手工完成的转换成算法</li>
<li>用于深度学习软件框架(TensorFlow, PyTorch, Chainer, etc.)</li>
</ul>
<h2 id="notes-03-neural-networks-backpropagation">Notes 03 Neural Networks, Backpropagation<a class="headerlink" href="#notes-03-neural-networks-backpropagation" title="Permanent link">&para;</a></h2>
<p><strong>Keyphrases</strong>: Neural networks.Forward computation.Backward.propagation.Neuron Units.Max-margin Loss.Gradient checks.Xavier parameter initialization.Learning rates.Adagrad.</p>
<p><strong>概述</strong>：这组笔记介绍了单层和多层神经网络，以及如何将它们用于分类目的。然后我们讨论如何使用一种称为反向传播的分布式梯度下降技术来训练它们。我们将看到如何使用链式法则按顺序进行参数更新。在对神经网络进行严格的数学讨论之后，我们将讨论一些训练神经网络的实用技巧和技巧，包括:神经元单元(非线性)、梯度检查、Xavier参数初始化、学习率、Adagrad等。最后,我们将鼓励使用递归神经网络作为语言模型。</p>
<h3 id="1-neural-networks-foundations">1 Neural Networks: Foundations<a class="headerlink" href="#1-neural-networks-foundations" title="Permanent link">&para;</a></h3>
<p>在前面的讨论中认为，因为大部分数据是线性不可分的所以需要非线性分类器，不然的话线性分类器在这些数据上的表现是有限的。神经网络就是如下图所示的一类具有非线性决策分界的分类器。现在我们知道神经网络创建的决策边界，让我们看看这是如何创建的。</p>
<p>神经网络是受生物学启发的分类器，这就是为什么它们经常被称为“人工神经网络”，以区别于有机类。然而，在现实中，人类神经网络比人工神经网络更有能力、更复杂，因此通常最好不要在两者之间画太多的相似点。</p>
<p><img alt="1560405499464" src="../imgs/1560405499464.png" /></p>
<h4 id="11-a-neuron">1.1 A Neuron<a class="headerlink" href="#11-a-neuron" title="Permanent link">&para;</a></h4>
<p>神经元是一个通用的计算单元，它接受 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 个输入并产生一个输出。不同的神经元根据它们不同的参数（一般认为是神经元的权值）会有不同的输出。对神经元来说一个常见的选择是 <span><span class="MathJax_Preview">sigmoid</span><script type="math/tex">sigmoid</script></span> ，或者称为“二元逻辑回归”单元。这种神经元以 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 维的向量作为输入，然后计算出一个激活标量（输出） <span><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> 。该神经元还与一个 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 维的权重向量 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 和一个偏置标量 <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 相关联。这个神经元的输出是</p>
<div>
<div class="MathJax_Preview">
a=\frac{1}{1+exp(-(w^{T}x+b))}
</div>
<script type="math/tex; mode=display">
a=\frac{1}{1+exp(-(w^{T}x+b))}
</script>
</div>
<p>我们也可以把上面公式中的权值和偏置项结合在一起：</p>
<div>
<div class="MathJax_Preview">
a=\frac{1}{1+exp(-[w^{T}\;\;x]\cdot [x\;\;1])}
</div>
<script type="math/tex; mode=display">
a=\frac{1}{1+exp(-[w^{T}\;\;x]\cdot [x\;\;1])}
</script>
</div>
<p>这个公式可以以下图的形式可视化</p>
<p><img alt="1560405918139" src="../imgs/1560405918139.png" /></p>
<p>神经元是神经网络的基本组成部分。我们将看到神经元可以是许多允许非线性在网络中积累的函数之一。</p>
<h4 id="12-a-single-layer-of-neurons">1.2 A Single Layer of Neurons<a class="headerlink" href="#12-a-single-layer-of-neurons" title="Permanent link">&para;</a></h4>
<p>我们将上述思想扩展到多个神经元，考虑输入 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 作为多个这样的神经元的输入，如下图所示。</p>
<p><img alt="1560405984674" src="../imgs/1560405984674.png" /></p>
<p>如果我们定义不同的神经元的权值为 <span><span class="MathJax_Preview">\{w^{(1)},...,w^{(m)}\}</span><script type="math/tex">\{w^{(1)},...,w^{(m)}\}</script></span> 、偏置为 <span><span class="MathJax_Preview">\{b_{1},...,b_{m}\}</span><script type="math/tex">\{b_{1},...,b_{m}\}</script></span> 和相对应的激活输出为 <span><span class="MathJax_Preview">\{a_{1},...,a_{m}\}</span><script type="math/tex">\{a_{1},...,a_{m}\}</script></span> ：</p>
<div>
<div class="MathJax_Preview">
         \begin{matrix}         a_{1} =\frac{1}{1+exp(-(w^{(1)T}x+b))} \\ \vdots \\ a_{m} =\frac{1}{1+exp(-(w^{(m)T}x+b))}          \end{matrix}    \\
</div>
<script type="math/tex; mode=display">
         \begin{matrix}         a_{1} =\frac{1}{1+exp(-(w^{(1)T}x+b))} \\ \vdots \\ a_{m} =\frac{1}{1+exp(-(w^{(m)T}x+b))}          \end{matrix}    \\
</script>
</div>
<p>让我们定义简化公式以便于更好地表达复杂的网络：</p>
<div>
<div class="MathJax_Preview">
         \sigma(z) =         \begin{bmatrix}         \frac{1}{1+exp(z_{1})} \\ \vdots \\ \frac{1}{1+exp(z_{m})}         \end{bmatrix} \\         b =         \begin{bmatrix}         b_{1} \\ \vdots \\ b_{m}         \end{bmatrix} \in \mathbb{R}^{m} \\         W =         \begin{bmatrix}         -\;\;w^{(1)T}\;\;- \\ \cdots \\ -\;\;w^{(m)T}\;\;-         \end{bmatrix} \in \mathbb{R}^{m\times n}  \\
</div>
<script type="math/tex; mode=display">
         \sigma(z) =         \begin{bmatrix}         \frac{1}{1+exp(z_{1})} \\ \vdots \\ \frac{1}{1+exp(z_{m})}         \end{bmatrix} \\         b =         \begin{bmatrix}         b_{1} \\ \vdots \\ b_{m}         \end{bmatrix} \in \mathbb{R}^{m} \\         W =         \begin{bmatrix}         -\;\;w^{(1)T}\;\;- \\ \cdots \\ -\;\;w^{(m)T}\;\;-         \end{bmatrix} \in \mathbb{R}^{m\times n}  \\
</script>
</div>
<p>我们现在可以将缩放和偏差的输出写成：</p>
<div>
<div class="MathJax_Preview">
\mathbf{z}=\mathbf{W}\mathbf{x}+\mathbf{b}  \\
</div>
<script type="math/tex; mode=display">
\mathbf{z}=\mathbf{W}\mathbf{x}+\mathbf{b}  \\
</script>
</div>
<p>激活函数 sigmoid 可以变为如下形式：</p>
<div>
<div class="MathJax_Preview">
         \begin{bmatrix}         a_{1} \\ \vdots \\ a_{m}         \end{bmatrix} = \sigma(\mathbf{z}) = \sigma(\mathbf{W}\mathbf{x}+\mathbf{b})  \\
</div>
<script type="math/tex; mode=display">
         \begin{bmatrix}         a_{1} \\ \vdots \\ a_{m}         \end{bmatrix} = \sigma(\mathbf{z}) = \sigma(\mathbf{W}\mathbf{x}+\mathbf{b})  \\
</script>
</div>
<p>那么这些激活的作用是什么呢？我们可以把这些激活看作是一些加权特征组合存在的指标。然后，我们可以使用这些激活的组合来执行分类任务。</p>
<h4 id="13-feed-forward-computation">1.3 Feed-forward Computation<a class="headerlink" href="#13-feed-forward-computation" title="Permanent link">&para;</a></h4>
<p>到目前为止我们知道一个输入向量 <span><span class="MathJax_Preview">x\in \mathbb{R}^{n}</span><script type="math/tex">x\in \mathbb{R}^{n}</script></span> 可以经过一层 <span><span class="MathJax_Preview">sigmoid</span><script type="math/tex">sigmoid</script></span> 单元的变换得到激活输出 <span><span class="MathJax_Preview">a\in \mathbb{R}^{m}</span><script type="math/tex">a\in \mathbb{R}^{m}</script></span> 。但是这么做的直觉是什么呢？让我们考虑一个 NLP 中的命名实体识别问题作为例子：</p>
<div>
<div class="MathJax_Preview">
Museums\;in\;Paris \;are\;amazing  \\
</div>
<script type="math/tex; mode=display">
Museums\;in\;Paris \;are\;amazing  \\
</script>
</div>
<p>这里我们想判断中心词  <span><span class="MathJax_Preview">Paris</span><script type="math/tex">Paris</script></span> 是不是以命名实体。在这种情况下，我们很可能不仅想要捕捉窗口中单词的单词向量，还想要捕捉单词之间的一些其他交互，以便进行分类。例如，可能只有 <span><span class="MathJax_Preview">Museums</span><script type="math/tex">Museums</script></span> 是第一个单词和 <span><span class="MathJax_Preview">in</span><script type="math/tex">in</script></span> 是第二个单词的时候， <span><span class="MathJax_Preview">Paris</span><script type="math/tex">Paris</script></span> 才是命名实体。这样的非线性决策通常不能被直接提供给Softmax函数的输入捕获，而是需要第1.2节中讨论的中间层进行评分。因此，我们可以使用另一个矩阵 <span><span class="MathJax_Preview">\mathbf{U} \in \mathbb{R}^{m \times 1}</span><script type="math/tex">\mathbf{U} \in \mathbb{R}^{m \times 1}</script></span> 与激活输出计算得到未归一化的得分用于分类任务：</p>
<div>
<div class="MathJax_Preview">
s=\mathbf{U}^{T}a=\mathbf{U}^{T}f(Wx+b)
</div>
<script type="math/tex; mode=display">
s=\mathbf{U}^{T}a=\mathbf{U}^{T}f(Wx+b)
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> 是激活函数（例如 sigmoid 函数）。</p>
<p><strong>维度分析</strong>：如果我们使用 4 维的词向量来表示每个单词并使用 5 个词的窗口，则输入是 <span><span class="MathJax_Preview">x\in \mathbb{R}^{20}</span><script type="math/tex">x\in \mathbb{R}^{20}</script></span> 。如果我们在隐藏层使用 8 个 sigmoid 单元和从激活函数中生成一个分数输出，其中 <span><span class="MathJax_Preview">W\in \mathbb{R}^{8\times 20}</span><script type="math/tex">W\in \mathbb{R}^{8\times 20}</script></span> ， <span><span class="MathJax_Preview">b\in \mathbb{R}^{8}</span><script type="math/tex">b\in \mathbb{R}^{8}</script></span> ， <span><span class="MathJax_Preview">U\in \mathbb{R}^{8\times 1}</span><script type="math/tex">U\in \mathbb{R}^{8\times 1}</script></span>， <span><span class="MathJax_Preview">s\in \mathbb{R}</span><script type="math/tex">s\in \mathbb{R}</script></span> 。</p>
<p><img alt="1560407221438" src="../imgs/1560407221438.png" /></p>
<h4 id="14-maximum-margin-objective-function">1.4 Maximum Margin Objective Function<a class="headerlink" href="#14-maximum-margin-objective-function" title="Permanent link">&para;</a></h4>
<p>类似很多的机器学习模型，神经网络需要一个优化目标函数，一个我们想要最小化或最大化的误差。这里我们讨论一个常用的误差度量方法：<strong>maximum margin objective</strong> 最大间隔目标函数。使用这个目标函数的背后的思想是保证对“真”标签数据的计算得分要比“假”标签数据的计算得分要高。</p>
<p>回到前面的例子，如果我们令“真”标签窗口 <span><span class="MathJax_Preview">\text{Museums in Paris are amazing}</span><script type="math/tex">\text{Museums in Paris are amazing}</script></span> 的计算得分为 s ，令“假”标签窗口 <span><span class="MathJax_Preview">\text{Not all museums in Paris}</span><script type="math/tex">\text{Not all museums in Paris}</script></span> 的计算得分为 <span><span class="MathJax_Preview">s_{c}</span><script type="math/tex">s_{c}</script></span> （下标 <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> 表示这个这个窗口 corrupt ）</p>
<p>然后，我们对目标函数最大化 <span><span class="MathJax_Preview">(s-s_{c})</span><script type="math/tex">(s-s_{c})</script></span> 或者最小化 <span><span class="MathJax_Preview">(s_{c}-s)</span><script type="math/tex">(s_{c}-s)</script></span> 。然而，我们修改目标函数来保证误差仅在 <span><span class="MathJax_Preview">s_{c} &gt; s \Rightarrow  (s_{c}-s) &gt; 0</span><script type="math/tex">s_{c} > s \Rightarrow  (s_{c}-s) > 0</script></span> 才进行计算。这样做的直觉是，我们只关心“正确”数据点的得分高于“错误”数据点，其余的都不重要。因此，当 <span><span class="MathJax_Preview">s_{c} &gt; s</span><script type="math/tex">s_{c} > s</script></span> 则误差为 <span><span class="MathJax_Preview">(s_{c}-s)</span><script type="math/tex">(s_{c}-s)</script></span> ，否则为 0 。因此，我们的优化的目标函数现在为：</p>
<div>
<div class="MathJax_Preview">
minimize\;J=max\,(s_{c}-s,0)
</div>
<script type="math/tex; mode=display">
minimize\;J=max\,(s_{c}-s,0)
</script>
</div>
<p>然而，上面的优化目标函数是有风险的，因为它不能创造一个安全的间隔。我们希望“真”数据要比“假”数据的得分大于某个正的间隔 <span><span class="MathJax_Preview">\Delta</span><script type="math/tex">\Delta</script></span> 。换而言之，我们想要误差在 <span><span class="MathJax_Preview">(s-s_{c} &lt; \Delta)</span><script type="math/tex">(s-s_{c} < \Delta)</script></span> 就开始计算，而不是当 <span><span class="MathJax_Preview">(s-s_{c} &lt; 0)</span><script type="math/tex">(s-s_{c} < 0)</script></span> 时就计算。因此，我们修改优化目标函数为：</p>
<div>
<div class="MathJax_Preview">
minimize\;J=max\,(\Delta+s_{c}-s,0)
</div>
<script type="math/tex; mode=display">
minimize\;J=max\,(\Delta+s_{c}-s,0)
</script>
</div>
<p>我们可以把这个间隔缩放使得 <span><span class="MathJax_Preview">\Delta=1</span><script type="math/tex">\Delta=1</script></span> ，让其他参数在优化过程中自动进行调整，并且不会影响模型的表现。如果想更多地了解这方面，可以去读一下 <span><span class="MathJax_Preview">SVM</span><script type="math/tex">SVM</script></span> 中的函数间隔和几何间隔中的相关内容。最后，我们定义在所有训练窗口上的优化目标函数为：</p>
<div>
<div class="MathJax_Preview">
minimize\;J=max\,(1+s_{c}-s,0)
</div>
<script type="math/tex; mode=display">
minimize\;J=max\,(1+s_{c}-s,0)
</script>
</div>
<p>按照上面的公式有，<span><span class="MathJax_Preview">s_{c}=\mathbf{U}^{T}f(Wx_{c}+b)</span><script type="math/tex">s_{c}=\mathbf{U}^{T}f(Wx_{c}+b)</script></span> 和 <span><span class="MathJax_Preview">s=\mathbf{U}^{T}f(Wx+b)</span><script type="math/tex">s=\mathbf{U}^{T}f(Wx+b)</script></span>。</p>
<blockquote>
<p>最大边际目标函数通常与支持向量机一起使用</p>
</blockquote>
<h4 id="15-training-with-backpropagation-elemental">1.5 Training with Backpropagation – Elemental<a class="headerlink" href="#15-training-with-backpropagation-elemental" title="Permanent link">&para;</a></h4>
<p>在这部分我们讨论当1.4节中讨论的损失函数 <span><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> 为正时，模型中不同参数时是如何训练的。如果损失为 0 时，那么不需要再更新参数。我们一般使用梯度下降（或者像 SGD 这样的变体）来更新参数，所以要知道在更新公式中需要的任意参数的梯度信息：</p>
<div>
<div class="MathJax_Preview">
\theta^{(t+1)}=\theta^{(t)}-\alpha\nabla_{\theta^{(t)}}J 
</div>
<script type="math/tex; mode=display">
\theta^{(t+1)}=\theta^{(t)}-\alpha\nabla_{\theta^{(t)}}J 
</script>
</div>
<p>反向传播是一种利用微分链式法则来计算模型上任意参数的损失梯度的方法。为了更进一步理解反向传播，我们先看下图中的一个简单的网络  </p>
<p><img alt="1560408903195" src="../imgs/1560408903195.png" /></p>
<p>这里我们使用只有单个隐藏层和单个输出单元的神经网络。现在让我们先建立一些符号定义：</p>
<ul>
<li><span><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> 是神经网络的输入</li>
<li><span><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span> 是神经网络的输出</li>
<li>每层（包括输入和输出层）的神经元都接收一个输入和生成一个输出。第 <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> 层的第 <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> 个神经元接收标量输入 <span><span class="MathJax_Preview">z_{j}^{(k)}</span><script type="math/tex">z_{j}^{(k)}</script></span> 和生成一个标量激活输出 <span><span class="MathJax_Preview">a_{j}^{(k)}</span><script type="math/tex">a_{j}^{(k)}</script></span> </li>
<li>我们把 <span><span class="MathJax_Preview">z_{j}^{(k)}</span><script type="math/tex">z_{j}^{(k)}</script></span> 计算出的反向传播误差定义为 <span><span class="MathJax_Preview">\delta_{j}^{(k)}</span><script type="math/tex">\delta_{j}^{(k)}</script></span> </li>
<li>第 <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> 层是输入层，而不是第 <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> 个隐藏层。对输入层而言， <span><span class="MathJax_Preview">x_{j}^{(k)}=z_{j}^{(k)}=a_{j}^{(k)}</span><script type="math/tex">x_{j}^{(k)}=z_{j}^{(k)}=a_{j}^{(k)}</script></span> </li>
<li><span><span class="MathJax_Preview">W^{(k)}</span><script type="math/tex">W^{(k)}</script></span> 是将第 k 层的输出映射到第 <span><span class="MathJax_Preview">k+1</span><script type="math/tex">k+1</script></span> 层的输入的转移矩阵，因此将这个新的符号用在 <span><span class="MathJax_Preview">Section\;1.3</span><script type="math/tex">Section\;1.3</script></span> 中的例子 <span><span class="MathJax_Preview">W^{(1)}=W</span><script type="math/tex">W^{(1)}=W</script></span> 和 <span><span class="MathJax_Preview">W^{(2)}=U</span><script type="math/tex">W^{(2)}=U</script></span> 。</li>
</ul>
<p><strong>现在开始反向传播</strong>：假设损失函数 <span><span class="MathJax_Preview">J=(1+s_{c}-s)</span><script type="math/tex">J=(1+s_{c}-s)</script></span> 为正值，我们想更新参数 <span><span class="MathJax_Preview">W_{14}^{(1)}</span><script type="math/tex">W_{14}^{(1)}</script></span> ，我们看到 <span><span class="MathJax_Preview">W_{14}^{(1)}</span><script type="math/tex">W_{14}^{(1)}</script></span> 只参与了 <span><span class="MathJax_Preview">z_{1}^{(2)}</span><script type="math/tex">z_{1}^{(2)}</script></span> 和 <span><span class="MathJax_Preview">a_{1}^{(2)}</span><script type="math/tex">a_{1}^{(2)}</script></span> 的计算。这点对于理解反向传播是非常重要的——<strong>反向传播的梯度只受它们所贡献的值的影响</strong>。 <span><span class="MathJax_Preview">a_{1}^{(2)}</span><script type="math/tex">a_{1}^{(2)}</script></span> 在随后的前向计算中和 <span><span class="MathJax_Preview">W_{1}^{(2)}</span><script type="math/tex">W_{1}^{(2)}</script></span> 相乘计算得分。我们可以从最大间隔损失看到：
$$
\frac{\partial J}{\partial s}=-\frac{\partial J}{\partial s_{c}}=-1 \
$$</p>
<p>为了简化我们只分析 <span><span class="MathJax_Preview">\frac{\partial s}{\partial W_{ij}^{(1)}}</span><script type="math/tex">\frac{\partial s}{\partial W_{ij}^{(1)}}</script></span> 。所以，</p>
<div>
<div class="MathJax_Preview">
\begin{eqnarray}  \frac{\partial s}{\partial W_{ij}^{(1)}} &amp;=&amp; \frac{\partial W^{(2)}a^{(2)}}{\partial W_{ij}^{(1)}}=\frac{\partial W_{i}^{(2)}a_{i}^{(2)}}{\partial W_{ij}^{(1)}}=W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ \Rightarrow W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial W_{ij}^{(1)}}&amp;=&amp; W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &amp;=&amp; W_{i}^{(2)}\frac{f(z_{i}^{(2)})}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial}{\partial W_{ij}^{(1)}}(b_{i}^{(1)}+a_{1}^{(1)}W_{i1}^{(1)}+a_{2}^{(1)}W_{i2}^{(1)}+a_{3}^{(1)}W_{i3}^{(1)}+a_{4}^{(1)}W_{i4}^{(1)}) \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial}{\partial W_{ij}^{(1)}}(b_{i}^{(1)}+\sum_{k}a_{k}^{(1)}W_{ik}^{(1)}) \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})a_{j}^{(1)} \nonumber \\ &amp;=&amp; \delta_{i}^{(2)}\cdot a_{j}^{(1)} \nonumber \end{eqnarray}    \\
</div>
<script type="math/tex; mode=display">
\begin{eqnarray}  \frac{\partial s}{\partial W_{ij}^{(1)}} &=& \frac{\partial W^{(2)}a^{(2)}}{\partial W_{ij}^{(1)}}=\frac{\partial W_{i}^{(2)}a_{i}^{(2)}}{\partial W_{ij}^{(1)}}=W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ \Rightarrow W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial W_{ij}^{(1)}}&=& W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &=& W_{i}^{(2)}\frac{f(z_{i}^{(2)})}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &=& W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &=& W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial}{\partial W_{ij}^{(1)}}(b_{i}^{(1)}+a_{1}^{(1)}W_{i1}^{(1)}+a_{2}^{(1)}W_{i2}^{(1)}+a_{3}^{(1)}W_{i3}^{(1)}+a_{4}^{(1)}W_{i4}^{(1)}) \nonumber \\ &=& W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial}{\partial W_{ij}^{(1)}}(b_{i}^{(1)}+\sum_{k}a_{k}^{(1)}W_{ik}^{(1)}) \nonumber \\ &=& W_{i}^{(2)}f'(z_{i}^{(2)})a_{j}^{(1)} \nonumber \\ &=& \delta_{i}^{(2)}\cdot a_{j}^{(1)} \nonumber \end{eqnarray}    \\
</script>
</div>
<p>其中， <span><span class="MathJax_Preview">a^{(1)}</span><script type="math/tex">a^{(1)}</script></span> 指输入层的输入。我们可以看到梯度计算最后可以简化为 <span><span class="MathJax_Preview">\delta_{i}^{(2)}\cdot a_{j}^{(1)}</span><script type="math/tex">\delta_{i}^{(2)}\cdot a_{j}^{(1)}</script></span> ，其中 <span><span class="MathJax_Preview">\delta_{i}^{(2)}</span><script type="math/tex">\delta_{i}^{(2)}</script></span> 本质上是第 <span><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span> 层中第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 个神经元反向传播的误差。 <span><span class="MathJax_Preview">a_{j}^{(1)}</span><script type="math/tex">a_{j}^{(1)}</script></span> 与 <span><span class="MathJax_Preview">W_{ij}</span><script type="math/tex">W_{ij}</script></span> 相乘的结果，输入第 <span><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span> 层中第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 个神经元中。</p>
<p>我们以下图为例，让我们从“误差共享/分配”的来阐释一下反向传播，现在我们要更新 <span><span class="MathJax_Preview">W_{14}^{(1)}</span><script type="math/tex">W_{14}^{(1)}</script></span> ：</p>
<p><img alt="1560415341614" src="../imgs/1560415341614.png" /></p>
<ol>
<li>我们从 <span><span class="MathJax_Preview">a_{1}^{(3)}</span><script type="math/tex">a_{1}^{(3)}</script></span> 的 1 的误差信号开始反向传播。</li>
<li>然后我们把误差与将 <span><span class="MathJax_Preview">z_{1}^{(3)}</span><script type="math/tex">z_{1}^{(3)}</script></span> 映射到 <span><span class="MathJax_Preview">a_{1}^{(3)}</span><script type="math/tex">a_{1}^{(3)}</script></span> 的神经元的局部梯度相乘。在这个例子中梯度正好等于  1 ，则误差仍然为 1 。所以有 <span><span class="MathJax_Preview">\delta_{1}^{(3)}=1</span><script type="math/tex">\delta_{1}^{(3)}=1</script></span> 。</li>
<li>这里误差信号 1 已经到达 <span><span class="MathJax_Preview">z_{1}^{(3)}</span><script type="math/tex">z_{1}^{(3)}</script></span> 。我们现在需要分配误差信号使得误差的“公平共享”到达 <span><span class="MathJax_Preview">a_{1}^{(2)}</span><script type="math/tex">a_{1}^{(2)}</script></span> 。</li>
<li>现在在 <span><span class="MathJax_Preview">a_{1}^{(2)}</span><script type="math/tex">a_{1}^{(2)}</script></span> 的误差为 <span><span class="MathJax_Preview">\delta_{1}^{(3)}\times W_{1}^{(2)}=W_{1}^{(2)}</span><script type="math/tex">\delta_{1}^{(3)}\times W_{1}^{(2)}=W_{1}^{(2)}</script></span> （在 <span><span class="MathJax_Preview">z_{1}^{(3)}</span><script type="math/tex">z_{1}^{(3)}</script></span> 的误差信号为 <span><span class="MathJax_Preview">\delta_{1}^{(3)}</span><script type="math/tex">\delta_{1}^{(3)}</script></span> ）。因此在 <span><span class="MathJax_Preview">a_{1}^{(2)}</span><script type="math/tex">a_{1}^{(2)}</script></span> 的误差为 <span><span class="MathJax_Preview">W_{1}^{(2)}</span><script type="math/tex">W_{1}^{(2)}</script></span> 。</li>
<li>与第 2 步的做法相同，我们在将 <span><span class="MathJax_Preview">z_{1}^{(2)}</span><script type="math/tex">z_{1}^{(2)}</script></span> 映射到 <span><span class="MathJax_Preview">a_{1}^{(2)}</span><script type="math/tex">a_{1}^{(2)}</script></span> 的神经元上移动误差，将 <span><span class="MathJax_Preview">a_{1}^{(2)}</span><script type="math/tex">a_{1}^{(2)}</script></span> 与局部梯度相乘，这里的局部梯度为  <span><span class="MathJax_Preview">f'(z_{1}^{(2)})</span><script type="math/tex">f'(z_{1}^{(2)})</script></span>  。</li>
<li>
<p>因此在 <span><span class="MathJax_Preview">z_{1}^{(2)}</span><script type="math/tex">z_{1}^{(2)}</script></span> 的误差是 <span><span class="MathJax_Preview">f'(z_{1}^{(2)})W_{1}^{(2)}</span><script type="math/tex">f'(z_{1}^{(2)})W_{1}^{(2)}</script></span> ，我们将其定义为 <span><span class="MathJax_Preview">\delta_{1}^{(2)}</span><script type="math/tex">\delta_{1}^{(2)}</script></span> 。</p>
</li>
<li>
<p>最后，我们通过将上面的误差与参与前向计算的 <span><span class="MathJax_Preview">a_{4}^{(1)}</span><script type="math/tex">a_{4}^{(1)}</script></span> 相乘，把误差的“误差共享”分配到 <span><span class="MathJax_Preview">W_{14}^{(1)}</span><script type="math/tex">W_{14}^{(1)}</script></span> 。</p>
</li>
<li>
<p>所以，对于 <span><span class="MathJax_Preview">W_{14}^{(1)}</span><script type="math/tex">W_{14}^{(1)}</script></span> 的梯度损失可以计算为 <span><span class="MathJax_Preview">a_{4}^{(1)}f'(z_{1}^{(2)})W_{1}^{(2)}</span><script type="math/tex">a_{4}^{(1)}f'(z_{1}^{(2)})W_{1}^{(2)}</script></span> 。</p>
</li>
</ol>
<p>注意我们使用这个方法得到的结果是和之前微分的方法的结果是完全一样的。因此，计算网络中的相应参数的梯度误差既可以使用链式法则也可以使用误差共享和分配的方法——这两个方法能得到相同结果，但是多种方式考虑它们可能是有帮助的。</p>
<p><strong>偏置更新</strong>：偏置项（例如 <span><span class="MathJax_Preview">b_{1}^{(1)}</span><script type="math/tex">b_{1}^{(1)}</script></span> ）和其他权值在数学形式是等价的，只是在计算下一层神经 <span><span class="MathJax_Preview">z_{1}^{(2)}</span><script type="math/tex">z_{1}^{(2)}</script></span> 元输入时相乘的值是常量 1 。因此在第 k 层的第 i 个神经元的偏置的梯度时 <span><span class="MathJax_Preview">\delta_{i}^{(k)}</span><script type="math/tex">\delta_{i}^{(k)}</script></span> 。例如在上面的例子中，我们更新的是 <span><span class="MathJax_Preview">b_{1}^{(1)}</span><script type="math/tex">b_{1}^{(1)}</script></span> 而不是  <span><span class="MathJax_Preview">W_{14}^{(1)}</span><script type="math/tex">W_{14}^{(1)}</script></span> ，那么这个梯度为 <span><span class="MathJax_Preview">f'(z_{1}^{(2)})W_{1}^{(2)}</span><script type="math/tex">f'(z_{1}^{(2)})W_{1}^{(2)}</script></span> 。</p>
<p>从 <span><span class="MathJax_Preview">\delta^{(k)}</span><script type="math/tex">\delta^{(k)}</script></span> 到 <span><span class="MathJax_Preview">\delta^{(k-1)}</span><script type="math/tex">\delta^{(k-1)}</script></span> 反向传播的一般步骤：</p>
<ul>
<li>我们有从 <span><span class="MathJax_Preview">z_{i}^{(k)}</span><script type="math/tex">z_{i}^{(k)}</script></span> 向后传播的误差 <span><span class="MathJax_Preview">\delta_{i}^{(k)}</span><script type="math/tex">\delta_{i}^{(k)}</script></span> ，如下图所示</li>
</ul>
<p><img alt="1560417982981" src="../imgs/1560417982981.png" /></p>
<ul>
<li>我们通过把 <span><span class="MathJax_Preview">\delta_{i}^{(k)}</span><script type="math/tex">\delta_{i}^{(k)}</script></span> 与路径上的权值 <span><span class="MathJax_Preview">W_{ij}^{(k-1)}</span><script type="math/tex">W_{ij}^{(k-1)}</script></span> 相乘，将这个误差反向传播到 <span><span class="MathJax_Preview">a_{j}^{(k-1)}</span><script type="math/tex">a_{j}^{(k-1)}</script></span> 。</li>
<li>因此在 <span><span class="MathJax_Preview">a_{j}^{(k-1)}</span><script type="math/tex">a_{j}^{(k-1)}</script></span> 接收的误差是 <span><span class="MathJax_Preview">\delta_{i}^{(k)}W_{ij}^{(k-1)}</span><script type="math/tex">\delta_{i}^{(k)}W_{ij}^{(k-1)}</script></span> 。</li>
<li>然而， <span><span class="MathJax_Preview">a_{j}^{(k-1)}</span><script type="math/tex">a_{j}^{(k-1)}</script></span> 在前向计算可能出下图的情况，会参与下一层中的多个神经元的计算。那么第 k 层的第 <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> 个神经元的误差也要使用上一步方法将误差反向传播到 <span><span class="MathJax_Preview">a_{j}^{(k-1)}</span><script type="math/tex">a_{j}^{(k-1)}</script></span> 上。</li>
</ul>
<p><img alt="1560417996607" src="../imgs/1560417996607.png" /></p>
<ul>
<li>因此现在在 <span><span class="MathJax_Preview">a_{j}^{(k-1)}</span><script type="math/tex">a_{j}^{(k-1)}</script></span> 接收的误差是 <span><span class="MathJax_Preview">\delta_{i}^{(k)}W_{ij}^{(k-1)}+\delta_{m}^{(k)}W_{mj}^{(k-1)}</span><script type="math/tex">\delta_{i}^{(k)}W_{ij}^{(k-1)}+\delta_{m}^{(k)}W_{mj}^{(k-1)}</script></span> 。</li>
<li>实际上，我们可以把上面误差和简化为 <span><span class="MathJax_Preview">\sum_{i}\delta_{i}^{(k)}W_{ij}^{(k-1)}</span><script type="math/tex">\sum_{i}\delta_{i}^{(k)}W_{ij}^{(k-1)}</script></span> 。</li>
<li>现在我们有在 <span><span class="MathJax_Preview">a_{j}^{(k-1)}</span><script type="math/tex">a_{j}^{(k-1)}</script></span> 正确的误差，然后将其与局部梯度  <span><span class="MathJax_Preview">f'(z_{j}^{(k-1)})</span><script type="math/tex">f'(z_{j}^{(k-1)})</script></span> 相乘，把误差信息反向传到第 <span><span class="MathJax_Preview">k-1</span><script type="math/tex">k-1</script></span> 层的第 <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> 个神经元上。</li>
<li>因此到达 <span><span class="MathJax_Preview">z_{j}^{(k-1)}</span><script type="math/tex">z_{j}^{(k-1)}</script></span> 的误差为 <span><span class="MathJax_Preview">f'(z_{j}^{(k-1)})\sum_{i}\delta_{i}^{(k)}W_{ij}^{(k-1)}</span><script type="math/tex">f'(z_{j}^{(k-1)})\sum_{i}\delta_{i}^{(k)}W_{ij}^{(k-1)}</script></span> 。</li>
</ul>
<h4 id="16-training-with-backpropagation-vectorized">1.6 Training with Backpropagation – Vectorized<a class="headerlink" href="#16-training-with-backpropagation-vectorized" title="Permanent link">&para;</a></h4>
<p>到目前为止，我们讨论了对模型中的给定参数计算梯度的方法。这里会一般泛化上面的方法，让我们可以直接一次过更新权值矩阵和偏置向量。注意这只是对上面模型的简单地扩展，这将有助于更好理解在矩阵-向量级别上进行误差反向传播的方法。</p>
<p>对更定的参数 <span><span class="MathJax_Preview">W_{ij}^{(k)}</span><script type="math/tex">W_{ij}^{(k)}</script></span> ，我们知道它的误差梯度是 <span><span class="MathJax_Preview">\delta_{j}^{(k+1)}\cdot a_{j}^{(k)}</span><script type="math/tex">\delta_{j}^{(k+1)}\cdot a_{j}^{(k)}</script></span> 。其中 <span><span class="MathJax_Preview">W^{(k)}</span><script type="math/tex">W^{(k)}</script></span> 是将 <span><span class="MathJax_Preview">a^{(k)}</span><script type="math/tex">a^{(k)}</script></span> 映射到 <span><span class="MathJax_Preview">z^{(k+1)}</span><script type="math/tex">z^{(k+1)}</script></span> 的矩阵。因此我们可以确定整个矩阵 <span><span class="MathJax_Preview">W^{(k)}</span><script type="math/tex">W^{(k)}</script></span> 的梯度误差为：</p>
<div>
<div class="MathJax_Preview">
\nabla_{W^{(k)}} =         \begin{bmatrix}         \delta_{1}^{(k+1)}a_{1}^{(k)} &amp; \delta_{1}^{(k+1)}a_{2}^{(k)} &amp; \cdots \\         \delta_{2}^{(k+1)}a_{1}^{(k)} &amp; \delta_{2}^{(k+1)}a_{2}^{(k)} &amp; \cdots \\         \vdots &amp; \vdots  &amp; \ddots \\         \end{bmatrix} = \delta^{(k+1)}a^{(k)T}  \\
</div>
<script type="math/tex; mode=display">
\nabla_{W^{(k)}} =         \begin{bmatrix}         \delta_{1}^{(k+1)}a_{1}^{(k)} & \delta_{1}^{(k+1)}a_{2}^{(k)} & \cdots \\         \delta_{2}^{(k+1)}a_{1}^{(k)} & \delta_{2}^{(k+1)}a_{2}^{(k)} & \cdots \\         \vdots & \vdots  & \ddots \\         \end{bmatrix} = \delta^{(k+1)}a^{(k)T}  \\
</script>
</div>
<p>因此我们可以将整个矩阵形式的梯度写为在矩阵中的反向传播的误差向量和前向激活输出的**外积**。</p>
<p>现在我们来看看如何能够计算误差向量 <span><span class="MathJax_Preview">\delta^{(k+1)}</span><script type="math/tex">\delta^{(k+1)}</script></span> 。我们从上面的例子中有， <span><span class="MathJax_Preview">\delta_{i}^{(k)}=f'(z_{j}^{(k)})\sum_{i}\delta_{i}^{(k+1)}W_{ij}^{(k)}</span><script type="math/tex">\delta_{i}^{(k)}=f'(z_{j}^{(k)})\sum_{i}\delta_{i}^{(k+1)}W_{ij}^{(k)}</script></span> 。这可以简单地改写为矩阵的形式：</p>
<div>
<div class="MathJax_Preview">
\delta_{i}^{(k)}=f'(z^{(k)})\circ (W^{(k)T}\delta^{(k+1)}) \\
</div>
<script type="math/tex; mode=display">
\delta_{i}^{(k)}=f'(z^{(k)})\circ (W^{(k)T}\delta^{(k+1)}) \\
</script>
</div>
<p>在上面的公式中 <span><span class="MathJax_Preview">\circ</span><script type="math/tex">\circ</script></span> 运算符是表示向量之间对应元素的相乘（ <span><span class="MathJax_Preview">\mathbb{R}^{N}\times \mathbb{R}^{N}\rightarrow \mathbb{R}^{N}</span><script type="math/tex">\mathbb{R}^{N}\times \mathbb{R}^{N}\rightarrow \mathbb{R}^{N}</script></span> ）。</p>
<p><strong>计算效率</strong>：在探索了 <strong>element-wise</strong> 的更新和 <strong>vector-wise</strong> 的更新之后，必须认识到在科学计算环境中，如 MATLAB 或 Python（使用 Numpy / Scipy 库），向量化运算的计算效率是非常高的。因此在实际中应该使用向量化运算。此外，我们也要减少反向传播中的多余的计算——例如，注意到 <span><span class="MathJax_Preview">\delta^{(k)}</span><script type="math/tex">\delta^{(k)}</script></span> 是直接依赖在 <span><span class="MathJax_Preview">\delta^{(k+1)}</span><script type="math/tex">\delta^{(k+1)}</script></span> 上。所以我们要保证使用 <span><span class="MathJax_Preview">\delta^{(k+1)}</span><script type="math/tex">\delta^{(k+1)}</script></span> 更新 <span><span class="MathJax_Preview">W^{(k)}</span><script type="math/tex">W^{(k)}</script></span> 时，要保存 <span><span class="MathJax_Preview">\delta^{(k+1)}</span><script type="math/tex">\delta^{(k+1)}</script></span> 用于后面 <span><span class="MathJax_Preview">\delta^{(k)}</span><script type="math/tex">\delta^{(k)}</script></span> 的计算-然后计算 <span><span class="MathJax_Preview">(k-1)...(1)</span><script type="math/tex">(k-1)...(1)</script></span> 层的时候重复上述的步骤。这样的递归过程是使得反向传播成为计算上可负担的过程。</p>
<h3 id="2-neural-networks-tips-and-tricks">2 Neural Networks: Tips and Tricks<a class="headerlink" href="#2-neural-networks-tips-and-tricks" title="Permanent link">&para;</a></h3>
<h4 id="21-gradient-check">2.1 Gradient Check<a class="headerlink" href="#21-gradient-check" title="Permanent link">&para;</a></h4>
<p>在上一部分中，我们详细地讨论了如何用基于微积分的方法计算神经网络中的参数的误差梯度／更新。这里我们介绍一种用数值近似这些梯度的方法——虽然在计算上的低效不能直接用于训练神经网络，这种方法可以非常准确地估计任何参数的导数；因此，它可以作为对导数的正确性的有用的检查。给定一个模型的参数向量 <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 和损失函数 <span><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> ，围绕 <span><span class="MathJax_Preview">\theta_{i}</span><script type="math/tex">\theta_{i}</script></span> 的数值梯度由 <strong>central difference formula</strong> 得出：</p>
<div>
<div class="MathJax_Preview">
f'(\theta)\approx \frac{J(\theta^{(i+)})-\theta^{(i-)})}{2\epsilon}  \\
</div>
<script type="math/tex; mode=display">
f'(\theta)\approx \frac{J(\theta^{(i+)})-\theta^{(i-)})}{2\epsilon}  \\
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">\epsilon</span><script type="math/tex">\epsilon</script></span> 是一个很小的值（一般约为 <span><span class="MathJax_Preview">1e^{-5}</span><script type="math/tex">1e^{-5}</script></span> ）。当我们使用  <span><span class="MathJax_Preview">+\epsilon</span><script type="math/tex">+\epsilon</script></span> 扰动参数 <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 的第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 个元素时，就可以在前向传播上计算误差 <span><span class="MathJax_Preview">J(\theta^{(i+)})</span><script type="math/tex">J(\theta^{(i+)})</script></span> 。相似地，当我们使用  <span><span class="MathJax_Preview">-\epsilon</span><script type="math/tex">-\epsilon</script></span> 扰动参数 <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 的第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 个元素时，就可以在前向传播上计算误差 <span><span class="MathJax_Preview">J(\theta^{(i-)})</span><script type="math/tex">J(\theta^{(i-)})</script></span> 。因此，计算两次前向传播，我们可以估计在模型中任意给定参数的梯度。我们注意到数值梯度的定义和导数的定义很相似，其中，在标量的情况下：</p>
<div>
<div class="MathJax_Preview">
f'(\theta)\approx \frac{f(x+\epsilon)-f(x)}{\epsilon}   \\
</div>
<script type="math/tex; mode=display">
f'(\theta)\approx \frac{f(x+\epsilon)-f(x)}{\epsilon}   \\
</script>
</div>
<p>当然，还是有一点不同——上面的定义仅仅在正向扰动 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 计算梯度。虽然是可以用这种方式定义数值梯度，但在实际中使用 <strong>central difference formula</strong> 常常可以更准确和更稳定，因为我们在两个方向都对参数扰动。为了更好地逼近一个点附近的导数/斜率，我们需要在该点的左边和右边检查函数 <span><span class="MathJax_Preview">f'</span><script type="math/tex">f'</script></span> 的行为。也可以使用泰勒定理来表示 <strong>central difference formula</strong> 有 <span><span class="MathJax_Preview">\epsilon^{2}</span><script type="math/tex">\epsilon^{2}</script></span> 比例误差，这相当小，而导数定义更容易出错。</p>
<p>现在你可能会产生疑问，如果这个方法这么准确，为什么我们不用它而不是用反向传播来计算神经网络的梯度？这是因为效率的问题——每当我们想计算一个元素的梯度，需要在网络中做两次前向传播，这样是很耗费计算资源的。再者，很多大规模的神经网络含有几百万的参数，对每个参数都计算两次明显不是一个好的选择。同时在例如 <strong>SGD</strong> 这样的优化技术中，我们需要通过数千次的迭代来计算梯度，使用这样的方法很快会变得难以应付。这种低效性是我们只使用梯度检验来验证我们的分析梯度的正确性的原因。梯度检验的实现如下所示：</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    a naive implementation of numerical gradient of f at x</span>
<span class="sd">    - f should be a function that takes a single argument</span>
<span class="sd">    - x is the point (numpy array) to evaluate the gradient  </span>
<span class="sd">    at</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate function value at original point</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.00001</span>

    <span class="c1"># iterate over all indexes in x</span>
    <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;multi_index&#39;</span><span class="p">,</span>
                     <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>

        <span class="c1"># evaluate function at x+h</span>
        <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
        <span class="n">old_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">h</span> <span class="c1"># increment by h</span>
        <span class="n">fxh_left</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate f(x + h)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">-</span> <span class="n">h</span> <span class="c1"># decrement by h</span>
        <span class="n">fxh_right</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate f(x - h)</span>
        <span class="c1"># restore to previous value (very important!)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> 

        <span class="c1"># compute the partial derivative</span>
        <span class="c1"># the slope</span>
        <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh_left</span> <span class="o">-</span> <span class="n">fxh_right</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
        <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span> <span class="c1"># step to next dimension</span>
    <span class="k">return</span> <span class="n">grad</span>
</code></pre></div>

<h4 id="22-regularization">2.2 Regularization<a class="headerlink" href="#22-regularization" title="Permanent link">&para;</a></h4>
<p>和很多机器学习的模型一样，神经网络很容易过拟合，这令到模型在训练集上能获得近乎完美的表现，但是却不能泛化到测试集上。一个常见的用于解决过拟合（“高方差问题”）的方法是使用 <span><span class="MathJax_Preview">L2</span><script type="math/tex">L2</script></span> 正则化。我们只需要在损失函数 <span><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> 上增加一个正则项，现在的损失函数如下：</p>
<div>
<div class="MathJax_Preview">
J_{R}=J+\lambda\sum_{i=1}^{L}\big|\big|W^{(i)}\big|\big|_{F}   \\
</div>
<script type="math/tex; mode=display">
J_{R}=J+\lambda\sum_{i=1}^{L}\big|\big|W^{(i)}\big|\big|_{F}   \\
</script>
</div>
<p>在上面的公式中， <span><span class="MathJax_Preview">\big|\big|W^{(i)}\big|\big|_{F}</span><script type="math/tex">\big|\big|W^{(i)}\big|\big|_{F}</script></span> 是矩阵 <span><span class="MathJax_Preview">W^{(i)}</span><script type="math/tex">W^{(i)}</script></span> （在神经网络中的第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 个权值矩阵）的 <span><span class="MathJax_Preview">Frobenius</span><script type="math/tex">Frobenius</script></span> 范数, <span><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 是超参数控制损失函数中的权值的大小。</p>
<blockquote>
<p>矩阵 <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> 的 <span><span class="MathJax_Preview">Frobenius</span><script type="math/tex">Frobenius</script></span> 范数的定义 <span><span class="MathJax_Preview">\|U\|_{F}=\sqrt{\sum_{i} \sum_{l} U_{i l}^{2}}</span><script type="math/tex">\|U\|_{F}=\sqrt{\sum_{i} \sum_{l} U_{i l}^{2}}</script></span></p>
</blockquote>
<p>当我们尝试去最小化 <span><span class="MathJax_Preview">J_{R}</span><script type="math/tex">J_{R}</script></span> ，正则化本质上就是当优化损失函数的时候，惩罚数值太大的权值（让权值的数值分配更加均衡，防止出现部分权值特别大的情况）。由于 <span><span class="MathJax_Preview">Frobenius</span><script type="math/tex">Frobenius</script></span> 范数的二次的性质（计算矩阵的元素的平方和）， <span><span class="MathJax_Preview">L2</span><script type="math/tex">L2</script></span> 正则项有效地降低了模型的灵活性和因此减少出现过拟合的可能性。增加这样一个约束可以**使用贝叶斯派的思想解释**，这个正则项是对模型的参数加上一个**先验分布**，优化权值使其接近于 0——有多接近是取决于 <span><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 的值。选择一个合适的 <span><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 值是很重要的，并且需要通过超参数调整来选择。 <span><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 的值太大会令很多权值都接近于  0 ，则模型就不能在训练集上学习到有意义的东西，经常在训练、验证和测试集上的表现都非常差。 <span><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 的值太小，会让模型仍旧出现过拟合的现象。需要注意的是，偏置项不会被正则化，不会计算入损失项中——尝试去思考一下为什么</p>
<p>为何在损失项中不计算偏置项？</p>
<blockquote>
<p>偏置项在模型中仅仅是偏移的关系，使用少量的数据就能拟合到这项，而且从经验上来说，偏置值的大小对模型表现没有很显著的影响，因此不需要正则化偏置项
<a href="https://www.zhihu.com/question/66894061">深度学习里面的偏置为什么不加正则？</a>
下面摘录已有的三条回答
首先正则化主要是为了防止过拟合，而过拟合一般表现为模型对于输入的微小改变产生了输出的较大差异，这主要是由于有些参数w过大的关系，通过对||w||进行惩罚，可以缓解这种问题。而如果对||b||进行惩罚，其实是没有作用的，因为在对输出结果的贡献中，参数b对于输入的改变是不敏感的，不管输入改变是大还是小，参数b的贡献就只是加个偏置而已。
举个例子，如果你在训练集中，w和b都表现得很好，但是在测试集上发生了过拟合，b是不背这个锅的，因为它对于所有的数据都是一视同仁的（都只是给它们加个偏置），要背锅的是w，因为它会对不同的数据产生不一样的加权。或者说，模型对于输入的微小改变产生了输出的较大差异，这是因为模型的“曲率”太大，而模型的曲率是由w决定的，b不贡献曲率（对输入进行求导，b是直接约掉的）。</p>
</blockquote>
<p>从贝叶斯的角度来讲，正则化项通常都包含一定的先验信息，神经网络倾向于较小的权重以便更好地泛化，但是对偏置就没有这样一致的先验知识。另外，很多神经网络更倾向于区分方向信息（对应于权重），而不是位置信息（对应于偏置），所以对偏置加正则化项对控制过拟合的作用是有限的，相反很可能会因为不恰当的正则强度影响神经网络找到最优点。</p>
<p>过拟合会使得模型对异常点很敏感，即准确插入异常点，导致拟合函数中的曲率很大（即函数曲线的切线斜率非常高），而偏置对模型的曲率没有贡献（对多项式模型进行求导，为W的线性加和），所以正则化他们也没有什么意义。</p>
<p>有时候我们会用到其他类型的正则项，例如 <span><span class="MathJax_Preview">L1</span><script type="math/tex">L1</script></span> 正则项，它将参数元素的绝对值全部加起来-然而，在实际中很少会用 <span><span class="MathJax_Preview">L1</span><script type="math/tex">L1</script></span> 正则项，因为会令权值参数变得稀疏。在下一部分，我们讨论 dropout ，这是另外一种有效的正则化方法，通过在前向传播过程随机将神经元设为 0</p>
<blockquote>
<p>Dropout 实际上是通过在每次迭代中忽略它们的权值来实现“冻结”部分 unit 。这些“冻结”的 unit 不是把它们设为 0 ，而是对于该迭代，网络假定它们为 0 。“冻结”的 unit 不会为此次迭代更新</p>
</blockquote>
<h4 id="23-dropout">2.3 Dropout<a class="headerlink" href="#23-dropout" title="Permanent link">&para;</a></h4>
<p><strong>Dropout</strong> 是一个非常强大的正则化技术，是 Srivastava 在论文 《Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting》中首次提出，下图展示了 dropout 如何应用在神经网络上。</p>
<p><img alt="1560432828326" src="../imgs/1560432828326.png" /></p>
<p>这个想法是简单而有效的——训练过程中，在每次的前向／反向传播中我们按照一定概率 <span><span class="MathJax_Preview">(1-p)</span><script type="math/tex">(1-p)</script></span> 随机地“ drop ”一些神经元子集（或者等价的，我们保持一定概率 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 的神经元是激活的）。然后，在测试阶段，我们将使用全部的神经元来进行预测。使用 Dropout 神经网络一般能从数据中学到更多有意义的信息，更少出现过拟合和通常在现今的任务上获得更高的整体表现。这种技术应该如此有效的一个直观原因是， dropout 本质上作的是一次以指数形式训练许多较小的网络，并对其预测进行平均。</p>
<p>实际上，我们使用 dropout 的方式是我们取每个神经元层的输出 <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> ，并保持概率  <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 的神经元是激活的，否则将神经元设置为 0 。然后，在反向传播中我们仅对在前向传播中激活的神经元回传梯度。最后，在测试过程，我们使用神经网络中全部的神经元进行前向传播计算。然而，有一个关键的微妙之处，为了使 dropout 有效地工作，测试阶段的神经元的预期输出应与训练阶段大致相同——否则输出的大小可能会有很大的不同，网络的表现已经不再明确了。因此，我们通常必须在测试阶段将每个神经元的输出除以某个值——这留给读者作为练习来确定这个值应该是多少，以便在训练和测试期间的预期输出相等（该值为 <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> ） 。</p>
<p>以下源于 <a href="https://nndl.github.io/">《神经网络与深度学习》</a> <code>P190</code></p>
<ul>
<li>
<p>目的：缓解过拟合问题，一定程度上达到正则化的效果</p>
</li>
<li>
<p>效果：减少下层节点对其的依赖，迫使网络去学习更加**鲁棒**的特征</p>
</li>
<li>
<p><strong>集成学习的解释</strong> 👍</p>
<ul>
<li>每做一次丢弃，相当于从原始的网络中采样得到一个子网络。
    如果一个神经网络有<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>个神经元，那么总共可以采样出<span><span class="MathJax_Preview">2^n</span><script type="math/tex">2^n</script></span>个子网络。每次迭代都相当于训练一个不同的子网络，这些子网络都共享原始网络的参数。那么，最终的网络可以近似看作是集成了指数级个不同网络的组合模型。</li>
</ul>
</li>
<li>
<p><strong>贝叶斯学习的解释</strong>  👍 </p>
<ul>
<li>丢弃法也可以解释为一种贝叶斯学习的近似。用<span><span class="MathJax_Preview">y=f(\mathbf{x}, \theta)</span><script type="math/tex">y=f(\mathbf{x}, \theta)</script></span>来表示要学习的神经网络，贝叶斯学习是假设参数<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>为随机向量，并且先验分布为<span><span class="MathJax_Preview">q(\theta)</span><script type="math/tex">q(\theta)</script></span>，贝叶斯方法的预测为</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned} \mathbb{E}_{q(\theta)}[y] &amp;=\int_{q} f(\mathbf{x}, \theta) q(\theta) d \theta \\ &amp; \approx \frac{1}{M} \sum_{m=1}^{M} f\left(\mathbf{x}, \theta_{m}\right) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \mathbb{E}_{q(\theta)}[y] &=\int_{q} f(\mathbf{x}, \theta) q(\theta) d \theta \\ & \approx \frac{1}{M} \sum_{m=1}^{M} f\left(\mathbf{x}, \theta_{m}\right) \end{aligned}
</script>
</div>
<ul>
<li>其中<span><span class="MathJax_Preview">f(\mathbf{x}, \theta_m)</span><script type="math/tex">f(\mathbf{x}, \theta_m)</script></span>为第m次应用丢弃方法后的网络，其参数<span><span class="MathJax_Preview">\theta_m</span><script type="math/tex">\theta_m</script></span>为对全部参数<span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>的一次采样。</li>
</ul>
</li>
<li>
<p>RNN中的变分Dropout <a href="https://arxiv.org/abs/1512.05287">Variational Dropout</a></p>
<ul>
<li>
<p>Dropout一般是针对神经元进行随机丢弃，但是也可以扩展到对神经元之间
    的连接进行随机丢弃，或每一层进行随机丢弃。</p>
</li>
<li>
<p>在RNN中，不能直接对每个时刻的隐状态进行随机
    丢弃，这样会损害循环网络在时间维度上记忆能力。</p>
</li>
<li>
<p>一种简单的方法是对非时间维度的连接（即非循环连接）进行随机丢失。如图所示，虚线边表示进行随机丢弃，不同的颜色表示不同的丢弃掩码。</p>
<p><img alt="1558861348710" src="../imgs/1558861348710.png" /></p>
</li>
<li>
<p>然而根据贝叶斯学习的解释，丢弃法是一种对参数<span><span class="MathJax_Preview">θ</span><script type="math/tex">θ</script></span>的采样。每次采样的参数需要在每个时刻保持不变。因此，在对循环神经网络上使用丢弃法时，需要对参数矩阵的每个元素进行随机丢弃，并在所有时刻都使用相同的丢弃掩码。这种方法称为变分丢弃法（Variational Dropout）。
    图7.12给出了变分丢弃法的示例，<strong>相同颜色表示使用相同的丢弃掩码</strong>。</p>
</li>
</ul>
</li>
</ul>
<p><img alt="1558861390725" src="../imgs/1558861390725.png" /></p>
<h4 id="24-neuron-units">2.4 Neuron Units<a class="headerlink" href="#24-neuron-units" title="Permanent link">&para;</a></h4>
<p>到目前为止，我们讨论了含有 sigmoidal neurons 的非线性分类的神经网络。但是在很多应用中，使用其他激活函数可以设计更好的神经网络。下面列出一些常见的激活函数和激活函数的梯度定义，它们可以和前面讨论过的 sigmoidal 函数互相替换。</p>
<p><strong>Sigmoid</strong>：这是我们讨论过的常用选择，激活函数 <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> 为：
$$
\sigma(z)=\frac{1}{1+exp(-z)}
$$</p>
<p>其中  <span><span class="MathJax_Preview">\sigma(z)\in (0,1)</span><script type="math/tex">\sigma(z)\in (0,1)</script></span> 。</p>
<p><img alt="1560433211653" src="../imgs/1560433211653.png" /></p>
<p><span><span class="MathJax_Preview">\sigma(z)</span><script type="math/tex">\sigma(z)</script></span> 的梯度为</p>
<div>
<div class="MathJax_Preview">
\sigma'(z)=\frac{-exp(-z)}{1+exp(-z)}=\sigma(z)(1-\sigma(z))  \\
</div>
<script type="math/tex; mode=display">
\sigma'(z)=\frac{-exp(-z)}{1+exp(-z)}=\sigma(z)(1-\sigma(z))  \\
</script>
</div>
<p><strong>Tanh</strong>： <span><span class="MathJax_Preview">tanh</span><script type="math/tex">tanh</script></span> 函数是 <span><span class="MathJax_Preview">sigmoid</span><script type="math/tex">sigmoid</script></span> 函数之外的另一个选择，在实际中它能更快地收敛。 <span><span class="MathJax_Preview">tanh</span><script type="math/tex">tanh</script></span> 和 <span><span class="MathJax_Preview">sigmoid</span><script type="math/tex">sigmoid</script></span> 的主要不同在于 <span><span class="MathJax_Preview">tanh</span><script type="math/tex">tanh</script></span> 的输出范围在  -1 到 1 ，而 <span><span class="MathJax_Preview">sigmoid</span><script type="math/tex">sigmoid</script></span> 的输出范围在 0 到 1 。
$$
tanh(z)=\frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}=2\sigma(2z)-1   \
$$</p>
<p>其中 <span><span class="MathJax_Preview">tanh(z)\in (-1, 1)</span><script type="math/tex">tanh(z)\in (-1, 1)</script></span> 。</p>
<p><img alt="1560433307386" src="../imgs/1560433307386.png" /></p>
<p><span><span class="MathJax_Preview">tanh(z)</span><script type="math/tex">tanh(z)</script></span> 的梯度为：</p>
<div>
<div class="MathJax_Preview">
tanh'(z)=\bigg(\frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}\bigg)^{2}=1-tanh^{2}(z)   \\
</div>
<script type="math/tex; mode=display">
tanh'(z)=\bigg(\frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}\bigg)^{2}=1-tanh^{2}(z)   \\
</script>
</div>
<p><strong>Hard tanh</strong>：有时候 <span><span class="MathJax_Preview">hard\;tanh</span><script type="math/tex">hard\;tanh</script></span> 函数有时比 <span><span class="MathJax_Preview">tanh</span><script type="math/tex">tanh</script></span> 函数的选择更为优先，因为它的计算量更小。然而当 <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> 的值大于 <span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span> 时，函数的数值会饱和（如下图所示会恒等于 1）。<span><span class="MathJax_Preview">hard\;tanh</span><script type="math/tex">hard\;tanh</script></span> 激活函数为：</p>
<div>
<div class="MathJax_Preview">
\begin{eqnarray}  hardtanh(z) &amp;=&amp; \begin{cases} -1&amp; \text{:$z&lt;1$}\\ z &amp; \text{:$-1\le z \le 1$} \\ 1 &amp; \text{:$z&gt;1$} \end{cases} \nonumber  \end{eqnarray}  \\
</div>
<script type="math/tex; mode=display">
\begin{eqnarray}  hardtanh(z) &=& \begin{cases} -1& \text{:$z<1$}\\ z & \text{:$-1\le z \le 1$} \\ 1 & \text{:$z>1$} \end{cases} \nonumber  \end{eqnarray}  \\
</script>
</div>
<p><img alt="1560433436646" src="../imgs/1560433436646.png" /></p>
<p><span><span class="MathJax_Preview">hard\;tanh</span><script type="math/tex">hard\;tanh</script></span> 这个函数的微分也可以用分段函数的形式表示：</p>
<div>
<div class="MathJax_Preview">
\begin{eqnarray}  hardtanh(z) &amp;=&amp; \begin{cases} 1 &amp; \text{:$-1\le z \le 1$} \\ 0 &amp; \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}   \\
</div>
<script type="math/tex; mode=display">
\begin{eqnarray}  hardtanh(z) &=& \begin{cases} 1 & \text{:$-1\le z \le 1$} \\ 0 & \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}   \\
</script>
</div>
<p><strong>Soft sign</strong>： <span><span class="MathJax_Preview">soft\;sign</span><script type="math/tex">soft\;sign</script></span> 函数是另外一种非线性激活函数，它可以是 <span><span class="MathJax_Preview">tanh</span><script type="math/tex">tanh</script></span> 的另外一种选择，因为它和 <span><span class="MathJax_Preview">hard\;clipped\;functions</span><script type="math/tex">hard\;clipped\;functions</script></span> 一样不会过早地饱和：
$$
softsign(z)=\frac{z}{1+|z|}<br />
$$</p>
<p><img alt="1561044890099" src="../imgs/1561044890099.png" /></p>
<p><span><span class="MathJax_Preview">soft\;sign</span><script type="math/tex">soft\;sign</script></span> 函数的微分表达式为：</p>
<div>
<div class="MathJax_Preview">
softsign'(z)=\frac{sgn(z)}{(1+z)^{2}}   \\
</div>
<script type="math/tex; mode=display">
softsign'(z)=\frac{sgn(z)}{(1+z)^{2}}   \\
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">sgn</span><script type="math/tex">sgn</script></span> 是符号函数，根据 <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> 的符号返回 1 或者 -1 。</p>
<p><strong>ReLU</strong>： <span><span class="MathJax_Preview">ReLU</span><script type="math/tex">ReLU</script></span> （ <span><span class="MathJax_Preview">Rectiﬁed\;Linear\;Unit</span><script type="math/tex">Rectiﬁed\;Linear\;Unit</script></span> ）函数是激活函数中的一个常见的选择，当 z 的值特别大的时候它也不会饱和。在计算机视觉应用中取得了很大的成功：
$$
rect(z)=max(z,0)
$$</p>
<p><img alt="1560433750964" src="../imgs/1560433750964.png" /></p>
<p><span><span class="MathJax_Preview">ReLU</span><script type="math/tex">ReLU</script></span> 函数的微分是一个分段函数：</p>
<div>
<div class="MathJax_Preview">
\begin{eqnarray}  rect'(z) &amp;=&amp; \begin{cases} 1 &amp; \text{$z &gt; 0$} \\ 0 &amp; \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}    \\
</div>
<script type="math/tex; mode=display">
\begin{eqnarray}  rect'(z) &=& \begin{cases} 1 & \text{$z > 0$} \\ 0 & \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}    \\
</script>
</div>
<p><strong>Leaky ReLU</strong>：传统的 <span><span class="MathJax_Preview">ReLU</span><script type="math/tex">ReLU</script></span> 单元当 z 的值小于 0 时，是不会反向传播误差 <span><span class="MathJax_Preview">leaky\;ReLU</span><script type="math/tex">leaky\;ReLU</script></span> 改善了这一点，当 <span><span class="MathJax_Preview">z</span><script type="math/tex">z</script></span> 的值小于 0 时，仍然会有一个很小的误差反向传播回去。
$$
leaky(z)=max(z, k\cdot z)  \
$$</p>
<p>其中 <span><span class="MathJax_Preview">0&lt;k&lt;1</span><script type="math/tex">0<k<1</script></span> 。</p>
<p><span><span class="MathJax_Preview">leaky\;ReLU</span><script type="math/tex">leaky\;ReLU</script></span> 函数的微分是一个分段函数：</p>
<div>
<div class="MathJax_Preview">
\begin{eqnarray}  leaky'(z) &amp;=&amp; \begin{cases} 1 &amp; \text{$z &gt; 0$} \\ k &amp; \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}
</div>
<script type="math/tex; mode=display">
\begin{eqnarray}  leaky'(z) &=& \begin{cases} 1 & \text{$z > 0$} \\ k & \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}
</script>
</div>
<p><img alt="1560433960028" src="../imgs/1560433960028.png" /></p>
<h4 id="25-data-preprocessing">2.5 Data Preprocessing<a class="headerlink" href="#25-data-preprocessing" title="Permanent link">&para;</a></h4>
<p>与机器学习模型的一般情况一样，确保模型在当前任务上获得合理性能的一个关键步骤是对数据执行基本的预处理。下面概述了一些常见的技术。</p>
<p><strong>Mean Subtraction</strong></p>
<p>给定一组输入数据 <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> ，一般把 <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 中的值减去 <span><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 的平均特征向量来使数据**零中心化**。在实践中很重要的一点是，<strong>只计算训练集的平均值</strong>，而且在**训练集，验证集和测试集**都是**减去同一平均值**。</p>
<p><strong>Normalization</strong></p>
<p>另外一个常见的技术（虽然没有 <span><span class="MathJax_Preview">mean\;Subtraction</span><script type="math/tex">mean\;Subtraction</script></span> 常用）是将每个输入特征维度缩小，让每个输入特征维度具有相似的幅度范围。这是很有用的，因此不同的输入特征是用不同“单位”度量，但是最初的时候我们经常认为所有的特征同样重要。实现方法是将特征除以它们各自在训练集中计算的标准差。</p>
<p><strong>Whitening</strong></p>
<p>相比上述的两个方法， <strong>whitening</strong> 没有那么常用，它本质上是数据经过转换后，特征之间相关性较低，所有特征具有相同的方差（协方差阵为 1 ）。首先对数据进行 <strong>Mean Subtraction</strong> 处理，得到 <span><span class="MathJax_Preview">X'</span><script type="math/tex">X'</script></span> 。然后我们对 <span><span class="MathJax_Preview">X'</span><script type="math/tex">X'</script></span> 进行奇异值分解得到矩阵 <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> , <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> , <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> ，计算 <span><span class="MathJax_Preview">UX'</span><script type="math/tex">UX'</script></span> 将 <span><span class="MathJax_Preview">X'</span><script type="math/tex">X'</script></span> 投影到由 <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> 的列定义的基上。我们最后将结果的每个维度除以 <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> 中的相应奇异值，从而适当地缩放我们的数据（如果其中有奇异值为 0 ，我们就除以一个很小的值代替）。</p>
<h4 id="26-parameter-initialization">2.6 Parameter Initialization<a class="headerlink" href="#26-parameter-initialization" title="Permanent link">&para;</a></h4>
<p>让神经网络实现最佳性能的关键一步是以合理的方式初始化参数。一个好的起始方法是将权值初始化为通常分布在 0 附近的很小的随机数-在实践中效果还不错。在论文***Understanding the difficulty***
<strong><em>of training deep feedforward neural networks</em></strong>
<strong><em>(2010)</em></strong>, Xavier 研究不同权值和偏置初始化方案对训练动力（ <strong>training dynamics</strong>​ ）的影响。实验结果表明，对于 sigmoid 和 tanh 激活单元，当一个权值矩阵 <span><span class="MathJax_Preview">W\in \mathbb{R}^{n^{(l+1)}\times n^{(l)}}</span><script type="math/tex">W\in \mathbb{R}^{n^{(l+1)}\times n^{(l)}}</script></span> 以如下的均匀分布的方式随机初始化，能够实现更快的收敛和得到更低的误差：</p>
<div>
<div class="MathJax_Preview">
W\sim U\bigg[-\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}},\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}}\;\bigg]   \\
</div>
<script type="math/tex; mode=display">
W\sim U\bigg[-\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}},\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}}\;\bigg]   \\
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">n^{(l)}</span><script type="math/tex">n^{(l)}</script></span> 是 W <span><span class="MathJax_Preview">(fan\text{-}in)</span><script type="math/tex">(fan\text{-}in)</script></span> 的输入单元数， <span><span class="MathJax_Preview">n^{(l+1)}</span><script type="math/tex">n^{(l+1)}</script></span> 是 W <span><span class="MathJax_Preview">(fan\text{-}out)</span><script type="math/tex">(fan\text{-}out)</script></span> 的输出单元数。在这个参数初始化方案中，偏置单元是初始化为 0 。这种方法是尝试保持跨层之间的激活方差以及反向传播梯度方差。如果没有这样的初始化，梯度方差（当中含有纠正信息）通常随着跨层的反向传播而衰减。</p>
<h4 id="27-learning-strategies">2.7 Learning Strategies<a class="headerlink" href="#27-learning-strategies" title="Permanent link">&para;</a></h4>
<p>训练期间模型参数更新的速率/幅度可以使用学习率进行控制。在最简单的梯度下降公式中， <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 是学习率：</p>
<div>
<div class="MathJax_Preview">
\theta^{new}=\theta^{old}-\alpha\nabla_{\theta}J_{t}(\theta)   \\
</div>
<script type="math/tex; mode=display">
\theta^{new}=\theta^{old}-\alpha\nabla_{\theta}J_{t}(\theta)   \\
</script>
</div>
<p>你可能会认为如果要更快地收敛，我们应该对 <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 取一个较大的值——然而，在更快的收敛速度下并不能保证更快的收敛。实际上，如果学习率非常高，我们可能会遇到损失函数难以收敛的情况，因为参数更新幅度过大，会导致模型越过凸优化的极小值点，如下图所示。在非凸模型中（我们很多时候遇到的模型都是非凸），高学习率的结果是难以预测的，但是损失函数难以收敛的可能性是非常高的。</p>
<p><img alt="1560434193931" src="../imgs/1560434193931.png" /></p>
<p>避免损失函数难以收敛的一个简答的解决方法是使用一个很小的学习率，让模型谨慎地在参数空间中迭代——当然，如果我们使用了一个太小的学习率，损失函数可能不会在合理的时间内收敛，或者会困在局部最优点。因此，与任何其他超参数一样，学习率必须有效地调整。</p>
<p>深度学习系统中最消耗计算资源的是训练阶段，一些研究已在尝试提升设置学习率的新方法。例如， <strong>Ronan Collobert</strong> 通过取 <span><span class="MathJax_Preview">fan\text{-}in</span><script type="math/tex">fan\text{-}in</script></span> 的神经元 <span><span class="MathJax_Preview">(n^{(l)})</span><script type="math/tex">(n^{(l)})</script></span> 的平方根的倒数来缩放权值 <span><span class="MathJax_Preview">W_{ij}</span><script type="math/tex">W_{ij}</script></span> ( <span><span class="MathJax_Preview">W\in \mathbb{R}^{n^{(l+1)}\times n^{(l)}}</span><script type="math/tex">W\in \mathbb{R}^{n^{(l+1)}\times n^{(l)}}</script></span>) 的学习率。</p>
<p>还有其他已经被证明有效的技术-这个方法叫 <strong>annealing</strong> <strong>退火</strong>，在多次迭代之后，学习率以以下方式降低：保证以一个高的的学习率开始训练和快速逼近最小值；当越来越接近最小值时，开始降低学习率，让我们可以在更细微的范围内找到最优值。一个常见的实现 <strong>annealing</strong> 的方法是在每 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 次的迭代学习后，通过一个因子 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 来降低学习率 <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 。</p>
<p>指数衰减也是很常见的方法，在 <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 次迭代后学习率变为 <span><span class="MathJax_Preview">\alpha(t)=\alpha_{0}e^{-kt}</span><script type="math/tex">\alpha(t)=\alpha_{0}e^{-kt}</script></span> ，其中 <span><span class="MathJax_Preview">\alpha_{0}</span><script type="math/tex">\alpha_{0}</script></span> 是初始的学习率和 <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> 是超参数。</p>
<p>还有另外一种方法是允许学习率随着时间减少：</p>
<div>
<div class="MathJax_Preview">
\alpha(t)=\frac{\alpha_{0}\tau}{max(t,\tau)}
</div>
<script type="math/tex; mode=display">
\alpha(t)=\frac{\alpha_{0}\tau}{max(t,\tau)}
</script>
</div>
<p>在上述的方案中， <span><span class="MathJax_Preview">\alpha_{0}</span><script type="math/tex">\alpha_{0}</script></span> 是一个可调的参数，代表起始的学习率。 <span><span class="MathJax_Preview">\tau</span><script type="math/tex">\tau</script></span> 也是一个可调参数，表示学习率应该在该时间点开始减少。在实际中，这个方法是很有效的。在下一部分我们讨论另外一种不需要手动设定学习率的自适应梯度下降的方法。</p>
<h4 id="28-momentum-updates">2.8 Momentum Updates<a class="headerlink" href="#28-momentum-updates" title="Permanent link">&para;</a></h4>
<p>动量方法，灵感来自于物理学中的对动力学的研究，是梯度下降方法的一种变体，尝试使用更新的“速度”的一种更有效的更新方案。动量更新的伪代码如下所示：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Computes a standard momentum update</span>
<span class="c1"># on parameters x</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_x</span>
<span class="n">x</span> <span class="o">+=</span> <span class="n">v</span>
</code></pre></div>

<h4 id="29-adaptive-optimization-methods">2.9 Adaptive Optimization Methods<a class="headerlink" href="#29-adaptive-optimization-methods" title="Permanent link">&para;</a></h4>
<p><strong>AdaGrad</strong> 是标准的随机梯度下降( <strong>SGD</strong> )的一种实现，但是有一点关键的不同：对每个参数学习率是不同的。每个参数的学习率取决于每个参数梯度更新的历史,参数的历史更新越小，就使用更大的学习率加快更新。换句话说，过去没有更新太大的参数现在更有可能有更高的学习率。</p>
<div>
<div class="MathJax_Preview">
\theta_{t,i}=\theta_{t-1,i}-\frac{\alpha}{\sqrt{\sum_{\tau=1}^{t}g_{\tau,i}^{2}}} g_{t,i} \ where \ g_{t,i}=\frac{\partial}{\partial\theta_{i}^{t}}J_{t}(\theta)
</div>
<script type="math/tex; mode=display">
\theta_{t,i}=\theta_{t-1,i}-\frac{\alpha}{\sqrt{\sum_{\tau=1}^{t}g_{\tau,i}^{2}}} g_{t,i} \ where \ g_{t,i}=\frac{\partial}{\partial\theta_{i}^{t}}J_{t}(\theta)
</script>
</div>
<p>在这个技术中，我们看到如果梯度的历史 <strong>RMS</strong> 很低，那么学习率会非常高。这个技术的一个简单的实现如下所示：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Assume the gradient dx and parameter vector x</span>
<span class="n">cache</span> <span class="o">+=</span> <span class="n">dx</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div>

<p>其他常见的自适应方法有 <strong>RMSProp</strong> 和 <strong>Adam</strong> ，其更新规则如下所示：</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Update rule for RMS prop</span>
<span class="n">cache</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">cache</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>

<span class="c1"># Update rule for Adam</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dx</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</code></pre></div>

<ul>
<li>
<p><strong>RMSProp</strong> 是利用平方梯度的移动平局值，是 <strong>AdaGrad</strong> 的一个变体——实际上，和 <strong>AdaGrad</strong> 不一样，<strong>它的更新不会单调变小</strong>。</p>
</li>
<li>
<p><strong>Adam</strong> 更新规则又是 <strong>RMSProp</strong> 的一个变体，但是加上了动量更新。</p>
</li>
</ul>
<h4 id="210-more-reference">2.10 More reference<a class="headerlink" href="#210-more-reference" title="Permanent link">&para;</a></h4>
<p>如果希望了解以上的梯度优化算法的具体细节，可以阅读这篇文章： <a href="http://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a>) 。</p>
<h2 id="computing-neural-network-gradients">Computing Neural Network Gradients<a class="headerlink" href="#computing-neural-network-gradients" title="Permanent link">&para;</a></h2>
<h3 id="1-introduction">1 Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h3>
<p>这些笔记的目的是演示如何以完全矢量化的方式快速计算神经网络梯度。这是对CS224n 2019第三讲的最后一部分的补充，是同样的材料</p>
<h3 id="2-vectorized-gradients">2 Vectorized Gradients<a class="headerlink" href="#2-vectorized-gradients" title="Permanent link">&para;</a></h3>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<p>以下是学习本课程时的可用参考书籍：</p>
<p><a href="https://item.jd.com/12355569.html">《基于深度学习的自然语言处理》</a> （车万翔老师等翻译）</p>
<p><a href="https://nndl.github.io/">《神经网络与深度学习》</a></p>
<p>以下是整理笔记的过程中参考的博客：</p>
<p><a href="https://zhuanlan.zhihu.com/p/59011576">斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p>
<p><a href="https://zhuanlan.zhihu.com/p/31977759">斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> <span class="critic comment">这是针对note部分的翻译</span></p>
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>var disqus_config=function(){this.page.url="None",this.page.identifier="None"};!function(){var e=document,i=e.createElement("script");i.src="//https-looperxx-github-io-my-wiki.disqus.com/embed.js",i.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(i)}()</script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="">
        
          <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" title="02 Word Vectors 2 and Word Senses" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                02 Word Vectors 2 and Word Senses
              </div>
            </div>
          </a>
        
        
          <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" title="04 Backpropagation and Computation Graphs" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  下一页
                </span>
                04 Backpropagation and Computation Graphs
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 - 2020 Xiao Xu
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="https://github.com/looperXX" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
      </a>
    
      
      
      <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.c51dfa35.min.js"></script>
      <script src="../assets/javascripts/bundle.eaaa3931.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "", "search.config.separator": "[\\uff0c\\u3002]+", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.58d22e8e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
    
  </body>
</html>