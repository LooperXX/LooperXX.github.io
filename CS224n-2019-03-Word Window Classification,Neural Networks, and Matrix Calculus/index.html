
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Xiao Xu - Homepage">
      
      
        <meta name="author" content="Xiao Xu">
      
      
        <link rel="canonical" href="https://looperxx.github.io/CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/">
      
      
        <link rel="prev" href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/">
      
      
        <link rel="next" href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/">
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.0.11">
    
    
      
        <title>03 Word Window Classification,Neural Networks, and Matrix Calculus - The Sun Also Rises.</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.0d440cfe.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.2505c338.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  


  <script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-CV5JZHXZY8"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-CV5JZHXZY8",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-CV5JZHXZY8",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>

  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href=".." title="The Sun Also Rises." class="md-header__button md-logo" aria-label="The Sun Also Rises." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            The Sun Also Rises.
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              03 Word Window Classification,Neural Networks, and Matrix Calculus
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
          
          <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/LooperXX/LooperXX.github.io" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Xiao Xu @ HIT-SCIR
    </a>
  </li>

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../blog/BridgeTower/" class="md-tabs__link">
        Blogs
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../Normalization/" class="md-tabs__link">
        Notes
      </a>
    </li>
  

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../CS224n-2019-00-Info/" class="md-tabs__link md-tabs__link--active">
        Notes on CS224n-2019
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../MkDocs_demo/" class="md-tabs__link">
        Notes on MkDocs
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="The Sun Also Rises." class="md-nav__button md-logo" aria-label="The Sun Also Rises." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    The Sun Also Rises.
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/LooperXX/LooperXX.github.io" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Xiao Xu @ HIT-SCIR
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Blogs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Blogs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blog/BridgeTower/" class="md-nav__link">
        AAAI 2023 (Oral) | BridgeTower: 在视觉语言表示学习中建立起编码器之间的桥梁
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../blog/Profile%20SLU/" class="md-nav__link">
        AAAI 2022 (Oral) | Profile SLU: 基于Profile信息的口语语言理解基准
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Normalization/" class="md-nav__link">
        Normalization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Transfer%20Learning/" class="md-nav__link">
        Transfer Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Attention/" class="md-nav__link">
        Attention
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" class="md-nav__link">
        Machine Reading Comprehension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Notes%20on%20NCRF%2B%2B/" class="md-nav__link">
        NCRF++
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          Notes on CS224n-2019
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Notes on CS224n-2019
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-00-Info/" class="md-nav__link">
        CS224n-2019 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-Assignment/" class="md-nav__link">
        CS224n-2019 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-01-Introduction%20and%20Word%20Vectors/" class="md-nav__link">
        01 Introduction and Word Vectors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" class="md-nav__link">
        02 Word Vectors 2 and Word Senses
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          03 Word Window Classification,Neural Networks, and Matrix Calculus
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        03 Word Window Classification,Neural Networks, and Matrix Calculus
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" class="md-nav__link">
    Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus
  </a>
  
    <nav class="md-nav" aria-label="Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-setup-and-notation" class="md-nav__link">
    Classification setup and notation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-network-classifiers" class="md-nav__link">
    Neural Network Classifiers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named-entity-recognition-ner" class="md-nav__link">
    Named Entity Recognition (NER)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#binary-word-window-classification" class="md-nav__link">
    Binary word window classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradients" class="md-nav__link">
    Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-03-neural-networks-backpropagation" class="md-nav__link">
    Notes 03 Neural Networks, Backpropagation
  </a>
  
    <nav class="md-nav" aria-label="Notes 03 Neural Networks, Backpropagation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-neural-networks-foundations" class="md-nav__link">
    1 Neural Networks: Foundations
  </a>
  
    <nav class="md-nav" aria-label="1 Neural Networks: Foundations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-a-neuron" class="md-nav__link">
    1.1 A Neuron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-a-single-layer-of-neurons" class="md-nav__link">
    1.2 A Single Layer of Neurons
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-feed-forward-computation" class="md-nav__link">
    1.3 Feed-forward Computation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-maximum-margin-objective-function" class="md-nav__link">
    1.4 Maximum Margin Objective Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-training-with-backpropagation-elemental" class="md-nav__link">
    1.5 Training with Backpropagation – Elemental
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-training-with-backpropagation-vectorized" class="md-nav__link">
    1.6 Training with Backpropagation – Vectorized
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-neural-networks-tips-and-tricks" class="md-nav__link">
    2 Neural Networks: Tips and Tricks
  </a>
  
    <nav class="md-nav" aria-label="2 Neural Networks: Tips and Tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-gradient-check" class="md-nav__link">
    2.1 Gradient Check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-regularization" class="md-nav__link">
    2.2 Regularization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-dropout" class="md-nav__link">
    2.3 Dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-neuron-units" class="md-nav__link">
    2.4 Neuron Units
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-data-preprocessing" class="md-nav__link">
    2.5 Data Preprocessing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26-parameter-initialization" class="md-nav__link">
    2.6 Parameter Initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27-learning-strategies" class="md-nav__link">
    2.7 Learning Strategies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#28-momentum-updates" class="md-nav__link">
    2.8 Momentum Updates
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#29-adaptive-optimization-methods" class="md-nav__link">
    2.9 Adaptive Optimization Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#210-more-reference" class="md-nav__link">
    2.10 More reference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-neural-network-gradients" class="md-nav__link">
    Computing Neural Network Gradients
  </a>
  
    <nav class="md-nav" aria-label="Computing Neural Network Gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1 Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-vectorized-gradients" class="md-nav__link">
    2 Vectorized Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" class="md-nav__link">
        04 Backpropagation and Computation Graphs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" class="md-nav__link">
        05 Linguistic Structure Dependency Parsing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" class="md-nav__link">
        06 The probability of a sentence Recurrent Neural Networks and Language Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" class="md-nav__link">
        07 Vanishing Gradients and Fancy RNNs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/" class="md-nav__link">
        08 Machine Translation, Sequence-to-sequence and Attention
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" class="md-nav__link">
        09 Practical Tips for Final Projects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/" class="md-nav__link">
        10 Question Answering and the Default Final Project
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" class="md-nav__link">
        11 ConvNets for NLP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/" class="md-nav__link">
        12 Information from parts of words Subword Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/" class="md-nav__link">
        13 Modeling contexts of use Contextual Representations and Pretraining
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/" class="md-nav__link">
        14 Transformers and Self-Attention For Generative Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-15-Natural%20Language%20Generation/" class="md-nav__link">
        15 Natural Language Generation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-16-Coreference%20Resolution/" class="md-nav__link">
        16 Coreference Resolution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-17-Multitask%20Learning/" class="md-nav__link">
        17 Multitask Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/" class="md-nav__link">
        18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/" class="md-nav__link">
        19 Safety, Bias, and Fairness
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/" class="md-nav__link">
        20 The Future of NLP + Deep Learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Notes on MkDocs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Notes on MkDocs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../MkDocs_demo/" class="md-nav__link">
        Demo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Material%20Theme%20Tutorial/" class="md-nav__link">
        Material Theme Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" class="md-nav__link">
    Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus
  </a>
  
    <nav class="md-nav" aria-label="Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classification-setup-and-notation" class="md-nav__link">
    Classification setup and notation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-network-classifiers" class="md-nav__link">
    Neural Network Classifiers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#named-entity-recognition-ner" class="md-nav__link">
    Named Entity Recognition (NER)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#binary-word-window-classification" class="md-nav__link">
    Binary word window classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradients" class="md-nav__link">
    Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-03-neural-networks-backpropagation" class="md-nav__link">
    Notes 03 Neural Networks, Backpropagation
  </a>
  
    <nav class="md-nav" aria-label="Notes 03 Neural Networks, Backpropagation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-neural-networks-foundations" class="md-nav__link">
    1 Neural Networks: Foundations
  </a>
  
    <nav class="md-nav" aria-label="1 Neural Networks: Foundations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-a-neuron" class="md-nav__link">
    1.1 A Neuron
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-a-single-layer-of-neurons" class="md-nav__link">
    1.2 A Single Layer of Neurons
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-feed-forward-computation" class="md-nav__link">
    1.3 Feed-forward Computation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-maximum-margin-objective-function" class="md-nav__link">
    1.4 Maximum Margin Objective Function
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-training-with-backpropagation-elemental" class="md-nav__link">
    1.5 Training with Backpropagation – Elemental
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-training-with-backpropagation-vectorized" class="md-nav__link">
    1.6 Training with Backpropagation – Vectorized
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-neural-networks-tips-and-tricks" class="md-nav__link">
    2 Neural Networks: Tips and Tricks
  </a>
  
    <nav class="md-nav" aria-label="2 Neural Networks: Tips and Tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-gradient-check" class="md-nav__link">
    2.1 Gradient Check
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-regularization" class="md-nav__link">
    2.2 Regularization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-dropout" class="md-nav__link">
    2.3 Dropout
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-neuron-units" class="md-nav__link">
    2.4 Neuron Units
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-data-preprocessing" class="md-nav__link">
    2.5 Data Preprocessing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26-parameter-initialization" class="md-nav__link">
    2.6 Parameter Initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27-learning-strategies" class="md-nav__link">
    2.7 Learning Strategies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#28-momentum-updates" class="md-nav__link">
    2.8 Momentum Updates
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#29-adaptive-optimization-methods" class="md-nav__link">
    2.9 Adaptive Optimization Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#210-more-reference" class="md-nav__link">
    2.10 More reference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#computing-neural-network-gradients" class="md-nav__link">
    Computing Neural Network Gradients
  </a>
  
    <nav class="md-nav" aria-label="Computing Neural Network Gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-introduction" class="md-nav__link">
    1 Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-vectorized-gradients" class="md-nav__link">
    2 Vectorized Gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  



  <h1>03 Word Window Classification,Neural Networks, and Matrix Calculus</h1>

<h2 id="lecture-03-word-window-classificationneural-networks-and-matrix-calculus">Lecture 03 Word Window Classification,Neural Networks, and Matrix Calculus<a class="headerlink" href="#lecture-03-word-window-classificationneural-networks-and-matrix-calculus" title="Permanent link">&para;</a></h2>
<p><strong>Lecture Plan</strong></p>
<ul>
<li>Classification review/introduction</li>
<li>Neural networks introduction</li>
<li>Named Entity Recognition</li>
<li>Binary true vs. corrupted word window classification</li>
<li>Matrix calculus introduction</li>
</ul>
<p><strong>提示</strong>：这对一些人而言将是困难的一周，课后需要阅读提供的资料。</p>
<h3 id="classification-setup-and-notation">Classification setup and notation<a class="headerlink" href="#classification-setup-and-notation" title="Permanent link">&para;</a></h3>
<p>通常我们有由样本组成的训练数据集</p>
<div class="arithmatex">\[
\left\{x_{i}, y_{i}\right\}_{i=1}^{N}
\]</div>
<p><span class="arithmatex">\(x_i\)</span> 是输入，例如单词（索引或是向量），句子，文档等等，维度为 <span class="arithmatex">\(d\)</span></p>
<p><span class="arithmatex">\(y_i\)</span> 是我们尝试预测的标签（ <span class="arithmatex">\(C\)</span> 个类别中的一个），例如：</p>
<ul>
<li>类别：感情，命名实体，购买/售出的决定</li>
<li>其他单词</li>
<li>之后：多词序列的</li>
</ul>
<p><strong>Classification intuition</strong></p>
<p><img alt="1560343189656" src="../imgs/1560343189656.png" /></p>
<p>训练数据： <span class="arithmatex">\(\left\{x_{i}, y_{i}\right\}_{i=1}^{N}\)</span></p>
<p>简单的说明情况</p>
<ul>
<li>固定的二维单词向量分类</li>
<li>使用softmax/logistic回归</li>
<li>线性决策边界</li>
</ul>
<p><strong>传统的机器学习/统计学方法</strong>：假设 <span class="arithmatex">\(x_i\)</span> 是固定的，训练 softmax/logistic 回归的权重 <span class="arithmatex">\(W \in \mathbb{R}^{C \times d}\)</span> 来决定决定边界(超平面)</p>
<p><strong>方法</strong>：对每个 <span class="arithmatex">\(x\)</span> ，预测
$$
p(y | x)=\frac{\exp \left(W_{y} . x\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}
$$
我们可以将预测函数分为两个步骤：</p>
<ol>
<li>
<p>将 <span class="arithmatex">\(W\)</span> 的 <span class="arithmatex">\(y^{th}\)</span> 行和 <span class="arithmatex">\(x\)</span> 中的对应行相乘得到分数
    $$
    W_{y} \cdot x=\sum_{i=1}^{d} W_{y i} x_{i}=f_{y}
    $$</p>
<p>计算所有的 <span class="arithmatex">\(f_c, for \ c=1,\dots,C\)</span></p>
</li>
<li>
<p>使用softmax函数获得归一化的概率</p>
</li>
</ol>
<div class="arithmatex">\[
p(y | x)=\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}=\operatorname{softmax}\left(f_{y}\right)
\]</div>
<p><strong>Training with softmax and cross-entropy loss</strong></p>
<p>对于每个训练样本 <span class="arithmatex">\((x,y)\)</span> ，我们的目标是最大化正确类 <span class="arithmatex">\(y\)</span> 的概率，或者我们可以最小化该类的负对数概率
$$
-\log p(y | x)=-\log \left(\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}\right)
$$
<strong>Background: What is “cross entropy” loss/error?</strong></p>
<ul>
<li>交叉熵”的概念来源于信息论，衡量两个分布之间的差异</li>
<li>令真实概率分布为 <span class="arithmatex">\(p\)</span></li>
<li>令我们计算的模型概率为 <span class="arithmatex">\(q\)</span></li>
<li>交叉熵为</li>
</ul>
<div class="arithmatex">\[
H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)
\]</div>
<ul>
<li>假设 groud truth (or true or gold or target)的概率分布在正确的类上为1，在其他任何地方为0：<span class="arithmatex">\(p = [0,…,0,1,0,…0]\)</span> </li>
<li>因为 <span class="arithmatex">\(p\)</span> 是独热向量，所以唯一剩下的项是真实类的负对数概率</li>
</ul>
<p><strong>Classification over a full dataset</strong></p>
<p>在整个数据集 <span class="arithmatex">\(\left\{x_{i}, y_{i}\right\}_{i=1}^{N}\)</span> 上的交叉熵损失函数，是所有样本的交叉熵的均值</p>
<div class="arithmatex">\[
J(\theta)=\frac{1}{N} \sum_{i=1}^{N}-\log \left(\frac{e^{f_{y_{i}}}}{\sum_{c=1}^{C} e^{f_{c}}}\right)
\]</div>
<p>我们不使用</p>
<div class="arithmatex">\[
f_{y}=f_{y}(x)=W_{y} \cdot x=\sum_{j=1}^{d} W_{y j} x_{j}
\]</div>
<p>我们使用矩阵来表示 <span class="arithmatex">\(f\)</span></p>
<div class="arithmatex">\[
f = Wx
\]</div>
<p><strong>Traditional ML optimization</strong></p>
<ul>
<li>一般机器学习的参数 <span class="arithmatex">\(\theta\)</span> 通常只由W的列组成</li>
</ul>
<div class="arithmatex">\[
\theta=\left[\begin{array}{c}{W_{\cdot 1}} \\ {\vdots} \\ {W_{\cdot d}}\end{array}\right]=W( :) \in \mathbb{R}^{C d}
\]</div>
<ul>
<li>因此，我们只通过以下方式更新决策边界</li>
</ul>
<div class="arithmatex">\[
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d}}}\end{array}\right] \in \mathbb{R}^{C d}
\]</div>
<h3 id="neural-network-classifiers">Neural Network Classifiers<a class="headerlink" href="#neural-network-classifiers" title="Permanent link">&para;</a></h3>
<p><img alt="1560345898614" src="../imgs/1560345898614.png" /></p>
<ul>
<li>单独使用Softmax(≈logistic回归)并不十分强大</li>
<li>Softmax只给出线性决策边界<ul>
<li>这可能是相当有限的，当问题很复杂时是无用的</li>
<li>纠正这些错误不是很酷吗?</li>
</ul>
</li>
</ul>
<p><strong>Neural Nets for the Win!</strong></p>
<p>神经网络可以学习更复杂的函数和非线性决策边界</p>
<p><img alt="1560346033994" src="../imgs/1560346033994.png" /></p>
<p>更高级的分类需要</p>
<ul>
<li>词向量</li>
<li>更深层次的深层神经网络</li>
</ul>
<p><strong>Classification difference with word vectors</strong></p>
<p>一般在NLP深度学习中</p>
<ul>
<li>我们学习了矩阵 <span class="arithmatex">\(W\)</span> 和词向量 <span class="arithmatex">\(x\)</span></li>
<li>我们学习传统参数和表示</li>
<li>词向量是对独热向量的重新表示——在中间层向量空间中移动它们——以便使用(线性)softmax分类器通过 x = Le 层进行分类<ul>
<li>即将词向量理解为一层神经网络，输入单词的独热向量并获得单词的词向量表示，并且我们需要对其进行更新。其中，<span class="arithmatex">\(Vd\)</span> 是数量很大的参数</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[
\nabla_{\theta} J(\theta)=\left[\begin{array}{c}{\nabla_{W_{1}}} \\ {\vdots} \\ {\nabla_{W_{d a r d v a r k}}} \\ {\vdots} \\ {\nabla_{x_{z e b r a}}}\end{array}\right] \in \mathbb{R}^{C d + V d}
\]</div>
<p><strong>Neural computation</strong></p>
<p><img alt="1560346664232" src="../imgs/1560346664232.png" /></p>
<p><strong>An artificial neuron</strong></p>
<ul>
<li>神经网络有自己的术语包</li>
<li>但如果你了解 softmax 模型是如何工作的，那么你就可以很容易地理解神经元的操作</li>
</ul>
<p><img alt="1560346716435" src="../imgs/1560346716435.png" /></p>
<p><strong>A neuron can be a binary logistic regression unit</strong></p>
<p><span class="arithmatex">\(f = \text{nonlinear activation fct. (e.g. sigmoid)} , w =\text{weights}, b =\text{bias}, h =\text{hidden}, x = \text{inputs}\)</span>
$$
\begin{array}{l}{h_{w, b}(x)=f\left(w^{\top} x+b\right)} \ {f(z)=\frac{1}{1+e^{-z}}}\end{array}
$$
<span class="arithmatex">\(b\)</span> : 我们可以有一个“总是打开”的特性，它给出一个先验类，或者将它作为一个偏向项分离出来</p>
<p><span class="arithmatex">\(w,b\)</span> 是神经元的参数</p>
<p><strong>A neural network</strong>
<strong>= running several logistic regressions at the same time</strong></p>
<p><img alt="1560347357837" src="../imgs/1560347357837.png" /></p>
<p>如果我们输入一个向量通过一系列逻辑回归函数，那么我们得到一个输出向量，但是我们不需要提前决定这些逻辑回归试图预测的变量是什么。</p>
<p><img alt="1560347481494" src="../imgs/1560347481494.png" /></p>
<p>我们可以输入另一个logistic回归函数。损失函数将指导中间隐藏变量应该是什么，以便更好地预测下一层的目标。我们当然可以使用更多层的神经网络。</p>
<p><strong>Matrix notation for a layer</strong></p>
<p><img alt="1560347762809" src="../imgs/1560347762809.png" />
$$
\begin{array}{l}{a_{1}=f\left(W_{11} x_{1}+W_{12} x_{2}+W_{13} x_{3}+b_{1}\right)} \ {a_{2}=f\left(W_{21} x_{1}+W_{22} x_{2}+W_{23} x_{3}+b_{2}\right)}\ {z=W x+b} \ {a=f(z)} \ f\left(\left[z_{1}, z_{2}, z_{3}\right]\right)=\left[f\left(z_{1}\right), f\left(z_{2}\right), f\left(z_{3}\right)\right] \end{array}
$$</p>
<ul>
<li><span class="arithmatex">\(f(x)\)</span> 在运算时是 element-wise 逐元素的</li>
</ul>
<p><strong>Non-linearities (aka “f ”): Why they’re needed</strong></p>
<p>例如：函数近似，如回归或分类</p>
<ul>
<li>没有非线性，深度神经网络只能做线性变换</li>
<li>多个线性变换可以组成一个的线性变换 <span class="arithmatex">\(W_1 W_2 x = Wx\)</span> <ul>
<li>因为线性变换是以某种方式旋转和拉伸空间，多次的旋转和拉伸可以融合为一次线性变换</li>
</ul>
</li>
<li>对于非线性函数而言，使用更多的层，他们可以近似更复杂的函数</li>
</ul>
<h3 id="named-entity-recognition-ner">Named Entity Recognition (NER)<a class="headerlink" href="#named-entity-recognition-ner" title="Permanent link">&para;</a></h3>
<ul>
<li>任务：例如，查找和分类文本中的名称</li>
</ul>
<p><img alt="1560359392887" src="../imgs/1560359392887.png" /></p>
<ul>
<li>可能的用途<ul>
<li>跟踪文档中提到的特定实体（组织、个人、地点、歌曲名、电影名等）</li>
<li>对于问题回答，答案通常是命名实体</li>
<li>许多需要的信息实际上是命名实体之间的关联</li>
<li>同样的技术可以扩展到其他 slot-filling 槽填充 分类</li>
</ul>
</li>
<li>通常后面是命名实体链接/规范化到知识库</li>
</ul>
<p><strong>Named Entity Recognition on word sequences</strong></p>
<p><img alt="1560359650543" src="../imgs/1560359650543.png" /></p>
<p>我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体</p>
<p><strong>Why might NER be hard?</strong></p>
<ul>
<li>很难计算出实体的边界<ul>
<li><img alt="1560359674788" src="../imgs/1560359674788.png" /></li>
<li>第一个实体是 “First National Bank” 还是 “National Bank”</li>
</ul>
</li>
<li>很难知道某物是否是一个实体<ul>
<li>是一所名为“Future School” 的学校，还是这是一所未来的学校？</li>
</ul>
</li>
<li>很难知道未知/新奇实体的类别<ul>
<li><img alt="1560359774508" src="../imgs/1560359774508.png" /></li>
<li>“Zig Ziglar” ?  一个人</li>
</ul>
</li>
<li>实体类是模糊的，依赖于上下文<ul>
<li><img alt="1560359806734" src="../imgs/1560359806734.png" /></li>
<li>这里的“Charles Schwab”  是 PER
    不是 ORG</li>
</ul>
</li>
</ul>
<h3 id="binary-word-window-classification">Binary word window classification<a class="headerlink" href="#binary-word-window-classification" title="Permanent link">&para;</a></h3>
<p>为在上下文中的语言构建分类器</p>
<ul>
<li>一般来说，很少对单个单词进行分类</li>
<li>有趣的问题，如上下文歧义出现</li>
<li>例子：auto-antonyms<ul>
<li>"To sanction" can mean "to permit" or "to punish”</li>
<li>"To seed" can mean "to place seeds" or "to remove seeds"</li>
</ul>
</li>
<li>例子：解决模糊命名实体的链接<ul>
<li>Paris → Paris, France vs. Paris Hilton vs. Paris, Texas</li>
<li>Hathaway → Berkshire Hathaway vs. Anne Hathaway</li>
</ul>
</li>
</ul>
<p><strong>Window classification</strong></p>
<ul>
<li>思想：在<strong>相邻词的上下文窗口</strong>中对一个词进行分类</li>
<li>例如，上下文中一个单词的命名实体分类<ul>
<li>人、地点、组织、没有</li>
</ul>
</li>
<li>在上下文中对单词进行分类的一个简单方法可能是对窗口中的单词向量进行<strong>平均</strong>，并对平均向量进行分类<ul>
<li>问题：<strong>这会丢失位置信息</strong></li>
</ul>
</li>
</ul>
<p><strong>Window classification: Softmax</strong></p>
<ul>
<li>训练softmax分类器对中心词进行分类，方法是在一个窗口内<strong>将中心词周围的词向量串联起来</strong></li>
<li>例子：在这句话的上下文中对“Paris”进行分类，窗口长度为2</li>
</ul>
<p><img alt="1560360448681" src="../imgs/1560360448681.png" /></p>
<ul>
<li>结果向量 <span class="arithmatex">\(x_{window} = x \in R^{5d}\)</span>  是一个列向量</li>
</ul>
<p><strong>Simplest window classifier: Softmax</strong></p>
<p>对于 <span class="arithmatex">\(x = x_{window}\)</span> ，我们可以使用与之前相同的softmax分类器</p>
<p><img alt="1560360599779" src="../imgs/1560360599779.png" /></p>
<ul>
<li>如何更新向量？</li>
<li>简而言之：就像上周那样，求导和优化</li>
</ul>
<p><strong>Binary classification with unnormalized scores</strong></p>
<ul>
<li>
<p>之前的例子：<span class="arithmatex">\(X_{\text { window }}=[\begin{array}{ccc}{\mathrm{X}_{\text { museums }}} &amp; {\mathrm{X}_{\text { in }}} &amp; {\mathrm{X}_{\text { paris }} \quad \mathrm{X}_{\text { are }} \quad \mathrm{X}_{\text { amazing }} ]}\end{array}\)</span></p>
</li>
<li>
<p>假设我们要对中心词是否为一个地点，进行分类</p>
</li>
<li>与word2vec类似，我们将遍历语料库中的所有位置。但这一次，它将受到监督，只有一些位置能够得到高分。</li>
<li>例如，在他们的中心有一个实际的NER Location的位置是“真实的”位置会获得高分</li>
</ul>
<p><strong>Binary classification for NER Location</strong></p>
<ul>
<li>
<p>例子：Not all museums in Paris are amazing</p>
</li>
<li>
<p>这里：一个真正的窗口，以Paris为中心的窗口和所有其他窗口都“损坏”了，因为它们的中心没有指定的实体位置。</p>
<ul>
<li>museums in Paris are amazing</li>
</ul>
</li>
<li>
<p>“损坏”窗口很容易找到，而且有很多：任何中心词没有在我们的语料库中明确标记为NER位置的窗口</p>
<ul>
<li>Not all museums in Paris</li>
</ul>
</li>
</ul>
<p><strong>Neural Network Feed-forward Computation</strong></p>
<p>使用神经激活 <span class="arithmatex">\(a\)</span> 简单地给出一个非标准化的分数
$$
score(x)=U^{T} a \in \mathbb{R}
$$
我们用一个三层神经网络计算一个窗口的得分</p>
<ul>
<li><span class="arithmatex">\(s = score("museums  \ in \ Paris \ are \ amazing”)\)</span></li>
</ul>
<div class="arithmatex">\[
\begin{array}{l}{s=U^{T} f(W x+b)} \\ {x \in \mathbb{R}^{20 \times 1}, W \in \mathbb{R}^{8 \times 20}, U \in \mathbb{R}^{8 \times 1}}\end{array}
\]</div>
<p><img alt="1560361207976" src="../imgs/1560361207976.png" /></p>
<p><strong>Main intuition for extra layer</strong></p>
<p>中间层学习输入词向量之间的<strong>非线性交互</strong></p>
<p>例如：只有当“museum”是第一个向量时，“in”放在第二个位置才重要</p>
<p><strong>The max-margin loss</strong></p>
<p><img alt="1560361550807" src="../imgs/1560361550807.png" /></p>
<ul>
<li>关于训练目标的想法：让真实窗口的得分更高，而破坏窗口的得分更低(直到足够好为止)</li>
<li><span class="arithmatex">\(s = score("museums  \ in \ Paris \ are \ amazing”)\)</span></li>
<li><span class="arithmatex">\(s_c = score("Not \ all \ museums  \ in \ Paris)\)</span></li>
<li>最小化 <span class="arithmatex">\(J=\max \left(0,1-s+s_{c}\right)\)</span></li>
<li>
<p>这是不可微的，但它是连续的→我们可以用SGD。</p>
</li>
<li>
<p>每个选项都是连续的</p>
</li>
<li>
<p>单窗口的目标函数为</p>
</li>
</ul>
<div class="arithmatex">\[
J=\max \left(0,1-s+s_{c}\right)
\]</div>
<ul>
<li>每个中心有NER位置的窗口的得分应该比中心没有位置的窗口高1分</li>
</ul>
<p><img alt="1560361673756" src="../imgs/1560361673756.png" /></p>
<ul>
<li>要获得完整的目标函数：为每个真窗口采样几个损坏的窗口。对所有培训窗口求和</li>
<li>
<p>类似于word2vec中的负抽样</p>
</li>
<li>
<p>使用SGD更新参数</p>
<ul>
<li><span class="arithmatex">\(\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)\)</span></li>
<li><span class="arithmatex">\(a\)</span> 是 步长或是学习率</li>
</ul>
</li>
<li>如何计算 <span class="arithmatex">\(\nabla_{\theta} J(\theta)\)</span> ？<ul>
<li>手工计算（本课）</li>
<li>算法：反向传播（下一课）</li>
</ul>
</li>
</ul>
<p><strong>Computing Gradients by Hand</strong></p>
<ul>
<li>回顾多元导数</li>
<li>矩阵微积分：完全矢量化的梯度<ul>
<li>比非矢量梯度快得多，也更有用</li>
<li>但做一个非矢量梯度可以是一个很好的实践；以上周的讲座为例</li>
<li><strong>notes</strong> 更详细地涵盖了这些材料</li>
</ul>
</li>
</ul>
<h3 id="gradients">Gradients<a class="headerlink" href="#gradients" title="Permanent link">&para;</a></h3>
<ul>
<li>给定一个函数，有1个输出和1个输入</li>
</ul>
<div class="arithmatex">\[
f(x) = x^3
\]</div>
<ul>
<li>斜率是它的导数</li>
</ul>
<div class="arithmatex">\[
\frac{d f}{d x}=3 x^{2}
\]</div>
<ul>
<li>给定一个函数，有1个输出和 n 个输入</li>
</ul>
<div class="arithmatex">\[
f(\boldsymbol{x})=f\left(x_{1}, x_{2}, \ldots, x_{n}\right)
\]</div>
<ul>
<li>梯度是关于每个输入的偏导数的向量</li>
</ul>
<div class="arithmatex">\[
\frac{\partial f}{\partial \boldsymbol{x}}=\left[\frac{\partial f}{\partial x_{1}}, \frac{\partial f}{\partial x_{2}}, \ldots, \frac{\partial f}{\partial x_{n}}\right]
\]</div>
<p><strong>Jacobian Matrix: Generalization of the Gradient</strong></p>
<ul>
<li>给定一个函数，有 m 个输出和 n 个输入</li>
</ul>
<div class="arithmatex">\[
\boldsymbol{f}(\boldsymbol{x})=\left[f_{1}\left(x_{1}, x_{2}, \ldots, x_{n}\right), \ldots, f_{m}\left(x_{1}, x_{2}, \ldots, x_{n}\right)\right]
\]</div>
<ul>
<li>其雅可比矩阵是一个<span class="arithmatex">\(m \times n\)</span>的偏导矩阵</li>
</ul>
<div class="arithmatex">\[
\frac{\partial \boldsymbol{f}}{\partial \boldsymbol{x}}=\left[\begin{array}{ccc}{\frac{\partial f_{1}}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{1}}{\partial x_{n}}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial f_{m}}{\partial x_{1}}} &amp; {\cdots} &amp; {\frac{\partial f_{m}}{\partial x_{n}}}\end{array}\right]
\]</div>
<div class="arithmatex">\[
\left(\frac{\partial f}{\partial x}\right)_{i j}=\frac{\partial f_{i}}{\partial x_{j}}
\]</div>
<p><strong>Chain Rule</strong></p>
<p>对于单变量函数：乘以导数
$$
\begin{array}{l}{z=3 y} \ {y=x^{2}} \ {\frac{d z}{d x}=\frac{d z}{d y} \frac{d y}{d x}=(3)(2 x)=6 x}\end{array}
$$
对于一次处理多个变量：乘以雅可比矩阵
$$
\begin{array}{l}{\textbf{h}=f(\textbf{z})} \ {\textbf{z}=\textbf{W} \textbf{x}+\textbf{b}} \ {\frac{\partial \textbf{h}}{\partial \textbf{x}}=\frac{\partial \textbf{h}}{\partial \textbf{z}} \frac{\partial \textbf{z}}{\partial \textbf{x}}=\dots}\end{array}
$$
<strong>Example Jacobian: Elementwise activation Function</strong></p>
<p><span class="arithmatex">\(h=f(z)\)</span> , <span class="arithmatex">\(\frac{\partial \textbf{h}}{\partial \textbf{z}} = ?, \textbf{h},\textbf{z} \in \mathbb{R}^{n}\)</span>  </p>
<p>由于使用的是 element-wise，所以 <span class="arithmatex">\(h_{i}=f\left(z_{i}\right)\)</span></p>
<p>函数有n个输出和n个输入 → n×n 的雅可比矩阵</p>
<div class="arithmatex">\[
\begin{aligned}\left(\frac{\partial h}{\partial z}\right)_{i j} &amp;=\frac{\partial h_{i}}{\partial z_{j}}=\frac{\partial}{\partial z_{j}} f\left(z_{i}\right), \text{definition of Jacobian} \\ &amp;=\left\{\begin{array}{ll}{f^{\prime}\left(z_{i}\right)} &amp; {\text { if } i=j} \\ {0} &amp; {\text { if otherwise }} , \text{regular 1-variable derivative} \end{array}\right.\end{aligned}
\]</div>
<div class="arithmatex">\[
\frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}= \left(\begin{array}{ccc}{f^{\prime}\left(z_{1}\right)} &amp; { } &amp; {0} \\ {} &amp; {\ddots} &amp; { } \\ {0} &amp; { } &amp; {f^{\prime}\left(z_{n}\right)}\end{array}\right)=\operatorname{diag}\left(\boldsymbol{f}^{\prime}(\boldsymbol{z})\right)
\]</div>
<p><strong>Other Jacobians</strong>
$$
\begin{array}{l}{\frac{\partial}{\partial \boldsymbol{x}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{W}} \ {\frac{\partial}{\partial \boldsymbol{b}}(\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b})=\boldsymbol{I} \text { (Identity matrix) }} \ \frac{\partial}{\partial \boldsymbol{u}}\left(\boldsymbol{u}^{T} \boldsymbol{h}\right)=\boldsymbol{h}^{\boldsymbol{T}} \end{array}
$$
这是正确的雅可比矩阵。稍后我们将讨论“形状约定”；用它则答案是 <span class="arithmatex">\(h\)</span> 。</p>
<p><strong>Back to our Neural Net!</strong></p>
<p><img alt="1560363626037" src="../imgs/1560363626037.png" /></p>
<p>如何计算 <span class="arithmatex">\(\frac{\partial s}{\partial b}\)</span> ？</p>
<p>实际上，我们关心的是损失的梯度，但是为了简单起见，我们将计算分数的梯度</p>
<p><strong>Break up equations into simple pieces</strong></p>
<p><img alt="1560363713598" src="../imgs/1560363713598.png" /></p>
<p><strong>Apply the chain rule</strong>
$$
\begin{array}{l}{s=\boldsymbol{u}^{T} \boldsymbol{h}} \ {\boldsymbol{h}=f(\boldsymbol{z})} \ {\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}} \ {\boldsymbol{x}} \text{ (input) }\end{array}
$$</p>
<div class="arithmatex">\[
\frac{\partial s}{\partial \boldsymbol{b}}=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}
\]</div>
<p><img alt="1560363929934" src="../imgs/1560363929934.png" /></p>
<p>如何计算 <span class="arithmatex">\(\frac{\partial s}{\partial \textbf{W}}\)</span> ？
$$
\begin{aligned} \frac{\partial s}{\partial \boldsymbol{W}} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \ \frac{\partial s}{\partial \boldsymbol{b}} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}} \end{aligned}
$$
前两项是重复的，无须重复计算
$$
\begin{aligned} \frac{\partial s}{\partial \boldsymbol{W}} &amp;=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}} \ \frac{\partial s}{\partial \boldsymbol{b}} &amp;=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{b}}=\boldsymbol{\delta} \ \boldsymbol{\delta} &amp;=\frac{\partial s}{\partial \boldsymbol{h}} \frac{\partial \boldsymbol{h}}{\partial \boldsymbol{z}}=\boldsymbol{u}^{T} \circ f^{\prime}(\boldsymbol{z}) \end{aligned}
$$
其中，<span class="arithmatex">\(\delta\)</span> 是局部误差符号</p>
<p><strong>Derivative with respect to Matrix: Output shape</strong></p>
<ul>
<li><span class="arithmatex">\(\boldsymbol{W} \in \mathbb{R}^{n \times m}\)</span> ，<span class="arithmatex">\(\frac{\partial s}{\partial \boldsymbol{W}}\)</span> 的形状是</li>
<li>1个输出，<span class="arithmatex">\(n \times m\)</span> 个输入：1 × nm 的雅可比矩阵？<ul>
<li>不方便更新参数 <span class="arithmatex">\(\theta^{n e w}=\theta^{o l d}-\alpha \nabla_{\theta} J(\theta)\)</span></li>
</ul>
</li>
<li>而是遵循惯例：导数的形状是参数的形状 （形状约定）<ul>
<li><span class="arithmatex">\(\frac{\partial s}{\partial \boldsymbol{W}}\)</span> 的形状是 <span class="arithmatex">\(n \times m\)</span> </li>
</ul>
</li>
</ul>
<div class="arithmatex">\[
\left[\begin{array}{ccc}{\frac{\partial s}{\partial W_{11}}} &amp; {\cdots} &amp; {\frac{\partial s}{\partial W_{1 m}}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial s}{\partial W_{n 1}}} &amp; {\cdots} &amp; {\frac{\partial s}{\partial W_{n m}}}\end{array}\right]
\]</div>
<p><strong>Derivative with respect to Matrix</strong></p>
<ul>
<li><span class="arithmatex">\(\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta} \frac{\partial \boldsymbol{z}}{\partial \boldsymbol{W}}\)</span><ul>
<li><span class="arithmatex">\(\delta\)</span> 将出现在我们的答案中</li>
<li>另一项应该是 <span class="arithmatex">\(x\)</span> ，因为 <span class="arithmatex">\(\boldsymbol{z}=\boldsymbol{W} \boldsymbol{x}+\boldsymbol{b}\)</span> </li>
</ul>
</li>
<li>这表明 <span class="arithmatex">\(\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}\)</span></li>
</ul>
<p><img alt="1560364755148" src="../imgs/1560364755148.png" /></p>
<p><strong>Why the Transposes?</strong>
$$
\begin{array}{l}{\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \quad \boldsymbol{x}^{T}} \ {[n \times m]} {[n \times 1]} {[1 \times m]}\end{array}
$$</p>
<ul>
<li>粗糙的回答是：这样就可以解决尺寸问题了<ul>
<li>检查工作的有用技巧</li>
</ul>
</li>
<li>课堂讲稿中有完整的解释<ul>
<li>每个输入到每个输出——你得到的是外部积</li>
</ul>
</li>
</ul>
<div class="arithmatex">\[
\frac{\partial s}{\partial \boldsymbol{W}}=\boldsymbol{\delta}^{T} \boldsymbol{x}^{T}=\left[\begin{array}{c}{\delta_{1}} \\ {\vdots} \\ {\delta_{n}}\end{array}\right]\left[x_{1}, \ldots, x_{m}\right]=\left[\begin{array}{ccc}{\delta_{1} x_{1}} &amp; {\dots} &amp; {\delta_{1} x_{m}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\delta_{n} x_{1}} &amp; {\dots} &amp; {\delta_{n} x_{m}}\end{array}\right]
\]</div>
<p><strong>What shape should derivatives be?</strong></p>
<ul>
<li><span class="arithmatex">\(\frac{\partial s}{\partial \boldsymbol{b}}=\boldsymbol{h}^{T} \circ f^{\prime}(\boldsymbol{z})\)</span> 是行向量<ul>
<li>但是习惯上说梯度应该是一个列向量因为 <span class="arithmatex">\(b\)</span> 是一个列向量</li>
</ul>
</li>
<li>
<p>雅可比矩阵形式(这使得链式法则很容易)和形状约定(这使得SGD很容易实现)之间的分歧</p>
<ul>
<li>我们希望答案遵循形状约定</li>
<li>但是雅可比矩阵形式对于计算答案很有用</li>
</ul>
</li>
<li>
<p>两个选择</p>
<ul>
<li>尽量使用雅可比矩阵形式，最后按照约定进行整形<ul>
<li>我们刚刚做的。但最后转置 <span class="arithmatex">\(\frac{\partial s}{\partial \boldsymbol{b}}\)</span> 使导数成为列向量，得到 <span class="arithmatex">\(\delta ^ T\)</span></li>
</ul>
</li>
<li>始终遵循惯例<ul>
<li>查看维度，找出何时转置 和/或 重新排序项。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>反向传播</p>
<ul>
<li>算法高效地计算梯度</li>
<li>将我们刚刚手工完成的转换成算法</li>
<li>用于深度学习软件框架(TensorFlow, PyTorch, Chainer, etc.)</li>
</ul>
<h2 id="notes-03-neural-networks-backpropagation">Notes 03 Neural Networks, Backpropagation<a class="headerlink" href="#notes-03-neural-networks-backpropagation" title="Permanent link">&para;</a></h2>
<p><strong>Keyphrases</strong>: Neural networks.Forward computation.Backward.propagation.Neuron Units.Max-margin Loss.Gradient checks.Xavier parameter initialization.Learning rates.Adagrad.</p>
<p><strong>概述</strong>：这组笔记介绍了单层和多层神经网络，以及如何将它们用于分类目的。然后我们讨论如何使用一种称为反向传播的分布式梯度下降技术来训练它们。我们将看到如何使用链式法则按顺序进行参数更新。在对神经网络进行严格的数学讨论之后，我们将讨论一些训练神经网络的实用技巧和技巧，包括:神经元单元(非线性)、梯度检查、Xavier参数初始化、学习率、Adagrad等。最后,我们将鼓励使用递归神经网络作为语言模型。</p>
<h3 id="1-neural-networks-foundations">1 Neural Networks: Foundations<a class="headerlink" href="#1-neural-networks-foundations" title="Permanent link">&para;</a></h3>
<p>在前面的讨论中认为，因为大部分数据是线性不可分的所以需要非线性分类器，不然的话线性分类器在这些数据上的表现是有限的。神经网络就是如下图所示的一类具有非线性决策分界的分类器。现在我们知道神经网络创建的决策边界，让我们看看这是如何创建的。</p>
<p>神经网络是受生物学启发的分类器，这就是为什么它们经常被称为“人工神经网络”，以区别于有机类。然而，在现实中，人类神经网络比人工神经网络更有能力、更复杂，因此通常最好不要在两者之间画太多的相似点。</p>
<p><img alt="1560405499464" src="../imgs/1560405499464.png" /></p>
<h4 id="11-a-neuron">1.1 A Neuron<a class="headerlink" href="#11-a-neuron" title="Permanent link">&para;</a></h4>
<p>神经元是一个通用的计算单元，它接受 <span class="arithmatex">\(n\)</span> 个输入并产生一个输出。不同的神经元根据它们不同的参数（一般认为是神经元的权值）会有不同的输出。对神经元来说一个常见的选择是 <span class="arithmatex">\(sigmoid\)</span> ，或者称为“二元逻辑回归”单元。这种神经元以 <span class="arithmatex">\(n\)</span> 维的向量作为输入，然后计算出一个激活标量（输出） <span class="arithmatex">\(a\)</span> 。该神经元还与一个 <span class="arithmatex">\(n\)</span> 维的权重向量 <span class="arithmatex">\(w\)</span> 和一个偏置标量 <span class="arithmatex">\(b\)</span> 相关联。这个神经元的输出是</p>
<div class="arithmatex">\[
a=\frac{1}{1+exp(-(w^{T}x+b))}
\]</div>
<p>我们也可以把上面公式中的权值和偏置项结合在一起：</p>
<div class="arithmatex">\[
a=\frac{1}{1+exp(-[w^{T}\;\;x]\cdot [x\;\;1])}
\]</div>
<p>这个公式可以以下图的形式可视化</p>
<p><img alt="1560405918139" src="../imgs/1560405918139.png" /></p>
<p>神经元是神经网络的基本组成部分。我们将看到神经元可以是许多允许非线性在网络中积累的函数之一。</p>
<h4 id="12-a-single-layer-of-neurons">1.2 A Single Layer of Neurons<a class="headerlink" href="#12-a-single-layer-of-neurons" title="Permanent link">&para;</a></h4>
<p>我们将上述思想扩展到多个神经元，考虑输入 <span class="arithmatex">\(x\)</span> 作为多个这样的神经元的输入，如下图所示。</p>
<p><img alt="1560405984674" src="../imgs/1560405984674.png" /></p>
<p>如果我们定义不同的神经元的权值为 <span class="arithmatex">\(\{w^{(1)},...,w^{(m)}\}\)</span> 、偏置为 <span class="arithmatex">\(\{b_{1},...,b_{m}\}\)</span> 和相对应的激活输出为 <span class="arithmatex">\(\{a_{1},...,a_{m}\}\)</span> ：</p>
<div class="arithmatex">\[
         \begin{matrix}         a_{1} =\frac{1}{1+exp(-(w^{(1)T}x+b))} \\ \vdots \\ a_{m} =\frac{1}{1+exp(-(w^{(m)T}x+b))}          \end{matrix}    \\
\]</div>
<p>让我们定义简化公式以便于更好地表达复杂的网络：</p>
<div class="arithmatex">\[
         \sigma(z) =         \begin{bmatrix}         \frac{1}{1+exp(z_{1})} \\ \vdots \\ \frac{1}{1+exp(z_{m})}         \end{bmatrix} \\         b =         \begin{bmatrix}         b_{1} \\ \vdots \\ b_{m}         \end{bmatrix} \in \mathbb{R}^{m} \\         W =         \begin{bmatrix}         -\;\;w^{(1)T}\;\;- \\ \cdots \\ -\;\;w^{(m)T}\;\;-         \end{bmatrix} \in \mathbb{R}^{m\times n}  \\
\]</div>
<p>我们现在可以将缩放和偏差的输出写成：</p>
<div class="arithmatex">\[
\mathbf{z}=\mathbf{W}\mathbf{x}+\mathbf{b}  \\
\]</div>
<p>激活函数 sigmoid 可以变为如下形式：</p>
<div class="arithmatex">\[
         \begin{bmatrix}         a_{1} \\ \vdots \\ a_{m}         \end{bmatrix} = \sigma(\mathbf{z}) = \sigma(\mathbf{W}\mathbf{x}+\mathbf{b})  \\
\]</div>
<p>那么这些激活的作用是什么呢？我们可以把这些激活看作是一些加权特征组合存在的指标。然后，我们可以使用这些激活的组合来执行分类任务。</p>
<h4 id="13-feed-forward-computation">1.3 Feed-forward Computation<a class="headerlink" href="#13-feed-forward-computation" title="Permanent link">&para;</a></h4>
<p>到目前为止我们知道一个输入向量 <span class="arithmatex">\(x\in \mathbb{R}^{n}\)</span> 可以经过一层 <span class="arithmatex">\(sigmoid\)</span> 单元的变换得到激活输出 <span class="arithmatex">\(a\in \mathbb{R}^{m}\)</span> 。但是这么做的直觉是什么呢？让我们考虑一个 NLP 中的命名实体识别问题作为例子：</p>
<div class="arithmatex">\[
Museums\;in\;Paris \;are\;amazing  \\
\]</div>
<p>这里我们想判断中心词  <span class="arithmatex">\(Paris\)</span> 是不是以命名实体。在这种情况下，我们很可能不仅想要捕捉窗口中单词的单词向量，还想要捕捉单词之间的一些其他交互，以便进行分类。例如，可能只有 <span class="arithmatex">\(Museums\)</span> 是第一个单词和 <span class="arithmatex">\(in\)</span> 是第二个单词的时候， <span class="arithmatex">\(Paris\)</span> 才是命名实体。这样的非线性决策通常不能被直接提供给Softmax函数的输入捕获，而是需要第1.2节中讨论的中间层进行评分。因此，我们可以使用另一个矩阵 <span class="arithmatex">\(\mathbf{U} \in \mathbb{R}^{m \times 1}\)</span> 与激活输出计算得到未归一化的得分用于分类任务：</p>
<div class="arithmatex">\[
s=\mathbf{U}^{T}a=\mathbf{U}^{T}f(Wx+b)
\]</div>
<p>其中 <span class="arithmatex">\(f\)</span> 是激活函数（例如 sigmoid 函数）。</p>
<p><strong>维度分析</strong>：如果我们使用 4 维的词向量来表示每个单词并使用 5 个词的窗口，则输入是 <span class="arithmatex">\(x\in \mathbb{R}^{20}\)</span> 。如果我们在隐藏层使用 8 个 sigmoid 单元和从激活函数中生成一个分数输出，其中 <span class="arithmatex">\(W\in \mathbb{R}^{8\times 20}\)</span> ， <span class="arithmatex">\(b\in \mathbb{R}^{8}\)</span> ， <span class="arithmatex">\(U\in \mathbb{R}^{8\times 1}\)</span>， <span class="arithmatex">\(s\in \mathbb{R}\)</span> 。</p>
<p><img alt="1560407221438" src="../imgs/1560407221438.png" /></p>
<h4 id="14-maximum-margin-objective-function">1.4 Maximum Margin Objective Function<a class="headerlink" href="#14-maximum-margin-objective-function" title="Permanent link">&para;</a></h4>
<p>类似很多的机器学习模型，神经网络需要一个优化目标函数，一个我们想要最小化或最大化的误差。这里我们讨论一个常用的误差度量方法：<strong>maximum margin objective</strong> 最大间隔目标函数。使用这个目标函数的背后的思想是保证对“真”标签数据的计算得分要比“假”标签数据的计算得分要高。</p>
<p>回到前面的例子，如果我们令“真”标签窗口 <span class="arithmatex">\(\text{Museums in Paris are amazing}\)</span> 的计算得分为 s ，令“假”标签窗口 <span class="arithmatex">\(\text{Not all museums in Paris}\)</span> 的计算得分为 <span class="arithmatex">\(s_{c}\)</span> （下标 <span class="arithmatex">\(c\)</span> 表示这个这个窗口 corrupt ）</p>
<p>然后，我们对目标函数最大化 <span class="arithmatex">\((s-s_{c})\)</span> 或者最小化 <span class="arithmatex">\((s_{c}-s)\)</span> 。然而，我们修改目标函数来保证误差仅在 <span class="arithmatex">\(s_{c} &gt; s \Rightarrow  (s_{c}-s) &gt; 0\)</span> 才进行计算。这样做的直觉是，我们只关心“正确”数据点的得分高于“错误”数据点，其余的都不重要。因此，当 <span class="arithmatex">\(s_{c} &gt; s\)</span> 则误差为 <span class="arithmatex">\((s_{c}-s)\)</span> ，否则为 0 。因此，我们的优化的目标函数现在为：</p>
<div class="arithmatex">\[
minimize\;J=max\,(s_{c}-s,0)
\]</div>
<p>然而，上面的优化目标函数是有风险的，因为它不能创造一个安全的间隔。我们希望“真”数据要比“假”数据的得分大于某个正的间隔 <span class="arithmatex">\(\Delta\)</span> 。换而言之，我们想要误差在 <span class="arithmatex">\((s-s_{c} &lt; \Delta)\)</span> 就开始计算，而不是当 <span class="arithmatex">\((s-s_{c} &lt; 0)\)</span> 时就计算。因此，我们修改优化目标函数为：</p>
<div class="arithmatex">\[
minimize\;J=max\,(\Delta+s_{c}-s,0)
\]</div>
<p>我们可以把这个间隔缩放使得 <span class="arithmatex">\(\Delta=1\)</span> ，让其他参数在优化过程中自动进行调整，并且不会影响模型的表现。如果想更多地了解这方面，可以去读一下 <span class="arithmatex">\(SVM\)</span> 中的函数间隔和几何间隔中的相关内容。最后，我们定义在所有训练窗口上的优化目标函数为：</p>
<div class="arithmatex">\[
minimize\;J=max\,(1+s_{c}-s,0)
\]</div>
<p>按照上面的公式有，<span class="arithmatex">\(s_{c}=\mathbf{U}^{T}f(Wx_{c}+b)\)</span> 和 <span class="arithmatex">\(s=\mathbf{U}^{T}f(Wx+b)\)</span>。</p>
<blockquote>
<p>最大边际目标函数通常与支持向量机一起使用</p>
</blockquote>
<h4 id="15-training-with-backpropagation-elemental">1.5 Training with Backpropagation – Elemental<a class="headerlink" href="#15-training-with-backpropagation-elemental" title="Permanent link">&para;</a></h4>
<p>在这部分我们讨论当1.4节中讨论的损失函数 <span class="arithmatex">\(J\)</span> 为正时，模型中不同参数时是如何训练的。如果损失为 0 时，那么不需要再更新参数。我们一般使用梯度下降（或者像 SGD 这样的变体）来更新参数，所以要知道在更新公式中需要的任意参数的梯度信息：</p>
<div class="arithmatex">\[
\theta^{(t+1)}=\theta^{(t)}-\alpha\nabla_{\theta^{(t)}}J 
\]</div>
<p>反向传播是一种利用微分链式法则来计算模型上任意参数的损失梯度的方法。为了更进一步理解反向传播，我们先看下图中的一个简单的网络  </p>
<p><img alt="1560408903195" src="../imgs/1560408903195.png" /></p>
<p>这里我们使用只有单个隐藏层和单个输出单元的神经网络。现在让我们先建立一些符号定义：</p>
<ul>
<li><span class="arithmatex">\(x_{i}\)</span> 是神经网络的输入</li>
<li><span class="arithmatex">\(s\)</span> 是神经网络的输出</li>
<li>每层（包括输入和输出层）的神经元都接收一个输入和生成一个输出。第 <span class="arithmatex">\(k\)</span> 层的第 <span class="arithmatex">\(j\)</span> 个神经元接收标量输入 <span class="arithmatex">\(z_{j}^{(k)}\)</span> 和生成一个标量激活输出 <span class="arithmatex">\(a_{j}^{(k)}\)</span> </li>
<li>我们把 <span class="arithmatex">\(z_{j}^{(k)}\)</span> 计算出的反向传播误差定义为 <span class="arithmatex">\(\delta_{j}^{(k)}\)</span> </li>
<li>第 <span class="arithmatex">\(1\)</span> 层是输入层，而不是第 <span class="arithmatex">\(1\)</span> 个隐藏层。对输入层而言， <span class="arithmatex">\(x_{j}^{(k)}=z_{j}^{(k)}=a_{j}^{(k)}\)</span> </li>
<li><span class="arithmatex">\(W^{(k)}\)</span> 是将第 k 层的输出映射到第 <span class="arithmatex">\(k+1\)</span> 层的输入的转移矩阵，因此将这个新的符号用在 <span class="arithmatex">\(Section\;1.3\)</span> 中的例子 <span class="arithmatex">\(W^{(1)}=W\)</span> 和 <span class="arithmatex">\(W^{(2)}=U\)</span> 。</li>
</ul>
<p><strong>现在开始反向传播</strong>：假设损失函数 <span class="arithmatex">\(J=(1+s_{c}-s)\)</span> 为正值，我们想更新参数 <span class="arithmatex">\(W_{14}^{(1)}\)</span> ，我们看到 <span class="arithmatex">\(W_{14}^{(1)}\)</span> 只参与了 <span class="arithmatex">\(z_{1}^{(2)}\)</span> 和 <span class="arithmatex">\(a_{1}^{(2)}\)</span> 的计算。这点对于理解反向传播是非常重要的——<strong>反向传播的梯度只受它们所贡献的值的影响</strong>。 <span class="arithmatex">\(a_{1}^{(2)}\)</span> 在随后的前向计算中和 <span class="arithmatex">\(W_{1}^{(2)}\)</span> 相乘计算得分。我们可以从最大间隔损失看到：
$$
\frac{\partial J}{\partial s}=-\frac{\partial J}{\partial s_{c}}=-1 \
$$</p>
<p>为了简化我们只分析 <span class="arithmatex">\(\frac{\partial s}{\partial W_{ij}^{(1)}}\)</span> 。所以，</p>
<div class="arithmatex">\[
\begin{eqnarray}  \frac{\partial s}{\partial W_{ij}^{(1)}} &amp;=&amp; \frac{\partial W^{(2)}a^{(2)}}{\partial W_{ij}^{(1)}}=\frac{\partial W_{i}^{(2)}a_{i}^{(2)}}{\partial W_{ij}^{(1)}}=W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ \Rightarrow W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial W_{ij}^{(1)}}&amp;=&amp; W_{i}^{(2)}\frac{\partial a_{i}^{(2)}}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &amp;=&amp; W_{i}^{(2)}\frac{f(z_{i}^{(2)})}{\partial z_{i}^{(2)}}\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial z_{i}^{(2)}}{\partial W_{ij}^{(1)}} \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial}{\partial W_{ij}^{(1)}}(b_{i}^{(1)}+a_{1}^{(1)}W_{i1}^{(1)}+a_{2}^{(1)}W_{i2}^{(1)}+a_{3}^{(1)}W_{i3}^{(1)}+a_{4}^{(1)}W_{i4}^{(1)}) \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})\frac{\partial}{\partial W_{ij}^{(1)}}(b_{i}^{(1)}+\sum_{k}a_{k}^{(1)}W_{ik}^{(1)}) \nonumber \\ &amp;=&amp; W_{i}^{(2)}f'(z_{i}^{(2)})a_{j}^{(1)} \nonumber \\ &amp;=&amp; \delta_{i}^{(2)}\cdot a_{j}^{(1)} \nonumber \end{eqnarray}    \\
\]</div>
<p>其中， <span class="arithmatex">\(a^{(1)}\)</span> 指输入层的输入。我们可以看到梯度计算最后可以简化为 <span class="arithmatex">\(\delta_{i}^{(2)}\cdot a_{j}^{(1)}\)</span> ，其中 <span class="arithmatex">\(\delta_{i}^{(2)}\)</span> 本质上是第 <span class="arithmatex">\(2\)</span> 层中第 <span class="arithmatex">\(i\)</span> 个神经元反向传播的误差。 <span class="arithmatex">\(a_{j}^{(1)}\)</span> 与 <span class="arithmatex">\(W_{ij}\)</span> 相乘的结果，输入第 <span class="arithmatex">\(2\)</span> 层中第 <span class="arithmatex">\(i\)</span> 个神经元中。</p>
<p>我们以下图为例，让我们从“误差共享/分配”的来阐释一下反向传播，现在我们要更新 <span class="arithmatex">\(W_{14}^{(1)}\)</span> ：</p>
<p><img alt="1560415341614" src="../imgs/1560415341614.png" /></p>
<ol>
<li>我们从 <span class="arithmatex">\(a_{1}^{(3)}\)</span> 的 1 的误差信号开始反向传播。</li>
<li>然后我们把误差与将 <span class="arithmatex">\(z_{1}^{(3)}\)</span> 映射到 <span class="arithmatex">\(a_{1}^{(3)}\)</span> 的神经元的局部梯度相乘。在这个例子中梯度正好等于  1 ，则误差仍然为 1 。所以有 <span class="arithmatex">\(\delta_{1}^{(3)}=1\)</span> 。</li>
<li>这里误差信号 1 已经到达 <span class="arithmatex">\(z_{1}^{(3)}\)</span> 。我们现在需要分配误差信号使得误差的“公平共享”到达 <span class="arithmatex">\(a_{1}^{(2)}\)</span> 。</li>
<li>现在在 <span class="arithmatex">\(a_{1}^{(2)}\)</span> 的误差为 <span class="arithmatex">\(\delta_{1}^{(3)}\times W_{1}^{(2)}=W_{1}^{(2)}\)</span> （在 <span class="arithmatex">\(z_{1}^{(3)}\)</span> 的误差信号为 <span class="arithmatex">\(\delta_{1}^{(3)}\)</span> ）。因此在 <span class="arithmatex">\(a_{1}^{(2)}\)</span> 的误差为 <span class="arithmatex">\(W_{1}^{(2)}\)</span> 。</li>
<li>与第 2 步的做法相同，我们在将 <span class="arithmatex">\(z_{1}^{(2)}\)</span> 映射到 <span class="arithmatex">\(a_{1}^{(2)}\)</span> 的神经元上移动误差，将 <span class="arithmatex">\(a_{1}^{(2)}\)</span> 与局部梯度相乘，这里的局部梯度为  <span class="arithmatex">\(f'(z_{1}^{(2)})\)</span>  。</li>
<li>
<p>因此在 <span class="arithmatex">\(z_{1}^{(2)}\)</span> 的误差是 <span class="arithmatex">\(f'(z_{1}^{(2)})W_{1}^{(2)}\)</span> ，我们将其定义为 <span class="arithmatex">\(\delta_{1}^{(2)}\)</span> 。</p>
</li>
<li>
<p>最后，我们通过将上面的误差与参与前向计算的 <span class="arithmatex">\(a_{4}^{(1)}\)</span> 相乘，把误差的“误差共享”分配到 <span class="arithmatex">\(W_{14}^{(1)}\)</span> 。</p>
</li>
<li>
<p>所以，对于 <span class="arithmatex">\(W_{14}^{(1)}\)</span> 的梯度损失可以计算为 <span class="arithmatex">\(a_{4}^{(1)}f'(z_{1}^{(2)})W_{1}^{(2)}\)</span> 。</p>
</li>
</ol>
<p>注意我们使用这个方法得到的结果是和之前微分的方法的结果是完全一样的。因此，计算网络中的相应参数的梯度误差既可以使用链式法则也可以使用误差共享和分配的方法——这两个方法能得到相同结果，但是多种方式考虑它们可能是有帮助的。</p>
<p><strong>偏置更新</strong>：偏置项（例如 <span class="arithmatex">\(b_{1}^{(1)}\)</span> ）和其他权值在数学形式是等价的，只是在计算下一层神经 <span class="arithmatex">\(z_{1}^{(2)}\)</span> 元输入时相乘的值是常量 1 。因此在第 k 层的第 i 个神经元的偏置的梯度时 <span class="arithmatex">\(\delta_{i}^{(k)}\)</span> 。例如在上面的例子中，我们更新的是 <span class="arithmatex">\(b_{1}^{(1)}\)</span> 而不是  <span class="arithmatex">\(W_{14}^{(1)}\)</span> ，那么这个梯度为 <span class="arithmatex">\(f'(z_{1}^{(2)})W_{1}^{(2)}\)</span> 。</p>
<p>从 <span class="arithmatex">\(\delta^{(k)}\)</span> 到 <span class="arithmatex">\(\delta^{(k-1)}\)</span> 反向传播的一般步骤：</p>
<ul>
<li>我们有从 <span class="arithmatex">\(z_{i}^{(k)}\)</span> 向后传播的误差 <span class="arithmatex">\(\delta_{i}^{(k)}\)</span> ，如下图所示</li>
</ul>
<p><img alt="1560417982981" src="../imgs/1560417982981.png" /></p>
<ul>
<li>我们通过把 <span class="arithmatex">\(\delta_{i}^{(k)}\)</span> 与路径上的权值 <span class="arithmatex">\(W_{ij}^{(k-1)}\)</span> 相乘，将这个误差反向传播到 <span class="arithmatex">\(a_{j}^{(k-1)}\)</span> 。</li>
<li>因此在 <span class="arithmatex">\(a_{j}^{(k-1)}\)</span> 接收的误差是 <span class="arithmatex">\(\delta_{i}^{(k)}W_{ij}^{(k-1)}\)</span> 。</li>
<li>然而， <span class="arithmatex">\(a_{j}^{(k-1)}\)</span> 在前向计算可能出下图的情况，会参与下一层中的多个神经元的计算。那么第 k 层的第 <span class="arithmatex">\(m\)</span> 个神经元的误差也要使用上一步方法将误差反向传播到 <span class="arithmatex">\(a_{j}^{(k-1)}\)</span> 上。</li>
</ul>
<p><img alt="1560417996607" src="../imgs/1560417996607.png" /></p>
<ul>
<li>因此现在在 <span class="arithmatex">\(a_{j}^{(k-1)}\)</span> 接收的误差是 <span class="arithmatex">\(\delta_{i}^{(k)}W_{ij}^{(k-1)}+\delta_{m}^{(k)}W_{mj}^{(k-1)}\)</span> 。</li>
<li>实际上，我们可以把上面误差和简化为 <span class="arithmatex">\(\sum_{i}\delta_{i}^{(k)}W_{ij}^{(k-1)}\)</span> 。</li>
<li>现在我们有在 <span class="arithmatex">\(a_{j}^{(k-1)}\)</span> 正确的误差，然后将其与局部梯度  <span class="arithmatex">\(f'(z_{j}^{(k-1)})\)</span> 相乘，把误差信息反向传到第 <span class="arithmatex">\(k-1\)</span> 层的第 <span class="arithmatex">\(j\)</span> 个神经元上。</li>
<li>因此到达 <span class="arithmatex">\(z_{j}^{(k-1)}\)</span> 的误差为 <span class="arithmatex">\(f'(z_{j}^{(k-1)})\sum_{i}\delta_{i}^{(k)}W_{ij}^{(k-1)}\)</span> 。</li>
</ul>
<h4 id="16-training-with-backpropagation-vectorized">1.6 Training with Backpropagation – Vectorized<a class="headerlink" href="#16-training-with-backpropagation-vectorized" title="Permanent link">&para;</a></h4>
<p>到目前为止，我们讨论了对模型中的给定参数计算梯度的方法。这里会一般泛化上面的方法，让我们可以直接一次过更新权值矩阵和偏置向量。注意这只是对上面模型的简单地扩展，这将有助于更好理解在矩阵-向量级别上进行误差反向传播的方法。</p>
<p>对更定的参数 <span class="arithmatex">\(W_{ij}^{(k)}\)</span> ，我们知道它的误差梯度是 <span class="arithmatex">\(\delta_{j}^{(k+1)}\cdot a_{j}^{(k)}\)</span> 。其中 <span class="arithmatex">\(W^{(k)}\)</span> 是将 <span class="arithmatex">\(a^{(k)}\)</span> 映射到 <span class="arithmatex">\(z^{(k+1)}\)</span> 的矩阵。因此我们可以确定整个矩阵 <span class="arithmatex">\(W^{(k)}\)</span> 的梯度误差为：</p>
<div class="arithmatex">\[
\nabla_{W^{(k)}} =         \begin{bmatrix}         \delta_{1}^{(k+1)}a_{1}^{(k)} &amp; \delta_{1}^{(k+1)}a_{2}^{(k)} &amp; \cdots \\         \delta_{2}^{(k+1)}a_{1}^{(k)} &amp; \delta_{2}^{(k+1)}a_{2}^{(k)} &amp; \cdots \\         \vdots &amp; \vdots  &amp; \ddots \\         \end{bmatrix} = \delta^{(k+1)}a^{(k)T}  \\
\]</div>
<p>因此我们可以将整个矩阵形式的梯度写为在矩阵中的反向传播的误差向量和前向激活输出的<strong>外积</strong>。</p>
<p>现在我们来看看如何能够计算误差向量 <span class="arithmatex">\(\delta^{(k+1)}\)</span> 。我们从上面的例子中有， <span class="arithmatex">\(\delta_{i}^{(k)}=f'(z_{j}^{(k)})\sum_{i}\delta_{i}^{(k+1)}W_{ij}^{(k)}\)</span> 。这可以简单地改写为矩阵的形式：</p>
<div class="arithmatex">\[
\delta_{i}^{(k)}=f'(z^{(k)})\circ (W^{(k)T}\delta^{(k+1)}) \\
\]</div>
<p>在上面的公式中 <span class="arithmatex">\(\circ\)</span> 运算符是表示向量之间对应元素的相乘（ <span class="arithmatex">\(\mathbb{R}^{N}\times \mathbb{R}^{N}\rightarrow \mathbb{R}^{N}\)</span> ）。</p>
<p><strong>计算效率</strong>：在探索了 <strong>element-wise</strong> 的更新和 <strong>vector-wise</strong> 的更新之后，必须认识到在科学计算环境中，如 MATLAB 或 Python（使用 Numpy / Scipy 库），向量化运算的计算效率是非常高的。因此在实际中应该使用向量化运算。此外，我们也要减少反向传播中的多余的计算——例如，注意到 <span class="arithmatex">\(\delta^{(k)}\)</span> 是直接依赖在 <span class="arithmatex">\(\delta^{(k+1)}\)</span> 上。所以我们要保证使用 <span class="arithmatex">\(\delta^{(k+1)}\)</span> 更新 <span class="arithmatex">\(W^{(k)}\)</span> 时，要保存 <span class="arithmatex">\(\delta^{(k+1)}\)</span> 用于后面 <span class="arithmatex">\(\delta^{(k)}\)</span> 的计算-然后计算 <span class="arithmatex">\((k-1)...(1)\)</span> 层的时候重复上述的步骤。这样的递归过程是使得反向传播成为计算上可负担的过程。</p>
<h3 id="2-neural-networks-tips-and-tricks">2 Neural Networks: Tips and Tricks<a class="headerlink" href="#2-neural-networks-tips-and-tricks" title="Permanent link">&para;</a></h3>
<h4 id="21-gradient-check">2.1 Gradient Check<a class="headerlink" href="#21-gradient-check" title="Permanent link">&para;</a></h4>
<p>在上一部分中，我们详细地讨论了如何用基于微积分的方法计算神经网络中的参数的误差梯度／更新。这里我们介绍一种用数值近似这些梯度的方法——虽然在计算上的低效不能直接用于训练神经网络，这种方法可以非常准确地估计任何参数的导数；因此，它可以作为对导数的正确性的有用的检查。给定一个模型的参数向量 <span class="arithmatex">\(\theta\)</span> 和损失函数 <span class="arithmatex">\(J\)</span> ，围绕 <span class="arithmatex">\(\theta_{i}\)</span> 的数值梯度由 <strong>central difference formula</strong> 得出：</p>
<div class="arithmatex">\[
f'(\theta)\approx \frac{J(\theta^{(i+)})-\theta^{(i-)})}{2\epsilon}  \\
\]</div>
<p>其中 <span class="arithmatex">\(\epsilon\)</span> 是一个很小的值（一般约为 <span class="arithmatex">\(1e^{-5}\)</span> ）。当我们使用  <span class="arithmatex">\(+\epsilon\)</span> 扰动参数 <span class="arithmatex">\(\theta\)</span> 的第 <span class="arithmatex">\(i\)</span> 个元素时，就可以在前向传播上计算误差 <span class="arithmatex">\(J(\theta^{(i+)})\)</span> 。相似地，当我们使用  <span class="arithmatex">\(-\epsilon\)</span> 扰动参数 <span class="arithmatex">\(\theta\)</span> 的第 <span class="arithmatex">\(i\)</span> 个元素时，就可以在前向传播上计算误差 <span class="arithmatex">\(J(\theta^{(i-)})\)</span> 。因此，计算两次前向传播，我们可以估计在模型中任意给定参数的梯度。我们注意到数值梯度的定义和导数的定义很相似，其中，在标量的情况下：</p>
<div class="arithmatex">\[
f'(\theta)\approx \frac{f(x+\epsilon)-f(x)}{\epsilon}   \\
\]</div>
<p>当然，还是有一点不同——上面的定义仅仅在正向扰动 <span class="arithmatex">\(x\)</span> 计算梯度。虽然是可以用这种方式定义数值梯度，但在实际中使用 <strong>central difference formula</strong> 常常可以更准确和更稳定，因为我们在两个方向都对参数扰动。为了更好地逼近一个点附近的导数/斜率，我们需要在该点的左边和右边检查函数 <span class="arithmatex">\(f'\)</span> 的行为。也可以使用泰勒定理来表示 <strong>central difference formula</strong> 有 <span class="arithmatex">\(\epsilon^{2}\)</span> 比例误差，这相当小，而导数定义更容易出错。</p>
<p>现在你可能会产生疑问，如果这个方法这么准确，为什么我们不用它而不是用反向传播来计算神经网络的梯度？这是因为效率的问题——每当我们想计算一个元素的梯度，需要在网络中做两次前向传播，这样是很耗费计算资源的。再者，很多大规模的神经网络含有几百万的参数，对每个参数都计算两次明显不是一个好的选择。同时在例如 <strong>SGD</strong> 这样的优化技术中，我们需要通过数千次的迭代来计算梯度，使用这样的方法很快会变得难以应付。这种低效性是我们只使用梯度检验来验证我们的分析梯度的正确性的原因。梯度检验的实现如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">eval_numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    a naive implementation of numerical gradient of f at x</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="sd">    - f should be a function that takes a single argument</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    - x is the point (numpy array) to evaluate the gradient  </span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    at</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate function value at original point</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="n">h</span> <span class="o">=</span> <span class="mf">0.00001</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="c1"># iterate over all indexes in x</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;multi_index&#39;</span><span class="p">,</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>                     <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>        <span class="c1"># evaluate function at x+h</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>        <span class="n">old_value</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">+</span> <span class="n">h</span> <span class="c1"># increment by h</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>        <span class="n">fxh_left</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate f(x + h)</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> <span class="o">-</span> <span class="n">h</span> <span class="c1"># decrement by h</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>        <span class="n">fxh_right</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># evaluate f(x - h)</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>        <span class="c1"># restore to previous value (very important!)</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>        <span class="n">x</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_value</span> 
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>        <span class="c1"># compute the partial derivative</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>        <span class="c1"># the slope</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        <span class="n">grad</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh_left</span> <span class="o">-</span> <span class="n">fxh_right</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>        <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span> <span class="c1"># step to next dimension</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>    <span class="k">return</span> <span class="n">grad</span>
</code></pre></div>
<h4 id="22-regularization">2.2 Regularization<a class="headerlink" href="#22-regularization" title="Permanent link">&para;</a></h4>
<p>和很多机器学习的模型一样，神经网络很容易过拟合，这令到模型在训练集上能获得近乎完美的表现，但是却不能泛化到测试集上。一个常见的用于解决过拟合（“高方差问题”）的方法是使用 <span class="arithmatex">\(L2\)</span> 正则化。我们只需要在损失函数 <span class="arithmatex">\(J\)</span> 上增加一个正则项，现在的损失函数如下：</p>
<div class="arithmatex">\[
J_{R}=J+\lambda\sum_{i=1}^{L}\big|\big|W^{(i)}\big|\big|_{F}   \\
\]</div>
<p>在上面的公式中， <span class="arithmatex">\(\big|\big|W^{(i)}\big|\big|_{F}\)</span> 是矩阵 <span class="arithmatex">\(W^{(i)}\)</span> （在神经网络中的第 <span class="arithmatex">\(i\)</span> 个权值矩阵）的 <span class="arithmatex">\(Frobenius\)</span> 范数, <span class="arithmatex">\(\lambda\)</span> 是超参数控制损失函数中的权值的大小。</p>
<blockquote>
<p>矩阵 <span class="arithmatex">\(U\)</span> 的 <span class="arithmatex">\(Frobenius\)</span> 范数的定义 <span class="arithmatex">\(\|U\|_{F}=\sqrt{\sum_{i} \sum_{l} U_{i l}^{2}}\)</span></p>
</blockquote>
<p>当我们尝试去最小化 <span class="arithmatex">\(J_{R}\)</span> ，正则化本质上就是当优化损失函数的时候，惩罚数值太大的权值（让权值的数值分配更加均衡，防止出现部分权值特别大的情况）。由于 <span class="arithmatex">\(Frobenius\)</span> 范数的二次的性质（计算矩阵的元素的平方和）， <span class="arithmatex">\(L2\)</span> 正则项有效地降低了模型的灵活性和因此减少出现过拟合的可能性。增加这样一个约束可以<strong>使用贝叶斯派的思想解释</strong>，这个正则项是对模型的参数加上一个<strong>先验分布</strong>，优化权值使其接近于 0——有多接近是取决于 <span class="arithmatex">\(\lambda\)</span> 的值。选择一个合适的 <span class="arithmatex">\(\lambda\)</span> 值是很重要的，并且需要通过超参数调整来选择。 <span class="arithmatex">\(\lambda\)</span> 的值太大会令很多权值都接近于  0 ，则模型就不能在训练集上学习到有意义的东西，经常在训练、验证和测试集上的表现都非常差。 <span class="arithmatex">\(\lambda\)</span> 的值太小，会让模型仍旧出现过拟合的现象。需要注意的是，偏置项不会被正则化，不会计算入损失项中——尝试去思考一下为什么</p>
<p>为何在损失项中不计算偏置项？</p>
<blockquote>
<p>偏置项在模型中仅仅是偏移的关系，使用少量的数据就能拟合到这项，而且从经验上来说，偏置值的大小对模型表现没有很显著的影响，因此不需要正则化偏置项
<a href="https://www.zhihu.com/question/66894061">深度学习里面的偏置为什么不加正则？</a>
下面摘录已有的三条回答
首先正则化主要是为了防止过拟合，而过拟合一般表现为模型对于输入的微小改变产生了输出的较大差异，这主要是由于有些参数w过大的关系，通过对||w||进行惩罚，可以缓解这种问题。而如果对||b||进行惩罚，其实是没有作用的，因为在对输出结果的贡献中，参数b对于输入的改变是不敏感的，不管输入改变是大还是小，参数b的贡献就只是加个偏置而已。
举个例子，如果你在训练集中，w和b都表现得很好，但是在测试集上发生了过拟合，b是不背这个锅的，因为它对于所有的数据都是一视同仁的（都只是给它们加个偏置），要背锅的是w，因为它会对不同的数据产生不一样的加权。或者说，模型对于输入的微小改变产生了输出的较大差异，这是因为模型的“曲率”太大，而模型的曲率是由w决定的，b不贡献曲率（对输入进行求导，b是直接约掉的）。</p>
</blockquote>
<p>从贝叶斯的角度来讲，正则化项通常都包含一定的先验信息，神经网络倾向于较小的权重以便更好地泛化，但是对偏置就没有这样一致的先验知识。另外，很多神经网络更倾向于区分方向信息（对应于权重），而不是位置信息（对应于偏置），所以对偏置加正则化项对控制过拟合的作用是有限的，相反很可能会因为不恰当的正则强度影响神经网络找到最优点。</p>
<p>过拟合会使得模型对异常点很敏感，即准确插入异常点，导致拟合函数中的曲率很大（即函数曲线的切线斜率非常高），而偏置对模型的曲率没有贡献（对多项式模型进行求导，为W的线性加和），所以正则化他们也没有什么意义。</p>
<p>有时候我们会用到其他类型的正则项，例如 <span class="arithmatex">\(L1\)</span> 正则项，它将参数元素的绝对值全部加起来-然而，在实际中很少会用 <span class="arithmatex">\(L1\)</span> 正则项，因为会令权值参数变得稀疏。在下一部分，我们讨论 dropout ，这是另外一种有效的正则化方法，通过在前向传播过程随机将神经元设为 0</p>
<blockquote>
<p>Dropout 实际上是通过在每次迭代中忽略它们的权值来实现“冻结”部分 unit 。这些“冻结”的 unit 不是把它们设为 0 ，而是对于该迭代，网络假定它们为 0 。“冻结”的 unit 不会为此次迭代更新</p>
</blockquote>
<h4 id="23-dropout">2.3 Dropout<a class="headerlink" href="#23-dropout" title="Permanent link">&para;</a></h4>
<p><strong>Dropout</strong> 是一个非常强大的正则化技术，是 Srivastava 在论文 《Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting》中首次提出，下图展示了 dropout 如何应用在神经网络上。</p>
<p><img alt="1560432828326" src="../imgs/1560432828326.png" /></p>
<p>这个想法是简单而有效的——训练过程中，在每次的前向／反向传播中我们按照一定概率 <span class="arithmatex">\((1-p)\)</span> 随机地“ drop ”一些神经元子集（或者等价的，我们保持一定概率 <span class="arithmatex">\(p\)</span> 的神经元是激活的）。然后，在测试阶段，我们将使用全部的神经元来进行预测。使用 Dropout 神经网络一般能从数据中学到更多有意义的信息，更少出现过拟合和通常在现今的任务上获得更高的整体表现。这种技术应该如此有效的一个直观原因是， dropout 本质上作的是一次以指数形式训练许多较小的网络，并对其预测进行平均。</p>
<p>实际上，我们使用 dropout 的方式是我们取每个神经元层的输出 <span class="arithmatex">\(h\)</span> ，并保持概率  <span class="arithmatex">\(p\)</span> 的神经元是激活的，否则将神经元设置为 0 。然后，在反向传播中我们仅对在前向传播中激活的神经元回传梯度。最后，在测试过程，我们使用神经网络中全部的神经元进行前向传播计算。然而，有一个关键的微妙之处，为了使 dropout 有效地工作，测试阶段的神经元的预期输出应与训练阶段大致相同——否则输出的大小可能会有很大的不同，网络的表现已经不再明确了。因此，我们通常必须在测试阶段将每个神经元的输出除以某个值——这留给读者作为练习来确定这个值应该是多少，以便在训练和测试期间的预期输出相等（该值为 <span class="arithmatex">\(p\)</span> ） 。</p>
<p>以下源于 <a href="https://nndl.github.io/">《神经网络与深度学习》</a> <code>P190</code></p>
<ul>
<li>
<p>目的：缓解过拟合问题，一定程度上达到正则化的效果</p>
</li>
<li>
<p>效果：减少下层节点对其的依赖，迫使网络去学习更加<strong>鲁棒</strong>的特征</p>
</li>
<li>
<p><strong>集成学习的解释</strong> 👍</p>
<ul>
<li>每做一次丢弃，相当于从原始的网络中采样得到一个子网络。
    如果一个神经网络有<span class="arithmatex">\(n\)</span>个神经元，那么总共可以采样出<span class="arithmatex">\(2^n\)</span>个子网络。每次迭代都相当于训练一个不同的子网络，这些子网络都共享原始网络的参数。那么，最终的网络可以近似看作是集成了指数级个不同网络的组合模型。</li>
</ul>
</li>
<li>
<p><strong>贝叶斯学习的解释</strong>  👍 </p>
<ul>
<li>丢弃法也可以解释为一种贝叶斯学习的近似。用<span class="arithmatex">\(y=f(\mathbf{x}, \theta)\)</span>来表示要学习的神经网络，贝叶斯学习是假设参数<span class="arithmatex">\(\theta\)</span>为随机向量，并且先验分布为<span class="arithmatex">\(q(\theta)\)</span>，贝叶斯方法的预测为</li>
</ul>
<div class="arithmatex">\[
\begin{aligned} \mathbb{E}_{q(\theta)}[y] &amp;=\int_{q} f(\mathbf{x}, \theta) q(\theta) d \theta \\ &amp; \approx \frac{1}{M} \sum_{m=1}^{M} f\left(\mathbf{x}, \theta_{m}\right) \end{aligned}
\]</div>
<ul>
<li>其中<span class="arithmatex">\(f(\mathbf{x}, \theta_m)\)</span>为第m次应用丢弃方法后的网络，其参数<span class="arithmatex">\(\theta_m\)</span>为对全部参数<span class="arithmatex">\(\theta\)</span>的一次采样。</li>
</ul>
</li>
<li>
<p>RNN中的变分Dropout <a href="https://arxiv.org/abs/1512.05287">Variational Dropout</a></p>
<ul>
<li>
<p>Dropout一般是针对神经元进行随机丢弃，但是也可以扩展到对神经元之间
    的连接进行随机丢弃，或每一层进行随机丢弃。</p>
</li>
<li>
<p>在RNN中，不能直接对每个时刻的隐状态进行随机
    丢弃，这样会损害循环网络在时间维度上记忆能力。</p>
</li>
<li>
<p>一种简单的方法是对非时间维度的连接（即非循环连接）进行随机丢失。如图所示，虚线边表示进行随机丢弃，不同的颜色表示不同的丢弃掩码。</p>
<p><img alt="1558861348710" src="../imgs/1558861348710.png" /></p>
</li>
<li>
<p>然而根据贝叶斯学习的解释，丢弃法是一种对参数<span class="arithmatex">\(θ\)</span>的采样。每次采样的参数需要在每个时刻保持不变。因此，在对循环神经网络上使用丢弃法时，需要对参数矩阵的每个元素进行随机丢弃，并在所有时刻都使用相同的丢弃掩码。这种方法称为变分丢弃法（Variational Dropout）。
    图7.12给出了变分丢弃法的示例，<strong>相同颜色表示使用相同的丢弃掩码</strong>。</p>
</li>
</ul>
</li>
</ul>
<p><img alt="1558861390725" src="../imgs/1558861390725.png" /></p>
<h4 id="24-neuron-units">2.4 Neuron Units<a class="headerlink" href="#24-neuron-units" title="Permanent link">&para;</a></h4>
<p>到目前为止，我们讨论了含有 sigmoidal neurons 的非线性分类的神经网络。但是在很多应用中，使用其他激活函数可以设计更好的神经网络。下面列出一些常见的激活函数和激活函数的梯度定义，它们可以和前面讨论过的 sigmoidal 函数互相替换。</p>
<p><strong>Sigmoid</strong>：这是我们讨论过的常用选择，激活函数 <span class="arithmatex">\(\sigma\)</span> 为：
$$
\sigma(z)=\frac{1}{1+exp(-z)}
$$</p>
<p>其中  <span class="arithmatex">\(\sigma(z)\in (0,1)\)</span> 。</p>
<p><img alt="1560433211653" src="../imgs/1560433211653.png" /></p>
<p><span class="arithmatex">\(\sigma(z)\)</span> 的梯度为</p>
<div class="arithmatex">\[
\sigma'(z)=\frac{-exp(-z)}{1+exp(-z)}=\sigma(z)(1-\sigma(z))  \\
\]</div>
<p><strong>Tanh</strong>： <span class="arithmatex">\(tanh\)</span> 函数是 <span class="arithmatex">\(sigmoid\)</span> 函数之外的另一个选择，在实际中它能更快地收敛。 <span class="arithmatex">\(tanh\)</span> 和 <span class="arithmatex">\(sigmoid\)</span> 的主要不同在于 <span class="arithmatex">\(tanh\)</span> 的输出范围在  -1 到 1 ，而 <span class="arithmatex">\(sigmoid\)</span> 的输出范围在 0 到 1 。
$$
tanh(z)=\frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}=2\sigma(2z)-1   \
$$</p>
<p>其中 <span class="arithmatex">\(tanh(z)\in (-1, 1)\)</span> 。</p>
<p><img alt="1560433307386" src="../imgs/1560433307386.png" /></p>
<p><span class="arithmatex">\(tanh(z)\)</span> 的梯度为：</p>
<div class="arithmatex">\[
tanh'(z)=\bigg(\frac{exp(z)-exp(-z)}{exp(z)+exp(-z)}\bigg)^{2}=1-tanh^{2}(z)   \\
\]</div>
<p><strong>Hard tanh</strong>：有时候 <span class="arithmatex">\(hard\;tanh\)</span> 函数有时比 <span class="arithmatex">\(tanh\)</span> 函数的选择更为优先，因为它的计算量更小。然而当 <span class="arithmatex">\(z\)</span> 的值大于 <span class="arithmatex">\(1\)</span> 时，函数的数值会饱和（如下图所示会恒等于 1）。<span class="arithmatex">\(hard\;tanh\)</span> 激活函数为：</p>
<div class="arithmatex">\[
\begin{eqnarray}  hardtanh(z) &amp;=&amp; \begin{cases} -1&amp; \text{:$z&lt;1$}\\ z &amp; \text{:$-1\le z \le 1$} \\ 1 &amp; \text{:$z&gt;1$} \end{cases} \nonumber  \end{eqnarray}  \\
\]</div>
<p><img alt="1560433436646" src="../imgs/1560433436646.png" /></p>
<p><span class="arithmatex">\(hard\;tanh\)</span> 这个函数的微分也可以用分段函数的形式表示：</p>
<div class="arithmatex">\[
\begin{eqnarray}  hardtanh(z) &amp;=&amp; \begin{cases} 1 &amp; \text{:$-1\le z \le 1$} \\ 0 &amp; \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}   \\
\]</div>
<p><strong>Soft sign</strong>： <span class="arithmatex">\(soft\;sign\)</span> 函数是另外一种非线性激活函数，它可以是 <span class="arithmatex">\(tanh\)</span> 的另外一种选择，因为它和 <span class="arithmatex">\(hard\;clipped\;functions\)</span> 一样不会过早地饱和：
$$
softsign(z)=\frac{z}{1+|z|}<br />
$$</p>
<p><img alt="1561044890099" src="../imgs/1561044890099.png" /></p>
<p><span class="arithmatex">\(soft\;sign\)</span> 函数的微分表达式为：</p>
<div class="arithmatex">\[
softsign'(z)=\frac{sgn(z)}{(1+z)^{2}}   \\
\]</div>
<p>其中 <span class="arithmatex">\(sgn\)</span> 是符号函数，根据 <span class="arithmatex">\(z\)</span> 的符号返回 1 或者 -1 。</p>
<p><strong>ReLU</strong>： <span class="arithmatex">\(ReLU\)</span> （ <span class="arithmatex">\(Rectiﬁed\;Linear\;Unit\)</span> ）函数是激活函数中的一个常见的选择，当 z 的值特别大的时候它也不会饱和。在计算机视觉应用中取得了很大的成功：
$$
rect(z)=max(z,0)
$$</p>
<p><img alt="1560433750964" src="../imgs/1560433750964.png" /></p>
<p><span class="arithmatex">\(ReLU\)</span> 函数的微分是一个分段函数：</p>
<div class="arithmatex">\[
\begin{eqnarray}  rect'(z) &amp;=&amp; \begin{cases} 1 &amp; \text{$z &gt; 0$} \\ 0 &amp; \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}    \\
\]</div>
<p><strong>Leaky ReLU</strong>：传统的 <span class="arithmatex">\(ReLU\)</span> 单元当 z 的值小于 0 时，是不会反向传播误差 <span class="arithmatex">\(leaky\;ReLU\)</span> 改善了这一点，当 <span class="arithmatex">\(z\)</span> 的值小于 0 时，仍然会有一个很小的误差反向传播回去。
$$
leaky(z)=max(z, k\cdot z)  \
$$</p>
<p>其中 <span class="arithmatex">\(0&lt;k&lt;1\)</span> 。</p>
<p><span class="arithmatex">\(leaky\;ReLU\)</span> 函数的微分是一个分段函数：</p>
<div class="arithmatex">\[
\begin{eqnarray}  leaky'(z) &amp;=&amp; \begin{cases} 1 &amp; \text{$z &gt; 0$} \\ k &amp; \text{:$otherwise$} \end{cases} \nonumber  \end{eqnarray}
\]</div>
<p><img alt="1560433960028" src="../imgs/1560433960028.png" /></p>
<h4 id="25-data-preprocessing">2.5 Data Preprocessing<a class="headerlink" href="#25-data-preprocessing" title="Permanent link">&para;</a></h4>
<p>与机器学习模型的一般情况一样，确保模型在当前任务上获得合理性能的一个关键步骤是对数据执行基本的预处理。下面概述了一些常见的技术。</p>
<p><strong>Mean Subtraction</strong></p>
<p>给定一组输入数据 <span class="arithmatex">\(X\)</span> ，一般把 <span class="arithmatex">\(X\)</span> 中的值减去 <span class="arithmatex">\(X\)</span> 的平均特征向量来使数据<strong>零中心化</strong>。在实践中很重要的一点是，<strong>只计算训练集的平均值</strong>，而且在<strong>训练集，验证集和测试集</strong>都是<strong>减去同一平均值</strong>。</p>
<p><strong>Normalization</strong></p>
<p>另外一个常见的技术（虽然没有 <span class="arithmatex">\(mean\;Subtraction\)</span> 常用）是将每个输入特征维度缩小，让每个输入特征维度具有相似的幅度范围。这是很有用的，因此不同的输入特征是用不同“单位”度量，但是最初的时候我们经常认为所有的特征同样重要。实现方法是将特征除以它们各自在训练集中计算的标准差。</p>
<p><strong>Whitening</strong></p>
<p>相比上述的两个方法， <strong>whitening</strong> 没有那么常用，它本质上是数据经过转换后，特征之间相关性较低，所有特征具有相同的方差（协方差阵为 1 ）。首先对数据进行 <strong>Mean Subtraction</strong> 处理，得到 <span class="arithmatex">\(X'\)</span> 。然后我们对 <span class="arithmatex">\(X'\)</span> 进行奇异值分解得到矩阵 <span class="arithmatex">\(U\)</span> , <span class="arithmatex">\(S\)</span> , <span class="arithmatex">\(V\)</span> ，计算 <span class="arithmatex">\(UX'\)</span> 将 <span class="arithmatex">\(X'\)</span> 投影到由 <span class="arithmatex">\(U\)</span> 的列定义的基上。我们最后将结果的每个维度除以 <span class="arithmatex">\(S\)</span> 中的相应奇异值，从而适当地缩放我们的数据（如果其中有奇异值为 0 ，我们就除以一个很小的值代替）。</p>
<h4 id="26-parameter-initialization">2.6 Parameter Initialization<a class="headerlink" href="#26-parameter-initialization" title="Permanent link">&para;</a></h4>
<p>让神经网络实现最佳性能的关键一步是以合理的方式初始化参数。一个好的起始方法是将权值初始化为通常分布在 0 附近的很小的随机数-在实践中效果还不错。在论文<strong><em>Understanding the difficulty</em></strong>
<strong><em>of training deep feedforward neural networks</em></strong>
<strong><em>(2010)</em></strong>, Xavier 研究不同权值和偏置初始化方案对训练动力（ <strong>training dynamics</strong>​ ）的影响。实验结果表明，对于 sigmoid 和 tanh 激活单元，当一个权值矩阵 <span class="arithmatex">\(W\in \mathbb{R}^{n^{(l+1)}\times n^{(l)}}\)</span> 以如下的均匀分布的方式随机初始化，能够实现更快的收敛和得到更低的误差：</p>
<div class="arithmatex">\[
W\sim U\bigg[-\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}},\sqrt{\frac{6}{n^{(l)}+n^{(l+1)}}}\;\bigg]   \\
\]</div>
<p>其中 <span class="arithmatex">\(n^{(l)}\)</span> 是 W <span class="arithmatex">\((fan\text{-}in)\)</span> 的输入单元数， <span class="arithmatex">\(n^{(l+1)}\)</span> 是 W <span class="arithmatex">\((fan\text{-}out)\)</span> 的输出单元数。在这个参数初始化方案中，偏置单元是初始化为 0 。这种方法是尝试保持跨层之间的激活方差以及反向传播梯度方差。如果没有这样的初始化，梯度方差（当中含有纠正信息）通常随着跨层的反向传播而衰减。</p>
<h4 id="27-learning-strategies">2.7 Learning Strategies<a class="headerlink" href="#27-learning-strategies" title="Permanent link">&para;</a></h4>
<p>训练期间模型参数更新的速率/幅度可以使用学习率进行控制。在最简单的梯度下降公式中， <span class="arithmatex">\(\alpha\)</span> 是学习率：</p>
<div class="arithmatex">\[
\theta^{new}=\theta^{old}-\alpha\nabla_{\theta}J_{t}(\theta)   \\
\]</div>
<p>你可能会认为如果要更快地收敛，我们应该对 <span class="arithmatex">\(\alpha\)</span> 取一个较大的值——然而，在更快的收敛速度下并不能保证更快的收敛。实际上，如果学习率非常高，我们可能会遇到损失函数难以收敛的情况，因为参数更新幅度过大，会导致模型越过凸优化的极小值点，如下图所示。在非凸模型中（我们很多时候遇到的模型都是非凸），高学习率的结果是难以预测的，但是损失函数难以收敛的可能性是非常高的。</p>
<p><img alt="1560434193931" src="../imgs/1560434193931.png" /></p>
<p>避免损失函数难以收敛的一个简答的解决方法是使用一个很小的学习率，让模型谨慎地在参数空间中迭代——当然，如果我们使用了一个太小的学习率，损失函数可能不会在合理的时间内收敛，或者会困在局部最优点。因此，与任何其他超参数一样，学习率必须有效地调整。</p>
<p>深度学习系统中最消耗计算资源的是训练阶段，一些研究已在尝试提升设置学习率的新方法。例如， <strong>Ronan Collobert</strong> 通过取 <span class="arithmatex">\(fan\text{-}in\)</span> 的神经元 <span class="arithmatex">\((n^{(l)})\)</span> 的平方根的倒数来缩放权值 <span class="arithmatex">\(W_{ij}\)</span> ( <span class="arithmatex">\(W\in \mathbb{R}^{n^{(l+1)}\times n^{(l)}}\)</span>) 的学习率。</p>
<p>还有其他已经被证明有效的技术-这个方法叫 <strong>annealing</strong> <strong>退火</strong>，在多次迭代之后，学习率以以下方式降低：保证以一个高的的学习率开始训练和快速逼近最小值；当越来越接近最小值时，开始降低学习率，让我们可以在更细微的范围内找到最优值。一个常见的实现 <strong>annealing</strong> 的方法是在每 <span class="arithmatex">\(n\)</span> 次的迭代学习后，通过一个因子 <span class="arithmatex">\(x\)</span> 来降低学习率 <span class="arithmatex">\(\alpha\)</span> 。</p>
<p>指数衰减也是很常见的方法，在 <span class="arithmatex">\(t\)</span> 次迭代后学习率变为 <span class="arithmatex">\(\alpha(t)=\alpha_{0}e^{-kt}\)</span> ，其中 <span class="arithmatex">\(\alpha_{0}\)</span> 是初始的学习率和 <span class="arithmatex">\(k\)</span> 是超参数。</p>
<p>还有另外一种方法是允许学习率随着时间减少：</p>
<div class="arithmatex">\[
\alpha(t)=\frac{\alpha_{0}\tau}{max(t,\tau)}
\]</div>
<p>在上述的方案中， <span class="arithmatex">\(\alpha_{0}\)</span> 是一个可调的参数，代表起始的学习率。 <span class="arithmatex">\(\tau\)</span> 也是一个可调参数，表示学习率应该在该时间点开始减少。在实际中，这个方法是很有效的。在下一部分我们讨论另外一种不需要手动设定学习率的自适应梯度下降的方法。</p>
<h4 id="28-momentum-updates">2.8 Momentum Updates<a class="headerlink" href="#28-momentum-updates" title="Permanent link">&para;</a></h4>
<p>动量方法，灵感来自于物理学中的对动力学的研究，是梯度下降方法的一种变体，尝试使用更新的“速度”的一种更有效的更新方案。动量更新的伪代码如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="c1"># Computes a standard momentum update</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="c1"># on parameters x</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">v</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">*</span> <span class="n">v</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad_x</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">x</span> <span class="o">+=</span> <span class="n">v</span>
</code></pre></div>
<h4 id="29-adaptive-optimization-methods">2.9 Adaptive Optimization Methods<a class="headerlink" href="#29-adaptive-optimization-methods" title="Permanent link">&para;</a></h4>
<p><strong>AdaGrad</strong> 是标准的随机梯度下降( <strong>SGD</strong> )的一种实现，但是有一点关键的不同：对每个参数学习率是不同的。每个参数的学习率取决于每个参数梯度更新的历史,参数的历史更新越小，就使用更大的学习率加快更新。换句话说，过去没有更新太大的参数现在更有可能有更高的学习率。</p>
<div class="arithmatex">\[
\theta_{t,i}=\theta_{t-1,i}-\frac{\alpha}{\sqrt{\sum_{\tau=1}^{t}g_{\tau,i}^{2}}} g_{t,i} \ where \ g_{t,i}=\frac{\partial}{\partial\theta_{i}^{t}}J_{t}(\theta)
\]</div>
<p>在这个技术中，我们看到如果梯度的历史 <strong>RMS</strong> 很低，那么学习率会非常高。这个技术的一个简单的实现如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="c1"># Assume the gradient dx and parameter vector x</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="n">cache</span> <span class="o">+=</span> <span class="n">dx</span> <span class="o">**</span> <span class="mi">2</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span>
</code></pre></div>
<p>其他常见的自适应方法有 <strong>RMSProp</strong> 和 <strong>Adam</strong> ，其更新规则如下所示：</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="c1"># Update rule for RMS prop</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="n">cache</span> <span class="o">=</span> <span class="n">decay_rate</span> <span class="o">*</span> <span class="n">cache</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay_rate</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">**</span> <span class="mi">2</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dx</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a><span class="c1"># Update rule for Adam</span>
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a><span class="n">m</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span> <span class="o">*</span> <span class="n">dx</span>
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="n">v</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">dx</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="n">x</span> <span class="o">+=</span> <span class="o">-</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">m</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
</code></pre></div>
<ul>
<li>
<p><strong>RMSProp</strong> 是利用平方梯度的移动平局值，是 <strong>AdaGrad</strong> 的一个变体——实际上，和 <strong>AdaGrad</strong> 不一样，<strong>它的更新不会单调变小</strong>。</p>
</li>
<li>
<p><strong>Adam</strong> 更新规则又是 <strong>RMSProp</strong> 的一个变体，但是加上了动量更新。</p>
</li>
</ul>
<h4 id="210-more-reference">2.10 More reference<a class="headerlink" href="#210-more-reference" title="Permanent link">&para;</a></h4>
<p>如果希望了解以上的梯度优化算法的具体细节，可以阅读这篇文章： <a href="http://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a>) 。</p>
<h2 id="computing-neural-network-gradients">Computing Neural Network Gradients<a class="headerlink" href="#computing-neural-network-gradients" title="Permanent link">&para;</a></h2>
<h3 id="1-introduction">1 Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h3>
<p>这些笔记的目的是演示如何以完全矢量化的方式快速计算神经网络梯度。这是对CS224n 2019第三讲的最后一部分的补充，是同样的材料</p>
<h3 id="2-vectorized-gradients">2 Vectorized Gradients<a class="headerlink" href="#2-vectorized-gradients" title="Permanent link">&para;</a></h3>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<p>以下是学习本课程时的可用参考书籍：</p>
<p><a href="https://item.jd.com/12355569.html">《基于深度学习的自然语言处理》</a> （车万翔老师等翻译）</p>
<p><a href="https://nndl.github.io/">《神经网络与深度学习》</a></p>
<p>以下是整理笔记的过程中参考的博客：</p>
<p><a href="https://zhuanlan.zhihu.com/p/59011576">斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p>
<p><a href="https://zhuanlan.zhihu.com/p/31977759">斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> <span class="critic comment">这是针对note部分的翻译</span></p>


  




                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            回到页面顶部
          </a>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2019 - 2022; Xiao Xu
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/looperXX" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/looperxx_nlp" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["content.code.annotate", "content.tooltips", "navigation.indexes", "navigation.tracking", "navigation.sections", "navigation.tabs", "navigation.top", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.db81ec45.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../assets/javascripts/bundle.6df46069.min.js"></script>
      
        <script src="../javascripts/baidu-tongji.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>