
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LooperXX's homepage">
      
      
      
        <meta name="author" content="Looper - Xiao Xu">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.0.2">
    
    
      
        <title>06 The probability of a sentence Recurrent Neural Networks and Language Models - Science is interesting.</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.38780c08.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.3f72e892.min.css">
        
      
    
    
    
      
        
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-164217558-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview",document.location.pathname)})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="preference" data-md-color-primary="" data-md-color-accent="">
      
        <script>matchMedia("(prefers-color-scheme: dark)").matches&&document.body.setAttribute("data-md-color-scheme","slate")</script>
      
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="Science is interesting." class="md-header-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Science is interesting.
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              06 The probability of a sentence Recurrent Neural Networks and Language Models
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          

  

<nav class="md-tabs md-tabs--active" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." class="md-tabs__link">
        Homepage
      </a>
    
  </li>

      
        
  
  
    
    
  
  
    <li class="md-tabs__item">
      
        <a href="../Attention/" class="md-tabs__link">
          Notes
        </a>
      
    </li>
  

  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-tabs__link md-tabs__link--active">
          Notes on CS224n
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../MkDocs_demo/" class="md-tabs__link">
          For MkDocs
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Science is interesting." class="md-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Science is interesting.
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." class="md-nav__link">
      Homepage
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Notes
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Notes" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon"></span>
        Notes
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-1" type="checkbox" id="nav-2-1">
    
    <label class="md-nav__link" for="nav-2-1">
      Theory
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Theory" data-md-level="2">
      <label class="md-nav__title" for="nav-2-1">
        <span class="md-nav__icon md-icon"></span>
        Theory
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Attention/" class="md-nav__link">
      Attention
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Normalization/" class="md-nav__link">
      Normalization
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2">
    
    <label class="md-nav__link" for="nav-2-2">
      Code
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Code" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        <span class="md-nav__icon md-icon"></span>
        Code
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Notes%20on%20NCRF%2B%2B/" class="md-nav__link">
      NCRF++
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-3" type="checkbox" id="nav-2-3">
    
    <label class="md-nav__link" for="nav-2-3">
      Book
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Book" data-md-level="2">
      <label class="md-nav__title" for="nav-2-3">
        <span class="md-nav__icon md-icon"></span>
        Book
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" class="md-nav__link">
      Machine Reading Comprehension
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%20Concepts/" class="md-nav__link">
      Some Concepts
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NNDL%20exercise/" class="md-nav__link">
      NNDL exercise
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3" checked>
    
    <label class="md-nav__link" for="nav-3">
      Notes on CS224n
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Notes on CS224n" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon"></span>
        Notes on CS224n
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-nav__link">
      CS224n-2019 Introduction
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-Assignment/" class="md-nav__link">
      CS224n-2019 Assignment
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-01-Introduction%20and%20Word%20Vectors/" class="md-nav__link">
      01 Introduction and Word Vectors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" class="md-nav__link">
      02 Word Vectors 2 and Word Senses
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/" class="md-nav__link">
      03 Word Window Classification,Neural Networks, and Matrix Calculus
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" class="md-nav__link">
      04 Backpropagation and Computation Graphs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" class="md-nav__link">
      05 Linguistic Structure Dependency Parsing
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        06 The probability of a sentence Recurrent Neural Networks and Language Models
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" class="md-nav__link md-nav__link--active">
      06 The probability of a sentence Recurrent Neural Networks and Language Models
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models" class="md-nav__link">
    Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-05-language-models-rnn-gru-and-lstm" class="md-nav__link">
    Notes 05 Language Models, RNN, GRU and LSTM
  </a>
  
    <nav class="md-nav" aria-label="Notes 05 Language Models, RNN, GRU and LSTM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-language-models" class="md-nav__link">
    1 Language Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-recurrent-neural-networks-rnn" class="md-nav__link">
    2 Recurrent Neural Networks (RNN)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-gated-recurrent-units" class="md-nav__link">
    3 Gated Recurrent Units
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-long-short-term-memories" class="md-nav__link">
    4 Long-Short-Term-Memories
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" class="md-nav__link">
      07 Vanishing Gradients and Fancy RNNs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/" class="md-nav__link">
      08 Machine Translation, Sequence-to-sequence and Attention
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" class="md-nav__link">
      09 Practical Tips for Final Projects
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/" class="md-nav__link">
      10 Question Answering and the Default Final Project
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" class="md-nav__link">
      11 ConvNets for NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/" class="md-nav__link">
      12 Information from parts of words Subword Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/" class="md-nav__link">
      13 Modeling contexts of use Contextual Representations and Pretraining
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/" class="md-nav__link">
      14 Transformers and Self-Attention For Generative Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-15-Natural%20Language%20Generation/" class="md-nav__link">
      15 Natural Language Generation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-16-Coreference%20Resolution/" class="md-nav__link">
      16 Coreference Resolution
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-17-Multitask%20Learning/" class="md-nav__link">
      17 Multitask Learning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/" class="md-nav__link">
      18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/" class="md-nav__link">
      19 Safety, Bias, and Fairness
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/" class="md-nav__link">
      20 The Future of NLP + Deep Learning
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      For MkDocs
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="For MkDocs" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon"></span>
        For MkDocs
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../MkDocs_demo/" class="md-nav__link">
      Demo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Material%20Theme%20Tutorial/" class="md-nav__link">
      Material Theme Tutorial
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models" class="md-nav__link">
    Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#notes-05-language-models-rnn-gru-and-lstm" class="md-nav__link">
    Notes 05 Language Models, RNN, GRU and LSTM
  </a>
  
    <nav class="md-nav" aria-label="Notes 05 Language Models, RNN, GRU and LSTM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-language-models" class="md-nav__link">
    1 Language Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-recurrent-neural-networks-rnn" class="md-nav__link">
    2 Recurrent Neural Networks (RNN)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-gated-recurrent-units" class="md-nav__link">
    3 Gated Recurrent Units
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-long-short-term-memories" class="md-nav__link">
    4 Long-Short-Term-Memories
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/LooperXX/LooperXX.github.io/edit/master/docs/CS224n-2019-06-The probability of a sentence Recurrent Neural Networks and Language Models.md" title="编辑此页" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  <h1>06 The probability of a sentence Recurrent Neural Networks and Language Models</h1>
                
                <h2 id="lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models">Lecture 06 The probability of a sentence Recurrent Neural Networks and Language Models<a class="headerlink" href="#lecture-06-the-probability-of-a-sentence-recurrent-neural-networks-and-language-models" title="Permanent link">&para;</a></h2>
<p><strong>Overview</strong></p>
<ul>
<li>介绍一个新的NLP任务<ul>
<li>Language Modeling (motivate RNNs)</li>
</ul>
</li>
<li>介绍一个新的神经网络家族<ul>
<li><strong>Recurrent Neural Networks (RNNs)</strong></li>
</ul>
</li>
</ul>
<p><strong>Language Modeling</strong></p>
<ul>
<li>语言建模的任务是预测下一个单词是什么。</li>
</ul>
<p><img alt="1560953937807" src="../imgs/1560953937807.png" /></p>
<ul>
<li>更正式的说法是：给定一个单词序列 <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</script></span> ，计算下一个单词 <span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 的概率分布</li>
</ul>
<div>
<div class="MathJax_Preview">
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
</div>
<script type="math/tex; mode=display">
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
</script>
</div>
<ul>
<li>其中，<span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 可以是词表中的任意单词 <span><span class="MathJax_Preview">V=\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{|V|}\right\}</span><script type="math/tex">V=\left\{\boldsymbol{w}_{1}, \ldots, \boldsymbol{w}_{|V|}\right\}</script></span></li>
<li>这样做的系统称为 <strong>Language Model</strong> 语言模型</li>
<li>还可以将语言模型看作是一个将概率分配给一段文本的系统</li>
<li>例如，如果我们有一段文本 <span><span class="MathJax_Preview">x^{(1)},\dots,x^{(T)}</span><script type="math/tex">x^{(1)},\dots,x^{(T)}</script></span>  则这段文本的概率(根据语言模型)为</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned} P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}\right) &amp;=P\left(\boldsymbol{x}^{(1)}\right) \times P\left(\boldsymbol{x}^{(2)} | \boldsymbol{x}^{(1)}\right) \times \cdots \times P\left(\boldsymbol{x}^{(T)} | \boldsymbol{x}^{(T-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \\ &amp;=\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}\right) &=P\left(\boldsymbol{x}^{(1)}\right) \times P\left(\boldsymbol{x}^{(2)} | \boldsymbol{x}^{(1)}\right) \times \cdots \times P\left(\boldsymbol{x}^{(T)} | \boldsymbol{x}^{(T-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \\ &=\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right) \end{aligned}
</script>
</div>
<ul>
<li>语言模型提供的是 <span><span class="MathJax_Preview">\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right)</span><script type="math/tex">\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} | \boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right)</script></span></li>
</ul>
<p><strong>n-gram Language Models</strong>
$$
\text{the students opened their  ______}
$$</p>
<ul>
<li><strong><u>问题</u></strong> ：如何学习一个语言模型？</li>
<li><strong><u>回答</u></strong> (pre-DeepLearning)：学习一个 n-gram 语言模型</li>
<li><u>定义</u> ：n-gram 是 一个由n个连续单词组成的块<ul>
<li>unigrams: “the”, “students”, “opened”, ”their”</li>
<li>bigrams: “the students”, “students opened”, “opened their”</li>
<li>trigrams: “the students opened”, “students opened their” </li>
<li>4-grams: “the students opened their”</li>
</ul>
</li>
<li>
<p><strong>想法</strong> ：收集关于不同n-gram出现频率的统计数据，并使用这些数据预测下一个单词。</p>
</li>
<li>
<p>首先，我们做一个 <strong>简化假设</strong> ：<span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 只依赖于前面的n-1个单词</p>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned}
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
&amp; =P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)
\\ &amp;=\frac{P\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{P\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)
& =P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)
\\ &=\frac{P\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{P\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
\end{aligned}
</script>
</div>
<p>具体含义如下图所示</p>
<p><img alt="1560954979111" src="../imgs/1560954979111.png" /></p>
<ul>
<li><strong><u>问题</u></strong> ：如何得到n-gram和(n-1)-gram的概率？</li>
<li><strong><u>回答</u></strong> ：通过在一些大型文本语料库中计算它们(统计近似)</li>
</ul>
<div>
<div class="MathJax_Preview">
\approx \frac{\operatorname{count}\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{\operatorname{count}\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
</div>
<script type="math/tex; mode=display">
\approx \frac{\operatorname{count}\left(\boldsymbol{x}^{(t+1)}, \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}{\operatorname{count}\left(\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(t-n+2)}\right)}
</script>
</div>
<p>假设我们正在学习一个 <strong>4-gram</strong> 的语言模型</p>
<p><img alt="1560955135094" src="../imgs/1560955135094.png" /></p>
<p>例如，假设在语料库中：</p>
<ul>
<li>“students opened their” 出现了1000次</li>
<li>“students opened their books” 出现了400次<ul>
<li><span><span class="MathJax_Preview">P(\text { books } | \text { students opened their })=0.4</span><script type="math/tex">P(\text { books } | \text { students opened their })=0.4</script></span></li>
</ul>
</li>
<li>“students opened their exams” 出现了100次<ul>
<li><span><span class="MathJax_Preview">P(\text { exams } | \text { students opened their })=0.1</span><script type="math/tex">P(\text { exams } | \text { students opened their })=0.1</script></span></li>
</ul>
</li>
<li>我们应该忽视上下文中的“proctor”吗？<ul>
<li>在本例中，上下文里出现了“proctor”，所以exams在这里的上下文中应该是比books概率更大的。</li>
</ul>
</li>
</ul>
<p><strong>Sparsity Problems with n-gram Language Models</strong></p>
<p><img alt="1560955544533" src="../imgs/1560955544533.png" /></p>
<ul>
<li><strong><u>问题</u></strong> ：如果“students open their <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span>” 从未出现在数据中，那么概率值为 0</li>
<li><strong><u>(Partial)解决方案</u></strong> ：为每个 <span><span class="MathJax_Preview">w\in V</span><script type="math/tex">w\in V</script></span> 添加极小数 <span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> 。这叫做平滑。这使得词表中的每个单词都至少有很小的概率。</li>
<li><strong><u>问题</u></strong> ：如果“students open their” 从未出现在数据中，那么我们将无法计算任何单词 <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> 的概率值</li>
<li><strong><u>(Partial)解决方案</u></strong> ：将条件改为“open their”。这叫做后退。</li>
</ul>
<blockquote>
<p>Note: n 的增加使稀疏性问题变得更糟。一般情况下 n 不能大于5。</p>
</blockquote>
<p><strong>Storage Problems with n-gram Language Models</strong></p>
<p><img alt="1560956032892" src="../imgs/1560956032892.png" /></p>
<p>增加 n 或增加语料库都会增加模型大小</p>
<p><strong>n-gram Language Models in practice</strong></p>
<ul>
<li>你可以在你的笔记本电脑上，在几秒钟内建立一个超过170万个单词库(Reuters)的简单的三元组语言模型</li>
<li>Reuters 是 商业和金融新闻的数据集</li>
</ul>
<p><img alt="1560956304459" src="../imgs/1560956304459.png" /></p>
<p><strong><u>稀疏性问题</u></strong> ：概率分布的粒度不大。“today the company” 和 “today the bank”都是<span><span class="MathJax_Preview">\frac{4}{26}</span><script type="math/tex">\frac{4}{26}</script></span> ，都只出现过四次</p>
<p><strong>Generating text with a n-gram Language Model</strong></p>
<ul>
<li>还可以使用语言模型来生成文本</li>
</ul>
<p><img alt="1560956554190" src="../imgs/1560956554190.png" /></p>
<p><img alt="1560956564197" src="../imgs/1560956564197.png" /></p>
<p><img alt="1560956573012" src="../imgs/1560956573012.png" /></p>
<p>使用trigram运行以上生成过程时，会得到如下文本</p>
<div class="highlight"><pre><span></span><code>today the price of gold per ton , while production of shoe lasts and shoe industry , the bank intervened just after it considered and rejected an imf demand to rebuild depleted european stocks , sept 30 end primary 76 cts a share .
</code></pre></div>

<p><strong>令人惊讶的是其具有语法但是是不连贯的。如果我们想要很好地模拟语言，我们需要同时考虑三个以上的单词。但增加 n 使模型的稀疏性问题恶化，模型尺寸增大。</strong></p>
<p><strong>How to build a neural Language Model?</strong></p>
<ul>
<li>回忆一下语言模型任务<ul>
<li>输入：单词序列 <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \ldots, \boldsymbol{x}^{(t)}</script></span></li>
<li>输出：下一个单词的概率分布 <span><span class="MathJax_Preview">P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)</span><script type="math/tex">P\left(\boldsymbol{x}^{(t+1)} | \boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)</script></span></li>
</ul>
</li>
</ul>
<p>window-based neural model 在第三讲中被用于NER问题</p>
<p><img alt="1560956976214" src="../imgs/1560956976214.png" /></p>
<p><strong>A fixed-window neural Language Model</strong></p>
<p><img alt="1560957097608" src="../imgs/1560957097608.png" /></p>
<p>使用和NER问题中同样网络结构</p>
<p><img alt="1560957211434" src="../imgs/1560957211434.png" /></p>
<p>超越 n-gram 语言模型的 <strong>改进</strong></p>
<ul>
<li>没有稀疏性问题</li>
<li>不需要观察到所有的n-grams</li>
</ul>
<p>存在的问题</p>
<ul>
<li>固定窗口太小</li>
<li>扩大窗口就需要扩大权重矩阵 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span></li>
<li>窗口再大也不够用</li>
<li><span><span class="MathJax_Preview">x^{(1)}</span><script type="math/tex">x^{(1)}</script></span> 和 <span><span class="MathJax_Preview">x^{(2)}</span><script type="math/tex">x^{(2)}</script></span> 乘以完全不同的权重。输入的处理 <strong>不对称</strong>。</li>
</ul>
<p>我们需要一个神经结构，可以处理任何长度的输入</p>
<p><strong>Recurrent Neural Networks (RNN)</strong></p>
<p><u>核心想法</u>：重复使用 <strong>相同</strong> 的权重矩阵 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span></p>
<p><img alt="1560957560354" src="../imgs/1560957560354.png" /></p>
<p><img alt="1560958598963" src="../imgs/1560958598963.png" /></p>
<p>RNN的 <strong>优点</strong></p>
<ul>
<li>可以处理 <strong>任意长度</strong> 的输入</li>
<li>步骤 t 的计算(理论上)可以使用 <strong>许多步骤前</strong> 的信息</li>
<li><strong>模型大小不会</strong> 随着输入的增加而**增加**</li>
<li>在每个时间步上应用相同的权重，因此在处理输入时具有 <strong>对称性</strong></li>
</ul>
<p>RNN的 <strong>缺点</strong></p>
<ul>
<li>递归计算速度 <strong>慢</strong></li>
<li>在实践中，很难从 **许多步骤前**返回信息</li>
<li>后面的课程中会详细介绍</li>
</ul>
<p><strong>Training a RNN Language Model</strong></p>
<ul>
<li>获取一个较大的文本语料库，该语料库是一个单词序列</li>
<li>输入RNN-LM；计算每个步骤 t 的输出分布<ul>
<li>即预测到目前为止给定的每个单词的概率分布</li>
</ul>
</li>
<li>步骤 t 上的损失函数为预测概率分布 <span><span class="MathJax_Preview">\hat{\boldsymbol{y}}^{(t)}</span><script type="math/tex">\hat{\boldsymbol{y}}^{(t)}</script></span> 与真实下一个单词 <span><span class="MathJax_Preview">{\boldsymbol{y}}^{(t)}</span><script type="math/tex">{\boldsymbol{y}}^{(t)}</script></span> (<span><span class="MathJax_Preview">x^{(t+1)}</span><script type="math/tex">x^{(t+1)}</script></span> 的独热向量)之间的交叉熵</li>
</ul>
<div>
<div class="MathJax_Preview">
J^{(t)}(\theta)=C E\left(\boldsymbol{y}^{(t)}, \hat{\boldsymbol{y}}^{(t)}\right)=-\sum_{w \in V} \boldsymbol{y}_{w}^{(t)} \log \hat{\boldsymbol{y}}_{w}^{(t)}=-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</div>
<script type="math/tex; mode=display">
J^{(t)}(\theta)=C E\left(\boldsymbol{y}^{(t)}, \hat{\boldsymbol{y}}^{(t)}\right)=-\sum_{w \in V} \boldsymbol{y}_{w}^{(t)} \log \hat{\boldsymbol{y}}_{w}^{(t)}=-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</script>
</div>
<ul>
<li>将其平均，得到整个培训集的 <strong>总体损失</strong></li>
</ul>
<div>
<div class="MathJax_Preview">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</div>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}
</script>
</div>
<p><img alt="1560959454417" src="../imgs/1560959454417.png" /></p>
<div>
<div class="MathJax_Preview">
J^{(1)}(\theta)+J^{(2)}(\theta)+J^{(3)}(\theta)+J^{(4)}(\theta)+\ldots=J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</div>
<script type="math/tex; mode=display">
J^{(1)}(\theta)+J^{(2)}(\theta)+J^{(3)}(\theta)+J^{(4)}(\theta)+\ldots=J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</script>
</div>
<ul>
<li>然而：计算 <strong>整个语料库</strong>  <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</script></span>的损失和梯度太昂贵了</li>
</ul>
<div>
<div class="MathJax_Preview">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</div>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)
</script>
</div>
<ul>
<li>在实践中，我们通常将 <span><span class="MathJax_Preview">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</span><script type="math/tex">\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}</script></span> 看做一个 <strong>句子</strong> 或是 <strong>文档</strong></li>
<li><u>回忆</u> ：随机梯度下降允许我们计算小块数据的损失和梯度，并进行更新。</li>
<li>计算一个句子的损失<span><span class="MathJax_Preview">J(\theta)</span><script type="math/tex">J(\theta)</script></span>(实际上是一批句子)，计算梯度和更新权重。重复上述操作。</li>
</ul>
<p><strong>Backpropagation for RNNs</strong></p>
<p><img alt="1560960125900" src="../imgs/1560960125900.png" /></p>
<p><strong><u>问题</u></strong> ：关于 <strong>重复的</strong> 权重矩阵 <span><span class="MathJax_Preview">W_h</span><script type="math/tex">W_h</script></span> 的偏导数 <span><span class="MathJax_Preview">J^{(t)}(\theta)</span><script type="math/tex">J^{(t)}(\theta)</script></span> </p>
<p><strong><u>回答</u></strong> ：重复权重的梯度是每次其出现时的梯度的总和</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}=\sum_{i=1}^{t}\left.\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}\right|_{(i)}
</div>
<script type="math/tex; mode=display">
\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}=\sum_{i=1}^{t}\left.\frac{\partial J^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}\right|_{(i)}
</script>
</div>
<p><strong>Multivariable Chain Rule</strong></p>
<p><img alt="1560960413574" src="../imgs/1560960413574.png" /></p>
<p>对于一个多变量函数 <span><span class="MathJax_Preview">f(x,y)</span><script type="math/tex">f(x,y)</script></span> 和两个单变量函数 <span><span class="MathJax_Preview">x(t)</span><script type="math/tex">x(t)</script></span> 和 <span><span class="MathJax_Preview">y(t)</span><script type="math/tex">y(t)</script></span> ，其链式法则如下：</p>
<div>
<div class="MathJax_Preview">
\frac{d}{d t} f(x(t), y(t))=\frac{\partial f}{\partial x} \frac{d x}{d t}+\frac{\partial f}{\partial y} \frac{d y}{d t}
</div>
<script type="math/tex; mode=display">
\frac{d}{d t} f(x(t), y(t))=\frac{\partial f}{\partial x} \frac{d x}{d t}+\frac{\partial f}{\partial y} \frac{d y}{d t}
</script>
</div>
<p><strong>Backpropagation for RNNs: Proof sketch</strong></p>
<p><img alt="1560960544222" src="../imgs/1560960544222.png" /></p>
<p><strong>Backpropagation for RNNs</strong></p>
<p><img alt="1560960622191" src="../imgs/1560960622191.png" /></p>
<ul>
<li><strong><u>问题</u></strong> ： 如何计算？</li>
<li><strong><u>回答</u></strong> ：反向传播的时间步长 <span><span class="MathJax_Preview">i=t,\dots,0</span><script type="math/tex">i=t,\dots,0</script></span> 。累加梯度。这个算法叫做“<strong>backpropagation through time</strong>”</li>
</ul>
<p><strong>Generating text with a RNN Language Model</strong></p>
<p>就像n-gram语言模型一样，您可以使用RNN语言模型通过 <strong>重复采样</strong> 来 <strong>生成文本</strong> 。采样输出是下一步的输入。</p>
<p><img alt="1560960753273" src="../imgs/1560960753273.png" /></p>
<ul>
<li>相比n-gram更流畅，语法正确，但总体上仍然很不连贯</li>
<li>食谱的例子中，生成的文本并没有记住文本的主题是什么</li>
<li>
<p>哈利波特的例子中，甚至有体现出了人物的特点，并且引号的开闭也没有出现问题</p>
<ul>
<li>也许某些神经元或者隐藏状态在跟踪模型的输出是否在引号中</li>
</ul>
</li>
<li>
<p>RNN是否可以和手工规则结合？</p>
<ul>
<li>例如Beam Serach，但是可能很难做到</li>
</ul>
</li>
</ul>
<p><strong>Evaluating Language Models</strong></p>
<ul>
<li>标准语言模型评估指标是 <strong>perplexity</strong> 困惑度</li>
</ul>
<p><img alt="1560961990924" src="../imgs/1560961990924.png" /></p>
<ul>
<li>这等于交叉熵损失 <span><span class="MathJax_Preview">J(\theta)</span><script type="math/tex">J(\theta)</script></span> 的指数</li>
</ul>
<div>
<div class="MathJax_Preview">
=\prod_{t=1}^{T}\left(\frac{1}{\hat{y}_{x_{t+1}}^{(t)}}\right)^{1 / T}=\exp \left(\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}\right)=\exp (J(\theta))
</div>
<script type="math/tex; mode=display">
=\prod_{t=1}^{T}\left(\frac{1}{\hat{y}_{x_{t+1}}^{(t)}}\right)^{1 / T}=\exp \left(\frac{1}{T} \sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}\right)=\exp (J(\theta))
</script>
</div>
<ul>
<li>低困惑度是更好的</li>
</ul>
<p><strong>RNNs have greatly improved perplexity</strong></p>
<p><img alt="1560962257990" src="../imgs/1560962257990.png" /></p>
<p><strong>Why should we care about Language Modeling?</strong></p>
<ul>
<li>语言模型是一项 <strong>基准测试</strong> 任务，它帮助我们 <strong>衡量</strong> 我们在理解语言方面的 <strong>进展</strong><ul>
<li>生成下一个单词，需要语法，句法，逻辑，推理，现实世界的知识等</li>
</ul>
</li>
<li>语言建模是许多NLP任务的 <strong>子组件</strong>，尤其是那些涉及 <strong>生成文本</strong> 或 <strong>估计文本概率</strong> 的任务<ul>
<li>预测性打字</li>
<li>语音识别</li>
<li>手写识别</li>
<li>拼写/语法纠正</li>
<li>作者识别</li>
<li>机器翻译</li>
<li>摘要</li>
<li>对话</li>
<li>等等</li>
</ul>
</li>
</ul>
<p><strong>Recap</strong></p>
<ul>
<li>语言模型： <strong>预测下一个单词</strong> 的系统</li>
<li>递归神经网络：一系列神经网络<ul>
<li>采用任意长度的顺序输入</li>
<li>在每一步上应用相同的权重</li>
<li>可以选择在每一步上生成输出</li>
</ul>
</li>
<li>递归神经网络<span><span class="MathJax_Preview">\neq</span><script type="math/tex">\neq</script></span>语言模型</li>
<li>我们已经证明，RNNs是构建LM的一个很好的方法。</li>
<li>但RNNs的用处要大得多!</li>
</ul>
<p><strong>RNNs can be used for tagging</strong> </p>
<p>e.g. <u>part-of-speech tagging</u>, named entity recognition</p>
<p><img alt="1560963423563" src="../imgs/1560963423563.png" /></p>
<p><strong>RNNs can be used for sentence classification</strong></p>
<p>e.g. <u>sentiment classification</u></p>
<p><img alt="1560963469195" src="../imgs/1560963469195.png" /></p>
<p><img alt="1560963485192" src="../imgs/1560963485192.png" /></p>
<p>如何计算句子编码</p>
<ul>
<li>使用最终隐层状态</li>
<li>使用所有隐层状态的逐元素最值或均值</li>
</ul>
<p><strong>RNNs can be used as an encoder module</strong> </p>
<p>e.g. <u>question answering</u>, machine translation, many other tasks!</p>
<p><img alt="1560963507012" src="../imgs/1560963507012.png" /></p>
<p>Encoder的结构在NLP中非常常见</p>
<p><strong>RNN-LMs can be used to generate text</strong> </p>
<p>e.g. <u>speech recognition</u>, machine translation, summarization</p>
<p><img alt="1560963533367" src="../imgs/1560963533367.png" /></p>
<p>这是一个条件语言模型的示例。我们使用语言模型组件，并且最关键的是，我们根据条件来调整它</p>
<p>稍后我们会更详细地看到机器翻译。</p>
<p><strong>A note on terminology</strong></p>
<p>本课提到的RNN是 ““vanilla RNN”</p>
<p>下节课将会学习GRU和LSTM以及多层RNN</p>
<p>本课程结束时，你会理解类似“stacked bidirectional LSTM with residual connections and self-attention”的短语</p>
<p><img alt="1560963659418" src="../imgs/1560963659418.png" /></p>
<h2 id="notes-05-language-models-rnn-gru-and-lstm">Notes 05 Language Models, RNN, GRU and LSTM<a class="headerlink" href="#notes-05-language-models-rnn-gru-and-lstm" title="Permanent link">&para;</a></h2>
<p><strong>Keyphrases</strong>: Language Models. RNN. Bi-directional RNN. Deep RNN. GRU. LSTM.</p>
<h3 id="1-language-models">1 Language Models<a class="headerlink" href="#1-language-models" title="Permanent link">&para;</a></h3>
<p><strong>1.1 Introduction</strong></p>
<p>语言模型计算特定序列中多个单词的出现概率。一个 m 个单词的序列 <span><span class="MathJax_Preview">\left\{w_{1}, \dots, w_{m}\right\}</span><script type="math/tex">\left\{w_{1}, \dots, w_{m}\right\}</script></span> 的概率定义为 <span><span class="MathJax_Preview">P\left(w_{1}, \dots, w_{m}\right)</span><script type="math/tex">P\left(w_{1}, \dots, w_{m}\right)</script></span> 。单词 <span><span class="MathJax_Preview">w_i</span><script type="math/tex">w_i</script></span> 前有一定数量的单词，其特性会根据它在文档中的位置而改变， <span><span class="MathJax_Preview">P\left(w_{1}, \dots, w_{m}\right)</span><script type="math/tex">P\left(w_{1}, \dots, w_{m}\right)</script></span> 一般只考虑前 <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 个单词而不是考虑全部之前的单词。</p>
<div>
<div class="MathJax_Preview">
P\left(w_{1}, \ldots, w_{m}\right)=\prod_{i=1}^{i=m} P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right) \approx \prod_{i=1}^{i=m} P\left(w_{i} | w_{i-n}, \ldots, w_{i-1}\right)
</div>
<script type="math/tex; mode=display">
P\left(w_{1}, \ldots, w_{m}\right)=\prod_{i=1}^{i=m} P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right) \approx \prod_{i=1}^{i=m} P\left(w_{i} | w_{i-n}, \ldots, w_{i-1}\right)
</script>
</div>
<p>上面的公式在语音识别和机器翻译系统对判断一个词序列是否为一个输入句子的准确翻译起到了重要的作用。在现有的机器翻译系统中，对每个短语/句子翻译，系统生成一些候选的词序列（例如，{I have,I has,I had,me have,me had}），并对其评分以确定最可能的翻译序列。</p>
<p>在机器翻译中，对一个输入短语，通过评判每个候选输出词序列的得分的高低，来选出最好的词顺序。为此，模型可以在不同的单词排序或单词选择之间进行选择。它将通过一个概率函数运行所有单词序列候选项，并为每个候选项分配一个分数，从而实现这一目标。最高得分的序列就是翻译结果。例如，相比 small is the cat，翻译系统会给 the cat is small 更高的得分；相比 walking house after school，翻译系统会给 walking home after school 更高的得分。</p>
<p><strong>1.2 n-gram Language Models</strong></p>
<p>为了计算这些概率，每个 n-gram 的计数将与每个单词的频率进行比较，这个称为 n-gram 语言模型。例如，如果选择 bi-gram 模型，每一个 bi-gram 的频率，通过将单词与其前一个单词相结合进行计算，然后除以对应的 uni-gram 的频率。下面的两个公式展示了 bi-gram 模型和 tri-gram 模型的区别。</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} p\left(w_{2} | w_{1}\right) &amp;=\frac{\operatorname{count}\left(w_{1}, w_{2}\right)}{\operatorname{count}\left(w_{1}\right)} \\ p\left(w_{3} | w_{1}, w_{2}\right) &amp;=\frac{\operatorname{count}\left(w_{1}, w_{2}, w_{3}\right)}{\operatorname{count}\left(w_{1}, w_{2}\right)} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} p\left(w_{2} | w_{1}\right) &=\frac{\operatorname{count}\left(w_{1}, w_{2}\right)}{\operatorname{count}\left(w_{1}\right)} \\ p\left(w_{3} | w_{1}, w_{2}\right) &=\frac{\operatorname{count}\left(w_{1}, w_{2}, w_{3}\right)}{\operatorname{count}\left(w_{1}, w_{2}\right)} \end{aligned}
</script>
</div>
<p>上式 tri-gram 模型的关系主要是基于一个固定的上下文窗口(即前n个单词)预测下一个单词。一般 n 的取值为多大才好呢？在某些情况下，前面的连续的 n 个单词的窗口可能不足以捕获足够的上下文信息。例如，考虑句子（类似完形填空，预测下一个最可能的单词）</p>
<p>“Asthe proctor started the clock, the students opened their ____”。如果窗口只是基于前面的三个单词“the students opened their”，那么急于这些语料计算的下划线中最有可能出现的单词就是为“books”——但是如果 n 足够大，能包括全部的上下文，那么下划线中最优可能出现的单词会是“exam”。</p>
<p>这就引出了 n-gram 语言模型的两个主要问题：稀疏性和存储。</p>
<p><strong>Sparsity problems with n-gram Language models</strong></p>
<p>n-gram 语言模型的问题源于两个问题。</p>
<p>首先，注意公式中的分子。如果 <span><span class="MathJax_Preview">w_1,w_2.w_3</span><script type="math/tex">w_1,w_2.w_3</script></span> 在语料中从未出现过，那么 <span><span class="MathJax_Preview">w_3</span><script type="math/tex">w_3</script></span> 的概率就是 0。为了解决这个问题，在每个单词计数后面加上一个很小的 <span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> ，这就是平滑操作。</p>
<p>然后，考虑公式中的分母。 如果 <span><span class="MathJax_Preview">w_1,w_2</span><script type="math/tex">w_1,w_2</script></span> 在语料中从未出现过，那么 <span><span class="MathJax_Preview">w_3</span><script type="math/tex">w_3</script></span> 的概率将会无法计算。为了解决这个问题，这里可以只是单独考虑 <span><span class="MathJax_Preview">w_2</span><script type="math/tex">w_2</script></span> ，这就是“backoff”操作。</p>
<p>增加 n 会让稀疏问题更加严重，所以一般 <span><span class="MathJax_Preview">n \leq 5</span><script type="math/tex">n \leq 5</script></span> 。</p>
<p><strong>Storage problems with n-gram Language models</strong></p>
<p>我们知道需要存储在语料库中看到的所有 n-gram 的统计数。随着 n 的增加(或语料库大小的增加)，模型的大小也会增加。</p>
<p><strong>1.3 Window-based Neural Language Model</strong></p>
<p>Bengio 的论文《A Neural Probabilistic Language Model》中首次解决了上面所说的“维度灾难”，这篇论文提出一个自然语言处理的大规模的深度学习模型，这个模型能够通过学习单词的分布式表示，以及用这些表示来表示单词的概率函数。下图展示了对应的神经网络结构，在这个模型中，输入向量在隐藏层和输出层中都被使用。</p>
<p><img alt="1560997851163" src="../imgs/1560997851163.png" /></p>
<p>下面公式展示了由标准 tanh 函数（即隐藏层）组成的 softmax 函数的参数以及线性函数 <span><span class="MathJax_Preview">W^{(3)} x+b^{(3)}</span><script type="math/tex">W^{(3)} x+b^{(3)}</script></span> ，捕获所有前面 n 个输入词向量。</p>
<div>
<div class="MathJax_Preview">
\hat{y}=\operatorname{softmax}\left(W^{(2)} \tanh \left(W^{(1)}x+b^{(1)}\right)+W^{(3)} x+b^{(3)}\right)
</div>
<script type="math/tex; mode=display">
\hat{y}=\operatorname{softmax}\left(W^{(2)} \tanh \left(W^{(1)}x+b^{(1)}\right)+W^{(3)} x+b^{(3)}\right)
</script>
</div>
<p>注意权重矩阵 <span><span class="MathJax_Preview">W^{(1)}</span><script type="math/tex">W^{(1)}</script></span> 是应用在词向量上（上图中的绿色实线箭头）， <span><span class="MathJax_Preview">W^{(2)}</span><script type="math/tex">W^{(2)}</script></span> 是应用在隐藏层（也是绿色实线箭头）和 <span><span class="MathJax_Preview">W^{(3)}</span><script type="math/tex">W^{(3)}</script></span> 是应用在词向量（绿色虚线箭头）。</p>
<p>这个模型的简化版本如下图所示，其中蓝色的层表示输入单词的 embedding 拼接： <span><span class="MathJax_Preview">e=\left[e^{(1)} ; e^{(2)} ; e^{(3)} ; e^{(4)}\right]</span><script type="math/tex">e=\left[e^{(1)} ; e^{(2)} ; e^{(3)} ; e^{(4)}\right]</script></span> ，红色的层表示隐藏层： <span><span class="MathJax_Preview">\boldsymbol{h}=f\left(\boldsymbol{W} e+\boldsymbol{b}_{1}\right)</span><script type="math/tex">\boldsymbol{h}=f\left(\boldsymbol{W} e+\boldsymbol{b}_{1}\right)</script></span> ，绿色的输出分布是对词表的一个 softmax 概率分布： <span><span class="MathJax_Preview">\hat{y}=\operatorname{softmax}\left(U h+b_{2}\right)</span><script type="math/tex">\hat{y}=\operatorname{softmax}\left(U h+b_{2}\right)</script></span> 。</p>
<p><img alt="1560998151143" src="../imgs/1560998151143.png" /></p>
<h3 id="2-recurrent-neural-networks-rnn">2 Recurrent Neural Networks (RNN)<a class="headerlink" href="#2-recurrent-neural-networks-rnn" title="Permanent link">&para;</a></h3>
<p>传统的翻译模型只能以有限窗口大小的前 n 个单词作为条件进行语言模型建模，循环神经网络与其不同，RNN 有能力以语料库中所有前面的单词为条件进行语言模型建模。</p>
<p>下图展示的 RNN 的架构，其中矩形框是在一个时间步的一个隐藏层 t。</p>
<p><img alt="1560998193031" src="../imgs/1560998193031.png" /></p>
<p>每个这样的隐藏层都有若干个神经元，每个神经元对输入向量用一个线性矩阵运算然后通过非线性变化（例如 tanh 函数）得到输出。在每一个时间步，隐藏层都有两个输入：前一个时间步的隐藏层 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 和当前时间步的输入 <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> ，前一个时间步的隐藏层 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 通过和权重矩阵 <span><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> 相乘和当前时间步的输入 <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 和权重矩阵 <span><span class="MathJax_Preview">W^{(hx)}</span><script type="math/tex">W^{(hx)}</script></span> 相乘得到当前时间步的隐藏层 <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> ，然后再将 <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 和权重矩阵 <span><span class="MathJax_Preview">W^{(S)}</span><script type="math/tex">W^{(S)}</script></span> 相乘，接着对整个词表通过 softmax 计算得到下一个单词的预测结果 <span><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span> ，如下面公式所示：</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} h_{t} &amp;=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{[t]}\right) \\ \hat{y} &amp;=\operatorname{softmax}\left(W^{(S)} h_{t}\right) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} h_{t} &=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{[t]}\right) \\ \hat{y} &=\operatorname{softmax}\left(W^{(S)} h_{t}\right) \end{aligned}
</script>
</div>
<p>每个神经元的输入和输出如下图所示：</p>
<p><img alt="1560998383025" src="../imgs/1560998383025.png" /></p>
<p>在这里一个有意思的地方是在每一个时间步使用相同的权重 <span><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> 和 <span><span class="MathJax_Preview">W^{(hx)}</span><script type="math/tex">W^{(hx)}</script></span> 。这样模型需要学习的参数就变少了，这与输入序列的长度无关——这从而解决了维度灾难。</p>
<p>以下是网络中每个参数相关的详细信息：</p>
<ul>
<li><span><span class="MathJax_Preview">x_{1}, \dots, x_{t-1}, x_{t}, x_{t+1}, \dots x_{T}</span><script type="math/tex">x_{1}, \dots, x_{t-1}, x_{t}, x_{t+1}, \dots x_{T}</script></span> ：含有 T 个单词的语料库对应的词向量。</li>
<li><span><span class="MathJax_Preview">h_{t}=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{t}\right)</span><script type="math/tex">h_{t}=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{t}\right)</script></span> ：每个时间步 t 的隐藏层的输出特征的计算关系</li>
<li><span><span class="MathJax_Preview">x_{t} \in \mathbb{R}^{d}</span><script type="math/tex">x_{t} \in \mathbb{R}^{d}</script></span> ：在时间步 t 的输入词向量。</li>
<li><span><span class="MathJax_Preview">W^{h x} \in \mathbb{R}^{D_{h} \times d}</span><script type="math/tex">W^{h x} \in \mathbb{R}^{D_{h} \times d}</script></span> ：输入词向量 <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 对应的权重矩阵。</li>
<li><span><span class="MathJax_Preview">W^{h h} \in \mathbb{R}^{D_{h} \times D_{h}}</span><script type="math/tex">W^{h h} \in \mathbb{R}^{D_{h} \times D_{h}}</script></span> ：上一个时间步的输出 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 对应的权重矩阵。</li>
<li><span><span class="MathJax_Preview">h_{t-1} \in \mathbb{R}^{D_{h}}</span><script type="math/tex">h_{t-1} \in \mathbb{R}^{D_{h}}</script></span> ：上一个时间步 t-1 的非线性函数输出。 <span><span class="MathJax_Preview">h_{0} \in \mathbb{R}^{D_{h}}</span><script type="math/tex">h_{0} \in \mathbb{R}^{D_{h}}</script></span> 是在时间步 t=0 的隐藏层的一个初始化向量。</li>
<li>
<p><span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> ：非线性函数（这里是 sigmoid 函数）。</p>
</li>
<li>
<p><span><span class="MathJax_Preview">\hat{y}=\operatorname{softmax}\left(W^{(S)} h_{t}\right)</span><script type="math/tex">\hat{y}=\operatorname{softmax}\left(W^{(S)} h_{t}\right)</script></span> ：在每个时间步 t 全部单词的概率分布输出。本质上， <span><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span> 是给定文档上下文分数（例如 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> ）和最后观测的词向量 <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> ，对一个出现单词的预测。这里， <span><span class="MathJax_Preview">W^{(S)} \in \mathbb{R}^{|V| \times D_{h}}</span><script type="math/tex">W^{(S)} \in \mathbb{R}^{|V| \times D_{h}}</script></span> ， <span><span class="MathJax_Preview">\hat{y} \in \mathbb{R}^{|V|}</span><script type="math/tex">\hat{y} \in \mathbb{R}^{|V|}</script></span> ，其中 <span><span class="MathJax_Preview">|V|</span><script type="math/tex">|V|</script></span> 是词汇表的大小。</p>
</li>
</ul>
<p>一个 RNN 语言模型的例子如下图所示。下图中的符号有一些的不同： <span><span class="MathJax_Preview">W_h</span><script type="math/tex">W_h</script></span> 等同于 <span><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> ，  <span><span class="MathJax_Preview">W_e</span><script type="math/tex">W_e</script></span> 等同于 <span><span class="MathJax_Preview">W^{(hx)}</span><script type="math/tex">W^{(hx)}</script></span> ， <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> 等同于 <span><span class="MathJax_Preview">W^{(S)}</span><script type="math/tex">W^{(S)}</script></span> 。 <span><span class="MathJax_Preview">E</span><script type="math/tex">E</script></span> 表示单词输入 <span><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> 转化为 <span><span class="MathJax_Preview">e^{(t)}</span><script type="math/tex">e^{(t)}</script></span> 。</p>
<p>在 RNN 中常用的损失函数是在之前介绍过的交叉熵误差。下面的公式是这个函数在时间步 t 全部单词的求和。最后计算词表中的 softmax 计算结果展示了基于前面所有的单词对输出单词 <span><span class="MathJax_Preview">x^{(5)}</span><script type="math/tex">x^{(5)}</script></span> 的不同选择的概率分布。这时的输入可以比 4 到 5 个单词更长。</p>
<p><img alt="1560998943983" src="../imgs/1560998943983.png" /></p>
<p><strong>2.1 RNN Loss and Perplexity</strong> </p>
<p>RNN 的损失函数一般是交叉熵误差。</p>
<div>
<div class="MathJax_Preview">
J^{(t)}(\theta)=\sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</div>
<script type="math/tex; mode=display">
J^{(t)}(\theta)=\sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</script>
</div>
<p>在大小为 T 的语料库上的交叉熵误差的计算如下：</p>
<div>
<div class="MathJax_Preview">
J=-\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</div>
<script type="math/tex; mode=display">
J=-\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</script>
</div>
<p><strong>2.2 Advantages, Disadvantages and Applications of RNNs</strong></p>
<p>RNN 有以下优点：</p>
<ol>
<li>它可以处理任意长度的序列</li>
<li>对更长的输入序列不会增加模型的参数大小</li>
<li>对时间步 t 的计算理论上可以利用前面很多时间步的信息</li>
<li>对输入的每个时间步都应用相同的权重，因此在处理输入时具有对称性</li>
</ol>
<p>但是 RNN 也有以下缺点：</p>
<ol>
<li>计算速度很慢——因为它每一个时间步需要依赖上一个时间步，所以不能并行化</li>
<li>在实际中因为梯度消失和梯度爆炸，很难利用到前面时间步的信息。</li>
</ol>
<p>运行一层 RNN 所需的内存量与语料库中的单词数成正比。例如，我们把一个句子是为一个 mini batch，那么一个有 k 个单词的句子在内存中就会占用 k 个词向量的存储空间。同时，RNN 必须维持两对 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 和 <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 矩阵。然而 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 的可能是非常大的，它的大小不会随着语料库的大小而变化（与传统的语言模型不一样）。对于具有 1000 个循环层的 RNN，矩阵 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 的大小为 <span><span class="MathJax_Preview">1000 \times 1000</span><script type="math/tex">1000 \times 1000</script></span> 而与语料库大小无关。</p>
<p>RNN 可以应用在很多任务，例如标注任务（词性标注、命名实体识别），句子分类（情感分类），编码模块（问答任务，机器翻译和其他很多任务）。在后面的两个任务，我们希望得到对句子的表示，这时可以通过采用该句子中时间步长的所有隐藏状态的 <span><span class="MathJax_Preview">element-wise</span><script type="math/tex">element-wise</script></span> 的最大值或平均值来获得。</p>
<p>下图是一些出版物中对 RNN 模型的另外一种表示。它将 RNN 的每个隐层用一个环来表示。</p>
<p><img alt="1560999097493" src="../imgs/1560999097493.png" /></p>
<p><strong>2.3 Vanishing Gradient &amp; Gradient Explosion Problems</strong></p>
<p>RNN 从一个时间步传播权值矩阵到下一个时间步。回想一下，RNN 实现的目标是通过长距离的时间步来传播上下文信息。例如，考虑以下两个句子：</p>
<p><img alt="1561000437148" src="../imgs/1561000437148.png" /></p>
<p>对上面的两个句子，根据上下文，都可以知道空白处的答案是“John”,第二个在两个句子的上下文中均提及了好几次的人。迄今为止我们对 RNN 的了解，在理想情况下，RNN 也是能够计算得到正确的答案。然而，在实际中，RNN 预测句子中的空白处答案正确可能性，第一句要比第二句高。这是因为在反向传播的阶段的过程中，从前面时间步中回传过来的梯度值会逐渐消失。因此，对于长句子，预测到“John”是空白处的答案的概率会随着上下文信息增大而减少。下面，我们讨论梯度消失问题背后的数学原因。</p>
<p>考虑公式在时间步 t ，计算 RNN 误差 <span><span class="MathJax_Preview">\frac{dE}{dW}</span><script type="math/tex">\frac{dE}{dW}</script></span> ，然后我们把每个时间步的误差都加起来。也就是说，计算并累积每个时间步长 t 的  <span><span class="MathJax_Preview"><span><span class="MathJax_Preview">\frac{dE_t}{dW}</span><script type="math/tex">\frac{dE_t}{dW}</script></span></span><script type="math/tex"><span><span class="MathJax_Preview">\frac{dE_t}{dW}</span><script type="math/tex">\frac{dE_t}{dW}</script></span></script></span> 。</p>
<div>
<div class="MathJax_Preview">
\frac{\partial E}{\partial W}=\sum_{i=1}^{T} \frac{\partial E_{t}}{\partial W}
</div>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial W}=\sum_{i=1}^{T} \frac{\partial E_{t}}{\partial W}
</script>
</div>
<p>通过将微分链式法则应用于公式 (6) 和 (5) 来计算每个时间步长的误差；公式 (11) 展示对应的微分计算。注意 <span><span class="MathJax_Preview">\frac{d h_{t}}{d h_{k}}</span><script type="math/tex">\frac{d h_{t}}{d h_{k}}</script></span> 是 <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 对之前所有的 k 个时间步的偏导数。</p>
<div>
<div class="MathJax_Preview">
\frac{\partial E_{t}}{\partial W}=\sum_{k=1}^{T} \frac{\partial E_{t}}{\partial y_{t}} \frac{\partial y_{t}}{\partial h_{t}} \frac{\partial h_{t}}{\partial h_{k}} \frac{\partial h_{k}}{\partial W}
</div>
<script type="math/tex; mode=display">
\frac{\partial E_{t}}{\partial W}=\sum_{k=1}^{T} \frac{\partial E_{t}}{\partial y_{t}} \frac{\partial y_{t}}{\partial h_{t}} \frac{\partial h_{t}}{\partial h_{k}} \frac{\partial h_{k}}{\partial W}
</script>
</div>
<p>下式 展示了计算每个 <span><span class="MathJax_Preview">\frac{d h_{t}}{d h_{k}}</span><script type="math/tex">\frac{d h_{t}}{d h_{k}}</script></span> 的关系；这是在时间间隔 <span><span class="MathJax_Preview">[k,t]</span><script type="math/tex">[k,t]</script></span> 内对所有的隐藏层的应用一个简单的微分链式法则。</p>
<div>
<div class="MathJax_Preview">
\frac{\partial h_{t}}{\partial h_{k}}=\prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}}=\prod_{j=k+1}^{t} W^{T} \times \operatorname{diag}\left[f^{\prime}\left(j_{j-1}\right)\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial h_{t}}{\partial h_{k}}=\prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}}=\prod_{j=k+1}^{t} W^{T} \times \operatorname{diag}\left[f^{\prime}\left(j_{j-1}\right)\right]
</script>
</div>
<p>因为 <span><span class="MathJax_Preview">h \in \mathbb{R}^{D_{n}}</span><script type="math/tex">h \in \mathbb{R}^{D_{n}}</script></span> ，每个 <span><span class="MathJax_Preview">\frac{\partial h_{j}}{\partial h_{j-1}}</span><script type="math/tex">\frac{\partial h_{j}}{\partial h_{j-1}}</script></span> 是 h 的 Jacobian 矩阵的元素：</p>
<div>
<div class="MathJax_Preview">
\frac{\partial h_{j}}{\partial h_{j-1}}=\left[\frac{\partial h_{j}}{\partial h_{j-1,1}} \cdots \frac{\partial h_{j}}{\partial h_{j-1, D_{n}}}\right]=\left[\begin{array}{ccc}{\frac{\partial h_{j, 1}}{\partial h_{j-1,1}}} &amp; {\cdots} &amp; {\frac{\partial h_{j,1}}{\partial h_{j-1, D_{n}}}} \\ {\vdots} &amp; {\ddots} &amp; {\vdots} \\ {\frac{\partial h_{j, D_{n}}}{\partial h_{j - 1,1}}} &amp; {\cdots} &amp; {\frac{\partial h_{j, D_{n}}}{\partial h_{j-1, D_{n}}}}\end{array}\right]
</div>
<script type="math/tex; mode=display">
\frac{\partial h_{j}}{\partial h_{j-1}}=\left[\frac{\partial h_{j}}{\partial h_{j-1,1}} \cdots \frac{\partial h_{j}}{\partial h_{j-1, D_{n}}}\right]=\left[\begin{array}{ccc}{\frac{\partial h_{j, 1}}{\partial h_{j-1,1}}} & {\cdots} & {\frac{\partial h_{j,1}}{\partial h_{j-1, D_{n}}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial h_{j, D_{n}}}{\partial h_{j - 1,1}}} & {\cdots} & {\frac{\partial h_{j, D_{n}}}{\partial h_{j-1, D_{n}}}}\end{array}\right]
</script>
</div>
<p>将公式合起来，我们有以下关系。</p>
<div>
<div class="MathJax_Preview">
\frac{\partial E}{\partial W}=\sum_{t=1}^{T} \sum_{k=1}^{t} \frac{\partial E_{t}}{\partial y_{t}} \frac{\partial y_{t}}{\partial h_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}}\right) \frac{\partial h_{k}}{\partial W}
</div>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial W}=\sum_{t=1}^{T} \sum_{k=1}^{t} \frac{\partial E_{t}}{\partial y_{t}} \frac{\partial y_{t}}{\partial h_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}}\right) \frac{\partial h_{k}}{\partial W}
</script>
</div>
<p>下式展示了 Jacobian 矩阵的范数。这里的 <span><span class="MathJax_Preview">\beta_{W}</span><script type="math/tex">\beta_{W}</script></span> 和 <span><span class="MathJax_Preview">\beta_{h}</span><script type="math/tex">\beta_{h}</script></span> 是这两个矩阵范数的上界值。因此通过公式所示的关系计算在每个时间步 t 的部分梯度范数。</p>
<div>
<div class="MathJax_Preview">
\left\|\frac{\partial h_{j}}{\partial h_{j-1}}\right\| \leq\left\|W^{T}\right\|\left\|\operatorname{diag}\left[f^{\prime}\left(h_{j-1}\right)\right]\right\| \leq \beta_{W} \beta_{h}
</div>
<script type="math/tex; mode=display">
\left\|\frac{\partial h_{j}}{\partial h_{j-1}}\right\| \leq\left\|W^{T}\right\|\left\|\operatorname{diag}\left[f^{\prime}\left(h_{j-1}\right)\right]\right\| \leq \beta_{W} \beta_{h}
</script>
</div>
<p>计算这两个矩阵的 L2 范数。在给定的非线性函数 sigmoid 下， <span><span class="MathJax_Preview">f^{\prime}\left(h_{j-1}\right)</span><script type="math/tex">f^{\prime}\left(h_{j-1}\right)</script></span> 的范数只能等于 1。</p>
<div>
<div class="MathJax_Preview">
\left\|\frac{\partial h_{t}}{\partial h_{k}}\right\|=\left\|\prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}}\right\| \leq\left(\beta_{W} \beta_{h}\right)^{t-k}
</div>
<script type="math/tex; mode=display">
\left\|\frac{\partial h_{t}}{\partial h_{k}}\right\|=\left\|\prod_{j=k+1}^{t} \frac{\partial h_{j}}{\partial h_{j-1}}\right\| \leq\left(\beta_{W} \beta_{h}\right)^{t-k}
</script>
</div>
<p>当 <span><span class="MathJax_Preview">t - k</span><script type="math/tex">t - k</script></span> 足够大和 <span><span class="MathJax_Preview">\beta_{W} \beta_{h}</span><script type="math/tex">\beta_{W} \beta_{h}</script></span> 远远小于 1 或者远远大于 1，指数项 <span><span class="MathJax_Preview">\left(\beta_{W} \beta_{h}\right)^{t-k}</span><script type="math/tex">\left(\beta_{W} \beta_{h}\right)^{t-k}</script></span> 的值就很容易变得非常小或者非常大。由于单词之间的距离过大，用一个很大的 <span><span class="MathJax_Preview">t-k</span><script type="math/tex">t-k</script></span> 评估交叉熵误差可能会出现问题。在反向传播的早期就出现梯度消失，那么远处单词对在时间步长 t 预测下一个单词中，所起到的作用就会变得很小。</p>
<p>在实验的过程中，一旦梯度的值变得非常大，会导致在运行过程中容易检测到其引起的溢出（即 NaN）；这样的问题称为梯度爆炸问题。然而，当梯度接近为 0 的时候，梯度近乎不再存在，同时降低模型对语料库中的远距离的单词的学习质量；这样的问题称为梯度消失问题。如果相对梯度消失问题的有更直观的了解，你可以访问这个<a href="https://link.zhihu.com/?target=http%3A//cs224d.stanford.edu/notebooks/vanishing_grad_example.html">样例网站</a>。</p>
<p><strong>2.4 Solution to the Exploding &amp; Vanishing Gradients</strong></p>
<p>现在我们知道了梯度消失问题的本质以及它在深度神经网络中如何表现出来，让我们使用一些简单实用的启发式方法来解决这些问题。</p>
<p>为了解决梯度爆炸的问题，Thomas Mikolov 等人首先提出了一个简单的启发式解决方案，每当梯度大于一个阈值的时候，将其截断为一个很小的值，具体如下面算法中的伪代码所示。</p>
<p><img alt="1561002299391" src="../imgs/1561002299391.png" /></p>
<p>下图可视化了梯度截断的效果。它展示了一个权值矩阵为 <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 和偏置项为 <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 的很小的 RNN 神经网络的决策界面。该模型由一个单一单元的循环神经网络组成，在少量的时间步长上运行；实心箭头阐述了在每个梯度下降步骤的训练过程。当在梯度下降的过程中，模型碰到目标函数中的高误差壁时，梯度被推到决策面上的一个遥远的位置。截断模型生成了虚线，在那里它将误差梯度拉回到靠近原始梯度的地方。</p>
<p><img alt="1561002370070" src="../imgs/1561002370070.png" /></p>
<p>为了解决梯度消失问题，我们提出两个技术。第一个技术是不去随机初始化 <span><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> ，而是初始化为单位矩阵。</p>
<p>第二个技术是使用 Rectified Linear（ReLU）单元代替 sigmoid 函数。ReLU 的导数是 0 或者 1。这样梯度传回神经元的导数是 1，而不会在反向传播了一定的时间步后梯度变小。</p>
<p><strong>2.5 Deep Bidirectional RNNs</strong></p>
<p>到目前为止，我们已经讨论了用 RNN 如何使用过去的词来预测序列中的下一个单词。同理，可以通过令 RNN 模型向反向读取语料库，根据未来单词进行预测。Irsoy 等人展示了一个双向深度神经网络；在每个时间步 t，这个网络维持两个隐藏层，一个是从左到右传播而另外一个是从右到左传播。为了在任何时候维持两个隐藏层，该网络要消耗的两倍存储空间来存储权值和偏置参数。最后的分类结果 <span><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span> ，是结合由两个 RNN 隐藏层生成的结果得分产生。下图展示了双向 RNN 的网络结构。</p>
<p><img alt="1561002748070" src="../imgs/1561002748070.png" /></p>
<p>而下式展示了给出了建立双向RNN隐层的数学公式。两个公式之间唯一的区别是递归读取语料库的方向不同。最后一行展示了通过总结过去和将来的单词表示，显示用于预测下一个单词的分类关系。</p>
<div>
<div class="MathJax_Preview">
\overrightarrow{h}_{t}=f\left(\overrightarrow{W} x_{t}+\overrightarrow{V} \overrightarrow{h}_{t-1}+\overrightarrow{b}\right) \\
\overleftarrow{h}_{t}=f\left(\overleftarrow{W} x_{t}+\overleftarrow{V} \overleftarrow{h}_{t-1}+\overleftarrow{b}\right) \\
\hat{y}_{t}=g\left(U h_{t}+c\right)=g\left(U\left[\overrightarrow{h}_{t} ; \overleftarrow{h}_{t}\right]+c\right)
</div>
<script type="math/tex; mode=display">
\overrightarrow{h}_{t}=f\left(\overrightarrow{W} x_{t}+\overrightarrow{V} \overrightarrow{h}_{t-1}+\overrightarrow{b}\right) \\
\overleftarrow{h}_{t}=f\left(\overleftarrow{W} x_{t}+\overleftarrow{V} \overleftarrow{h}_{t-1}+\overleftarrow{b}\right) \\
\hat{y}_{t}=g\left(U h_{t}+c\right)=g\left(U\left[\overrightarrow{h}_{t} ; \overleftarrow{h}_{t}\right]+c\right)
</script>
</div>
<p>RNN也可以是多层的。下图展示一个多层的双向 RNN，其中下面的隐藏层传播到下一层。如图所示，在该网络架构中，在时间步 t，每个中间神经元从前一个时间步（在相同的 RNN 层）接收一组参数和前一个 RNN 隐藏层的两组参数；这两组参数一组是从左到右的 RNN 输入，另外一组是从右到左的 RNN 输入。</p>
<p><img alt="1561002850473" src="../imgs/1561002850473.png" /></p>
<p>为了构建一个 L 层的深度 RNN，上述的关系要修改为在公式中的关系，其中在第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 层的每个中间神经元的输入是在相同时间步 t 的 RNN 第 <span><span class="MathJax_Preview">i-1</span><script type="math/tex">i-1</script></span> 层的输出。最后的输出 <span><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span> ，每个时间步都是输入参数通过所有隐层传播的结果。</p>
<div>
<div class="MathJax_Preview">
\overrightarrow{h}_{t}^{(i)}=f\left(\overrightarrow{W}^{(i)} h_{t}+\overrightarrow{V} \overrightarrow{h}_{t-1}+\overrightarrow{b}\right) \\
\overleftarrow{h}_{t}=f\left(\overleftarrow{W} x_{t}+\overleftarrow{V} \overleftarrow{h}_{t-1}+\overleftarrow{b}\right) \\
\hat{y}_{t}=g\left(U h_{t}+c\right)=g\left(U\left[\overrightarrow{h}_{t} ; \overleftarrow{h}_{t}\right]+c\right)
</div>
<script type="math/tex; mode=display">
\overrightarrow{h}_{t}^{(i)}=f\left(\overrightarrow{W}^{(i)} h_{t}+\overrightarrow{V} \overrightarrow{h}_{t-1}+\overrightarrow{b}\right) \\
\overleftarrow{h}_{t}=f\left(\overleftarrow{W} x_{t}+\overleftarrow{V} \overleftarrow{h}_{t-1}+\overleftarrow{b}\right) \\
\hat{y}_{t}=g\left(U h_{t}+c\right)=g\left(U\left[\overrightarrow{h}_{t} ; \overleftarrow{h}_{t}\right]+c\right)
</script>
</div>
<p><strong>2.6 Application: RNN Translation Model</strong></p>
<p>传统的翻译模型是非常复杂的：它们包含很多应用在语言翻译流程的不同阶段的机器学习算法。在本节中，我们讨论采用 RNN 作为传统翻译模型的替代方法的潜力。考虑下图中展示的 RNN 模型；其中德语短语 Echt dicke Kiste 翻译为 Awesome sauce。</p>
<p><img alt="1561003009898" src="../imgs/1561003009898.png" /></p>
<p>首先，前三个时间步的隐藏层 编码 德语单词为一些语言的单词特征（ <span><span class="MathJax_Preview">h_3</span><script type="math/tex">h_3</script></span> ）。后面两个时间步解码 <span><span class="MathJax_Preview">h_3</span><script type="math/tex">h_3</script></span> 为英语单词输出。下式分别展示了编码阶段和解码阶段(后两行)。</p>
<div>
<div class="MathJax_Preview">
\begin{array}{c}{h_{t}=\phi\left(h_{t-1}, x_{t}\right)=f\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{t}\right)} \\ {h_{t}=\phi\left(h_{t-1}\right)=f\left(W^{(h h)} h_{t-1}\right)} \\ {y_{t}=\operatorname{softmax}\left(W^{(s)} h_{t}\right)}\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{c}{h_{t}=\phi\left(h_{t-1}, x_{t}\right)=f\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{t}\right)} \\ {h_{t}=\phi\left(h_{t-1}\right)=f\left(W^{(h h)} h_{t-1}\right)} \\ {y_{t}=\operatorname{softmax}\left(W^{(s)} h_{t}\right)}\end{array}
</script>
</div>
<p>一般可以认为使用交叉熵函数的 RNN 模型可以生成高精度的翻译结果。在实际中，在模型中增加一些扩展方法可以提升翻译的准确度表现。</p>
<div>
<div class="MathJax_Preview">
\max _{\theta} \frac{1}{N} \sum_{n=1}^{N} \log p_{\theta}\left(y^{(n)} | x^{(n)}\right)
</div>
<script type="math/tex; mode=display">
\max _{\theta} \frac{1}{N} \sum_{n=1}^{N} \log p_{\theta}\left(y^{(n)} | x^{(n)}\right)
</script>
</div>
<p><strong>扩展 1</strong>：在训练 RNN 的编码和解码阶段时，使用不同的权值。这使两个单元解耦，让两个 RNN 模块中的每一个进行更精确的预测。这意味着在公式 (23) 和 (24) 在 <span><span class="MathJax_Preview">\phi( )</span><script type="math/tex">\phi( )</script></span> 函数中是使用不同的 <span><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> 矩阵。</p>
<p><img alt="1561008573530" src="../imgs/1561008573530.png" /></p>
<p><strong>扩展 2</strong>：使用三个不同的输入计算解码器中的每个隐藏状态</p>
<ul>
<li>前一个隐藏状态 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span>（标准的）</li>
<li>编码阶段的最后一个隐藏层（上图中的 <span><span class="MathJax_Preview">c=h_T</span><script type="math/tex">c=h_T</script></span>）</li>
<li>前一个预测的输出单词 <span><span class="MathJax_Preview">\hat y_{t-1}</span><script type="math/tex">\hat y_{t-1}</script></span></li>
</ul>
<p>将上述的三个输入结合将之前公式的解码函数中的 <span><span class="MathJax_Preview">\phi</span><script type="math/tex">\phi</script></span> 函数转换为下式的 <span><span class="MathJax_Preview">\phi</span><script type="math/tex">\phi</script></span> 函数。上图展示了这个模型。</p>
<div>
<div class="MathJax_Preview">
h_{t}=\phi\left(h_{t-1}, c, y_{t-1}\right)
</div>
<script type="math/tex; mode=display">
h_{t}=\phi\left(h_{t-1}, c, y_{t-1}\right)
</script>
</div>
<p><strong>扩展 3</strong>：使用多个 RNN 层来训练深度循环神经网络。神经网络的层越深，模型的就具有更强的学习能力从而能提升预测的准确度。当然，这也意味着需要使用大规模的语料库来训练这个模型。</p>
<p><strong>扩展 4</strong>：训练双向编码器，提高准确度。</p>
<p><strong>扩展 5</strong>：给定一个德语词序列 A B C，它的英语翻译是 X Y。在训练 <span><span class="MathJax_Preview">RNN</span><script type="math/tex">RNN</script></span> 时不使用 A B C <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> X Y，而是使用 C B A <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> X Y。这么处理的原因是 A 更有可能被翻译成 X。因此对前面讨论的梯度消失问题，<strong>反转输入句子的顺序有助于降低输出短语的错误率</strong>。</p>
<h3 id="3-gated-recurrent-units">3 Gated Recurrent Units<a class="headerlink" href="#3-gated-recurrent-units" title="Permanent link">&para;</a></h3>
<p>除了迄今为止讨论的扩展方法之外，我们已经发现 RNN 通过使用更复杂的激活单元来获得表现更好。到目前为止，我们已经讨论了从隐藏状态 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 向 <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 转换的方法，使用了一个仿射转换和 <span><span class="MathJax_Preview">point-wise</span><script type="math/tex">point-wise</script></span> 的非线性转换。在这里，我们讨论门激活函数的使用并修改 RNN 结构。虽然理论上 RNN 能捕获长距离信息，但实际上很难训练网络做到这一点。门控制单元可以让 RNN 具有更多的持久性内存，从而更容易捕获长距离信息。让我们从数学角度上讨论 GRU 如何使用 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 和  <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 来生成下一个隐藏状态 <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 。然后我们将深入了解 GRU 架构。</p>
<div>
<div class="MathJax_Preview">
\begin{array}{rlr}{z_{t}} &amp; {=\sigma\left(W^{(z)} x_{t}+U^{(z)} h_{t-1}\right)} &amp; {(\text {Update gate})} \\ {r_{t}} &amp; {=\sigma\left(W^{(r)} x_{t}+U^{(r)} h_{t-1}\right)} &amp; {\text { (Reset gate) }} \\ {\tilde{h}_{t}} &amp; {=\tanh \left(r_{t} \circ U h_{t-1}+W x_{t}\right)} &amp; {\text { (New memory) }} \\ {h_{t}} &amp; {=\left(1-z_{t}\right) \circ \tilde{h}_{t}+z_{t} \circ h_{t-1}} &amp; {\text { (Hidden state) }}\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{rlr}{z_{t}} & {=\sigma\left(W^{(z)} x_{t}+U^{(z)} h_{t-1}\right)} & {(\text {Update gate})} \\ {r_{t}} & {=\sigma\left(W^{(r)} x_{t}+U^{(r)} h_{t-1}\right)} & {\text { (Reset gate) }} \\ {\tilde{h}_{t}} & {=\tanh \left(r_{t} \circ U h_{t-1}+W x_{t}\right)} & {\text { (New memory) }} \\ {h_{t}} & {=\left(1-z_{t}\right) \circ \tilde{h}_{t}+z_{t} \circ h_{t-1}} & {\text { (Hidden state) }}\end{array}
</script>
</div>
<p>上述的共识可以认为是 GRU 的四个基本操作阶段，下面对这些公式作出更直观的解释，下图展示了 GRU 的基本结构和计算流程：</p>
<p><img alt="1561009471646" src="../imgs/1561009471646.png" /></p>
<ol>
<li><strong>New memory generation</strong>：一个新的记忆 <span><span class="MathJax_Preview">\tilde{h}_{t}</span><script type="math/tex">\tilde{h}_{t}</script></span> 是由一个新的输入单词 <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 和过去的隐藏状态  <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 共同计算所得。这个阶段是将新输入的单词与过去的隐藏状态 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 相结合，根据过去的上下文来总结得到向量 <span><span class="MathJax_Preview">\tilde{h}_{t}</span><script type="math/tex">\tilde{h}_{t}</script></span> 。</li>
<li><strong>Reset Gate</strong>：复位信号 <span><span class="MathJax_Preview">r_t</span><script type="math/tex">r_t</script></span> 是负责确定 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 对总结 <span><span class="MathJax_Preview">\tilde{h}_{t}</span><script type="math/tex">\tilde{h}_{t}</script></span> 的重要程度。如果确定 <span><span class="MathJax_Preview">\tilde{h}_{t}</span><script type="math/tex">\tilde{h}_{t}</script></span> 与新的记忆的计算无关，则复位门能够完全消除过去的隐藏状态（即忽略之前隐藏的信息）。</li>
<li><strong>Update Gate</strong>：更新信号 <span><span class="MathJax_Preview">z_t</span><script type="math/tex">z_t</script></span> 负责确定有多少 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 可以向前传递到下一个状态。例如，如果  <span><span class="MathJax_Preview">z_{t} \approx 1</span><script type="math/tex">z_{t} \approx 1</script></span> ，然后 <span><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 几乎是完全向前传递到下一个隐藏状态。反过来，如果 <span><span class="MathJax_Preview">z_{t} \approx 0</span><script type="math/tex">z_{t} \approx 0</script></span> ，然后大部分的新的记忆 <span><span class="MathJax_Preview">\tilde{h}_{t}</span><script type="math/tex">\tilde{h}_{t}</script></span> 向前传递到下一个隐藏状态。</li>
<li><strong>Hidden state</strong>：利用更新门的建议，使用过去的隐藏输入 <span><span class="MathJax_Preview">{h}_{t-1}</span><script type="math/tex">{h}_{t-1}</script></span> 和新生成的记忆 <span><span class="MathJax_Preview">\tilde{h}_{t}</span><script type="math/tex">\tilde{h}_{t}</script></span> 生成隐藏状态  <span><span class="MathJax_Preview">{h}_{t}</span><script type="math/tex">{h}_{t}</script></span> 。</li>
</ol>
<p>需要注意的是，为了训练 GRU，我们需要学习所有不同的参数：<span><span class="MathJax_Preview">W, U, W^{(r)}, U^{(r)}, W^{(z)}, U^{(z)}</span><script type="math/tex">W, U, W^{(r)}, U^{(r)}, W^{(z)}, U^{(z)}</script></span> 。这些参数同样是通过反向传播算法学习所得。</p>
<h3 id="4-long-short-term-memories">4 Long-Short-Term-Memories<a class="headerlink" href="#4-long-short-term-memories" title="Permanent link">&para;</a></h3>
<p>Long-Short-Term-Memories 是和 GRU 有一点不同的另外一种类型的复杂激活神经元。它的作用与 GRU 类似，但是神经元的结构有一点区别。我们首先来看看 LSTM 神经元的数学公式，然后再深入了解这个神经元的设计架构：</p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
{i_{t}} &amp; {=\sigma\left(W^{(i)} x_{t}+U^{(i)} h_{t-1}\right)} &amp; {\text { (Input gate) }} \\
{f_{t}} &amp; {=\sigma\left(W^{(f)} x_{t}+U^{(f)} h_{t-1}\right)} &amp; {\text { (Forget gate) }} \\ 
{o_{t}} &amp; {=\sigma\left(W^{(o)} x_{t}+U^{(o)} h_{t-1}\right)} &amp; {\text { (Output/Exposure gate) }} \\
{\tilde{c}_{t}} &amp; {=\tanh \left(W^{(c)} x_{t}+U^{(c)} h_{t-1}\right)} &amp; {\text { (New memory cell) }} \\ 
{c_{t}} &amp; {=f_{t} \circ c_{t-1}+i_{t} \circ \tilde{c}_{t}} &amp; {\text { (Final memory cell) }} \\ 
{h_{t}} &amp; {=o_{t} \circ \tanh \left(c_{t}\right)}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
{i_{t}} & {=\sigma\left(W^{(i)} x_{t}+U^{(i)} h_{t-1}\right)} & {\text { (Input gate) }} \\
{f_{t}} & {=\sigma\left(W^{(f)} x_{t}+U^{(f)} h_{t-1}\right)} & {\text { (Forget gate) }} \\ 
{o_{t}} & {=\sigma\left(W^{(o)} x_{t}+U^{(o)} h_{t-1}\right)} & {\text { (Output/Exposure gate) }} \\
{\tilde{c}_{t}} & {=\tanh \left(W^{(c)} x_{t}+U^{(c)} h_{t-1}\right)} & {\text { (New memory cell) }} \\ 
{c_{t}} & {=f_{t} \circ c_{t-1}+i_{t} \circ \tilde{c}_{t}} & {\text { (Final memory cell) }} \\ 
{h_{t}} & {=o_{t} \circ \tanh \left(c_{t}\right)}
\end{aligned}
</script>
</div>
<p>下图是LSTM的计算图示</p>
<p><img alt="1561010965268" src="../imgs/1561010965268.png" /></p>
<p>我们可以通过以下步骤了解 LSTM 的架构以及这个架构背后的意义：</p>
<ol>
<li><strong>New memory generation</strong>：这个阶段是类似于 GRU 生成新的记忆的阶段。我们基本上是用输入单词 <span><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 和过去的隐藏状态来生成一个包括新单词 <span><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> 的新的记忆 <span><span class="MathJax_Preview">\tilde{c}_{t}</span><script type="math/tex">\tilde{c}_{t}</script></span> 。</li>
<li><strong>Input Gate</strong>：我们看到在生成新的记忆之前，新的记忆的生成阶段不会检查新单词是否重要——这需要输入门函数来做这个判断。输入门使用输入词和过去的隐藏状态来决定输入值是否值得保存，从而用来进入新内存。因此，它产生它作为这个信息的指示器。</li>
<li><strong>Forget Gate</strong>：这个门与输入门类似，只是它不确定输入单词的有用性——而是评估过去的记忆是否对当前记忆的计算有用。因此，遗忘门查看输入单词和过去的隐藏状态，并生成 <span><span class="MathJax_Preview">f_t</span><script type="math/tex">f_t</script></span>。</li>
<li><strong>Final memory generation</strong>：这个阶段首先根据忘记门 <span><span class="MathJax_Preview">f_t</span><script type="math/tex">f_t</script></span> 的判断，相应地忘记过去的记忆  <span><span class="MathJax_Preview">c_{t-1}</span><script type="math/tex">c_{t-1}</script></span> 。类似地，根据输入门  <span><span class="MathJax_Preview">i_t</span><script type="math/tex">i_t</script></span> 的判断，相应地输入新的记忆 <span><span class="MathJax_Preview">\tilde c_t</span><script type="math/tex">\tilde c_t</script></span> 。然后将上面的两个结果相加生成最终的记忆 <span><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 。</li>
<li><strong>Output/Exposure Gate</strong>：这是 GRU 中没有明确存在的门。这个门的目的是从隐藏状态中分离最终的记忆。最终的记忆 <span><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 包含很多不需要存储在隐藏状态的信息。隐藏状态用于 LSTM 的每个单个门，因此，该门是要评估关于记忆单元 <span><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 的哪些部分需要显露在隐藏状态 <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 中。用于评估的信号是 <span><span class="MathJax_Preview">o_t</span><script type="math/tex">o_t</script></span> ，然后与 <span><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 通过 <span><span class="MathJax_Preview">o_{t} \circ \tanh \left(c_{t}\right)</span><script type="math/tex">o_{t} \circ \tanh \left(c_{t}\right)</script></span> 运算得到最终的 <span><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 。</li>
</ol>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<p>以下是学习本课程时的可用参考书籍：</p>
<p><a href="https://item.jd.com/12355569.html">《基于深度学习的自然语言处理》</a> （车万翔老师等翻译）</p>
<p><a href="https://nndl.github.io/">《神经网络与深度学习》</a></p>
<p>以下是整理笔记的过程中参考的博客：</p>
<p><a href="https://zhuanlan.zhihu.com/p/59011576">斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p>
<p><a href="https://zhuanlan.zhihu.com/p/31977759">斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> <span class="critic comment">这是针对note部分的翻译</span></p>
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>var disqus_config=function(){this.page.url="None",this.page.identifier="None"};window.addEventListener("load",function(){var e=document,i=e.createElement("script");i.src="//https-looperxx-github-io-my-wiki.disqus.com/embed.js",i.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(i)})</script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                05 Linguistic Structure Dependency Parsing
              </div>
            </div>
          </a>
        
        
          <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  下一页
                </span>
                07 Vanishing Gradients and Fancy RNNs
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 - 2020 Xiao Xu
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://github.com/looperXX" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.77e55a48.min.js"></script>
      <script src="../assets/javascripts/bundle.9554a270.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ['navigation.tabs', 'header.autohide'],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.4ac00218.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
    
  </body>
</html>