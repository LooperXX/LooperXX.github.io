<!DOCTYPE html><html class=no-js lang=zh> <head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="Xiao Xu - Homepage" name=description><meta content="Xiao Xu" name=author><link href=https://looperxx.github.io/CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/ rel=canonical><link href=../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/ rel=prev><link href=../CS224n-2019-11-ConvNets%20for%20NLP/ rel=next><link href=../assets/images/favicon.png rel=icon><meta content="mkdocs-1.4.2, mkdocs-material-9.1.2" name=generator><title>10 Question Answering and the Default Final Project - The Sun Also Rises.</title><link href=../assets/stylesheets/main.7bf56d0a.min.css rel=stylesheet><link href=../assets/stylesheets/palette.a0c5b2b5.min.css rel=stylesheet><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback" rel=stylesheet><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-CV5JZHXZY8"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-CV5JZHXZY8",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-CV5JZHXZY8",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            
                .gscrollbar-fixer { padding-right: 15px; }
                .gdesc-inner { font-size: 0.75rem; }
                body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
                body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
                body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
                </style><script src=../assets/javascripts/glightbox.min.js></script></head> <body data-md-color-accent=indigo data-md-color-primary=indigo data-md-color-scheme=default dir=ltr> <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script> <input autocomplete=off class=md-toggle data-md-toggle=drawer id=__drawer type=checkbox> <input autocomplete=off class=md-toggle data-md-toggle=search id=__search type=checkbox> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a class=md-skip href=#lecture-10-question-answering-and-the-default-final-project> 跳转至 </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav aria-label=页眉 class="md-header__inner md-grid"> <a aria-label="The Sun Also Rises." class="md-header__button md-logo" data-md-component=logo href=.. title="The Sun Also Rises."> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> The Sun Also Rises. </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 10 Question Answering and the Default Final Project </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input aria-label="Switch to dark mode" class=md-option data-md-color-accent=indigo data-md-color-media data-md-color-primary=indigo data-md-color-scheme=default id=__palette_1 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_2 hidden title="Switch to dark mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg> </label> <input aria-label="Switch to light mode" class=md-option data-md-color-accent=indigo data-md-color-media data-md-color-primary=indigo data-md-color-scheme=slate id=__palette_2 name=__palette type=radio> <label class="md-header__button md-icon" for=__palette_1 hidden title="Switch to light mode"> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"></path></svg> </label> </form> <label class="md-header__button md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input aria-label=搜索 autocapitalize=off autocomplete=off autocorrect=off class=md-search__input data-md-component=search-query name=query placeholder=搜索 required spellcheck=false type=text> <label class="md-search__icon md-icon" for=__search> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"></path></svg> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"></path></svg> </label> <nav aria-label=查找 class=md-search__options> <a aria-label=分享 class="md-search__icon md-icon" data-clipboard data-clipboard-text data-md-component=search-share href=javascript:void(0) tabindex=-1 title=分享> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"></path></svg> </a> <button aria-label=清空当前内容 class="md-search__icon md-icon" tabindex=-1 title=清空当前内容 type=reset> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> 正在初始化搜索引擎 </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a class=md-source data-md-component=source href=https://github.com/LooperXX/LooperXX.github.io title=前往仓库> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg> </div> <div class=md-source__repository> LooperXX/LooperXX.github.io </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav aria-label=标签 class=md-tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a class=md-tabs__link href=..> Xiao Xu @ HIT-SCIR </a> </li> <li class=md-tabs__item> <a href=../blog/BridgeTower/ class=md-tabs__link> Blogs </a> </li> <li class=md-tabs__item> <a href=../Normalization/ class=md-tabs__link> Notes </a> </li> <li class=md-tabs__item> <a href=../CS224n-2019-00-Info/ class="md-tabs__link md-tabs__link--active"> Notes on CS224n-2019 </a> </li> <li class=md-tabs__item> <a href=../MkDocs_demo/ class=md-tabs__link> Notes on MkDocs </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=导航栏 class="md-nav md-nav--primary md-nav--lifted" data-md-level=0> <label class=md-nav__title for=__drawer> <a aria-label="The Sun Also Rises." class="md-nav__button md-logo" data-md-component=logo href=.. title="The Sun Also Rises."> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"></path></svg> </a> The Sun Also Rises. </label> <div class=md-nav__source> <a class=md-source data-md-component=source href=https://github.com/LooperXX/LooperXX.github.io title=前往仓库> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"></path></svg> </div> <div class=md-source__repository> LooperXX/LooperXX.github.io </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=..> Xiao Xu @ HIT-SCIR </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_2 type=checkbox> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> Blogs <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_2_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blogs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../blog/BridgeTower/ class=md-nav__link> AAAI 2023 (Oral) | BridgeTower: 在视觉语言表示学习中建立编码器间的桥梁 </a> </li> <li class=md-nav__item> <a href=../blog/Profile%20SLU/ class=md-nav__link> AAAI 2022 (Oral) | Profile SLU: 基于Profile信息的口语语言理解基准 </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_3 type=checkbox> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> Notes <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_3_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../Normalization/ class=md-nav__link> Normalization </a> </li> <li class=md-nav__item> <a href=../Transfer%20Learning/ class=md-nav__link> Transfer Learning </a> </li> <li class=md-nav__item> <a href=../Attention/ class=md-nav__link> Attention </a> </li> <li class=md-nav__item> <a href=../Neural%20Reading%20Comprehension%20and%20beyond/ class=md-nav__link> Machine Reading Comprehension </a> </li> <li class=md-nav__item> <a href=../Notes%20on%20NCRF%2B%2B/ class=md-nav__link> NCRF++ </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input checked class="md-nav__toggle md-toggle" id=__nav_4 type=checkbox> <label class=md-nav__link for=__nav_4 id=__nav_4_label tabindex=0> Notes on CS224n-2019 <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=true aria-labelledby=__nav_4_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Notes on CS224n-2019 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../CS224n-2019-00-Info/ class=md-nav__link> CS224n-2019 Introduction </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-Assignment/ class=md-nav__link> CS224n-2019 Assignment </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-01-Introduction%20and%20Word%20Vectors/ class=md-nav__link> 01 Introduction and Word Vectors </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/ class=md-nav__link> 02 Word Vectors 2 and Word Senses </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/ class=md-nav__link> 03 Word Window Classification,Neural Networks, and Matrix Calculus </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/ class=md-nav__link> 04 Backpropagation and Computation Graphs </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/ class=md-nav__link> 05 Linguistic Structure Dependency Parsing </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/ class=md-nav__link> 06 The probability of a sentence Recurrent Neural Networks and Language Models </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/ class=md-nav__link> 07 Vanishing Gradients and Fancy RNNs </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/ class=md-nav__link> 08 Machine Translation, Sequence-to-sequence and Attention </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/ class=md-nav__link> 09 Practical Tips for Final Projects </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id=__toc type=checkbox> <label class="md-nav__link md-nav__link--active" for=__toc> 10 Question Answering and the Default Final Project <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> 10 Question Answering and the Default Final Project </a> <nav aria-label=目录 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#lecture-10-question-answering-and-the-default-final-project> Lecture 10 Question Answering and the Default Final Project </a> <nav aria-label="Lecture 10 Question Answering and the Default Final Project" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#2-motivation-question-answering> 2. Motivation: Question answering </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#3-stanford-question-answering-dataset-squad> 3. Stanford Question Answering Dataset (SQuAD) </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#4-stanford-attentive-reader> 4. Stanford Attentive Reader </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#5-bidaf-bi-directional-attention-flow-for-machine-comprehension> 5. BiDAF: Bi-Directional Attention Flow for Machine Comprehension </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#6-recent-more-advanced-architectures> 6. Recent, more advanced architectures </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#7-elmo-and-bert-preview> 7. ELMo and BERT preview </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#note-07-question-answering> Note 07 Question Answering </a> <nav aria-label="Note 07 Question Answering" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#1-dynamic-memory-networks-for-question-answering-over-text-and-images> 1 Dynamic Memory Networks for Question Answering over Text and Images </a> <nav aria-label="1 Dynamic Memory Networks for Question Answering over Text and Images" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#11-input-module> 1.1 Input Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#12-question-module> 1.2 Question Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#13-episodic-memory-module> 1.3 Episodic Memory Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#14-answer-module> 1.4 Answer Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#15-experiments> 1.5 Experiments </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#16-summary> 1.6 Summary </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#reference> Reference </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../CS224n-2019-11-ConvNets%20for%20NLP/ class=md-nav__link> 11 ConvNets for NLP </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/ class=md-nav__link> 12 Information from parts of words Subword Models </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/ class=md-nav__link> 13 Modeling contexts of use Contextual Representations and Pretraining </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/ class=md-nav__link> 14 Transformers and Self-Attention For Generative Models </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-15-Natural%20Language%20Generation/ class=md-nav__link> 15 Natural Language Generation </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-16-Coreference%20Resolution/ class=md-nav__link> 16 Coreference Resolution </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-17-Multitask%20Learning/ class=md-nav__link> 17 Multitask Learning </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/ class=md-nav__link> 18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/ class=md-nav__link> 19 Safety, Bias, and Fairness </a> </li> <li class=md-nav__item> <a href=../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/ class=md-nav__link> 20 The Future of NLP + Deep Learning </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id=__nav_5 type=checkbox> <label class=md-nav__link for=__nav_5 id=__nav_5_label tabindex=0> Notes on MkDocs <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded=false aria-labelledby=__nav_5_label class=md-nav data-md-level=1> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Notes on MkDocs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../MkDocs_demo/ class=md-nav__link> Demo </a> </li> <li class=md-nav__item> <a href=../Material%20Theme%20Tutorial/ class=md-nav__link> Material Theme Tutorial </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav aria-label=目录 class="md-nav md-nav--secondary"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> 目录 </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a class=md-nav__link href=#lecture-10-question-answering-and-the-default-final-project> Lecture 10 Question Answering and the Default Final Project </a> <nav aria-label="Lecture 10 Question Answering and the Default Final Project" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#2-motivation-question-answering> 2. Motivation: Question answering </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#3-stanford-question-answering-dataset-squad> 3. Stanford Question Answering Dataset (SQuAD) </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#4-stanford-attentive-reader> 4. Stanford Attentive Reader </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#5-bidaf-bi-directional-attention-flow-for-machine-comprehension> 5. BiDAF: Bi-Directional Attention Flow for Machine Comprehension </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#6-recent-more-advanced-architectures> 6. Recent, more advanced architectures </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#7-elmo-and-bert-preview> 7. ELMo and BERT preview </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#note-07-question-answering> Note 07 Question Answering </a> <nav aria-label="Note 07 Question Answering" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#1-dynamic-memory-networks-for-question-answering-over-text-and-images> 1 Dynamic Memory Networks for Question Answering over Text and Images </a> <nav aria-label="1 Dynamic Memory Networks for Question Answering over Text and Images" class=md-nav> <ul class=md-nav__list> <li class=md-nav__item> <a class=md-nav__link href=#11-input-module> 1.1 Input Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#12-question-module> 1.2 Question Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#13-episodic-memory-module> 1.3 Episodic Memory Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#14-answer-module> 1.4 Answer Module </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#15-experiments> 1.5 Experiments </a> </li> <li class=md-nav__item> <a class=md-nav__link href=#16-summary> 1.6 Summary </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a class=md-nav__link href=#reference> Reference </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1>10 Question Answering and the Default Final Project</h1> <h2 id=lecture-10-question-answering-and-the-default-final-project>Lecture 10 Question Answering and the Default Final Project<a class=headerlink href=#lecture-10-question-answering-and-the-default-final-project title="Permanent link">¶</a></h2> <p><strong>Lecture Plan</strong></p> <ol> <li>Final final project notes, etc. </li> <li>Motivation/History </li> <li>The SQuADdataset </li> <li>The Stanford Attentive Reader model </li> <li>BiDAF </li> <li>Recent, more advanced architectures </li> <li>ELMo and BERT preview</li> </ol> <p><strong>Project writeup</strong> </p> <ul> <li>Abstract Introduction</li> <li>Prior related work </li> <li>Model</li> <li>Data</li> <li>Experiments</li> <li>Results</li> <li>Analysis &amp; Conclusion</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561387440177.png><img alt=1561387440177 src=../imgs/1561387440177.png></a></p> <p>在谷歌中检索谁是澳大利亚第三任总理，可以获得答案。</p> <p>技术说明：这是从web页面中提取的“特性片段”回答，而不是使用(结构化的)谷歌知识图(以前称为Freebase)回答的问题。</p> <p>我们今天要谈论的就是这样的问题，而不是基于结构化数据存储的问答。</p> <h3 id=2-motivation-question-answering>2. Motivation: Question answering<a class=headerlink href=#2-motivation-question-answering title="Permanent link">¶</a></h3> <ul> <li>拥有大量的全文文档集合，例如网络，简单地返回相关文档的作用是有限的</li> <li>相反，我们经常想要得到问题的答案</li> <li>尤其是在移动设备上</li> <li>或是使用像Alexa、Google assistant这样的数字助理设备。</li> <li>我们可以把它分解成两部分<ul> <li>查找(可能)包含答案的文档<ul> <li>可以通过传统的信息检索/web搜索处理</li> <li>(下个季度我将讲授cs276，它将处理这个问题</li> </ul> </li> <li>在一段或一份文件中找到答案<ul> <li>这个问题通常被称为阅读理解</li> <li>这就是我们今天要关注的</li> </ul> </li> </ul> </li> </ul> <p><strong>A Brief History of Reading Comprehension</strong> </p> <ul> <li>许多早期的NLP工作尝试阅读理解<ul> <li>Schank, Abelson, Lehnert et al. c. 1977 –“Yale A.I. Project” </li> </ul> </li> <li>由Lynette Hirschman在1999年复活<ul> <li>NLP系统能回答三至六年级学生的人类阅读理解问题吗?简单的方法尝试</li> </ul> </li> <li>Chris Burges于2013年通过 MCTest 又重新复活 RC<ul> <li>再次通过简单的故事文本回答问题</li> </ul> </li> <li>2015/16年，随着大型数据集的产生，闸门开启，可以建立监督神经系统<ul> <li>Hermann et al. (NIPS 2015) DeepMind CNN/DM dataset</li> <li>Rajpurkaret al. (EMNLP 2016) <strong>SQuAD</strong></li> <li>MS MARCO, TriviaQA, RACE, NewsQA, NarrativeQA, …</li> </ul> </li> </ul> <p><strong>Machine Comprehension (Burges 2013)</strong> </p> <p>“一台机器能够理解文本的段落，对于大多数母语使用者能够正确回答的关于文本的任何问题，该机器都能提供一个字符串，这些说话者既能回答该问题，又不会包含与该问题无关的信息。”</p> <p><strong>MCTestReading Comprehension</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561388069044.png><img alt=1561388069044 src=../imgs/1561388069044.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561388079886.png><img alt=1561388079886 src=../imgs/1561388079886.png></a></p> <p><strong>A Brief History of Open-domain Question Answering</strong> </p> <ul> <li>Simmons et al. (1964) 首先探索了如何基于匹配问题和答案的依赖关系解析，从说明性文本中回答问题</li> <li>Murax(Kupiec1993) 旨在使用IR和浅层语言处理在在线百科全书上回答问题</li> <li>NIST TREC QA track 始于1999年，首次严格调查了对大量文档的事实问题的回答</li> <li>IBM的冒险！System (DeepQA, 2011)提出了一个版本的问题;它使用了许多方法的集合</li> <li>DrQA(Chen et al. 2016)采用IR结合神经阅读理解，将深度学习引入开放领域的QA</li> </ul> <p><strong>Turn-of-the Millennium Full NLP QA</strong></p> <p>[architecture of LCC (Harabagiu/Moldovan) QA system, circa 2003] 复杂的系统，但他们在“事实”问题上做得相当好</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561388511187.png><img alt=1561388511187 src=../imgs/1561388511187.png></a></p> <ul> <li>非常复杂的多模块多组件的系统<ul> <li>首先对问题进行解析，使用手写的语义规范化规则，将其转化为更好的语义形式</li> <li>在通过问题类型分类器，找出问题在寻找的语义类型</li> <li>信息检索系统找到可能包含答案的段落，排序后进行选择</li> <li>NER识别候选实体再进行判断</li> </ul> </li> <li>这样的QA系统在特定领域很有效：Factoid Question Answering 针对实体的问答</li> </ul> <h3 id=3-stanford-question-answering-dataset-squad>3. Stanford Question Answering Dataset (SQuAD)<a class=headerlink href=#3-stanford-question-answering-dataset-squad title="Permanent link">¶</a></h3> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561389617242.png><img alt=1561389617242 src=../imgs/1561389617242.png></a></p> <ul> <li> <p>Passage 是来自维基百科的一段文本，系统需要回答问题，在文章中找出答案</p> </li> <li> <p>答案必须是文章中的一系列单词序列，也就是提取式问答</p> </li> <li>100k examples</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561390034569.png><img alt=1561390034569 src=../imgs/1561390034569.png></a></p> <p><strong>SQuAD evaluation, v1.1</strong> </p> <ul> <li> <p>作者收集了3个黄金答案</p> </li> <li> <p>系统在两个指标上计算得分</p> <ul> <li>精确匹配：1/0的准确度，您是否匹配三个答案中的一个</li> <li>F1：将系统和每个答案都视为词袋，并评估</li> </ul> <div class=arithmatex>\[ \text{Precision} =\frac{T P}{T P+F P}, \text { Recall }=\frac{T P}{T P+F N}, \text { harmonic mean } \mathrm{F} 1=\frac{2 P R}{P+R} \]</div> <ul> <li>Precision 和 Recall 的调和平均值</li> <li>分数是(宏观)平均每题F1分数</li> </ul> </li> <li> <p>F1测量被视为更可靠的指标，作为主要指标使用</p> <ul> <li>它不是基于选择是否和人类选择的跨度完全相同，人类选择的跨度容易受到各种影响，包括换行</li> <li>在单次级别匹配不同的答案</li> </ul> </li> <li> <p>这两个指标忽视标点符号和冠词(a, an, the only)</p> </li> </ul> <p><strong>SQuAD2.0</strong> </p> <ul> <li>SQuAD1.0的一个缺陷是，所有问题都有答案的段落</li> <li>系统(隐式地)排名候选答案并选择最好的一个，这就变成了一种排名任务</li> <li>你不必判断一个span是否回答了这个问题</li> <li>SQuAD2.0中 ⅓ 的训练问题没有回答，大约 ½ 的开发/测试问题没有回答<ul> <li>对于No Answer examples, no answer 获得的得分为1，对于精确匹配和F1，任何其他响应的得分都为0</li> </ul> </li> <li>SQuAD2.0最简单的系统方法<ul> <li>对于一个 span 是否回答了一个问题有一个阈值评分</li> </ul> </li> <li>或者您可以有第二个确认回答的组件<ul> <li>类似 自然语言推理 或者 答案验证</li> </ul> </li> </ul> <p>Example</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561391753141.png><img alt=1561391753141 src=../imgs/1561391753141.png></a></p> <p>得分高的系统并不能真正理解人类语言！</p> <p><strong>Good systems are great, but still basic NLU errors</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561392135476.png><img alt=1561392135476 src=../imgs/1561392135476.png></a></p> <p>系统没有真正了解一切，仍然在做一种匹配问题</p> <p><strong>SQuAD limitations</strong></p> <ul> <li>SQuAD 也有其他一些关键限制<ul> <li>只有 span-based 答案(没有 yes/no，计数，隐式的为什么)</li> <li>问题是看着段落构造的<ul> <li>通常不是真正的信息需求</li> <li>一般来说，问题和答案之间的词汇和句法匹配比IRL更大</li> <li>问题与文章高度重叠，无论是单词还是句法结构</li> </ul> </li> <li>除了共同参照，几乎没有任何多事实/句子推理</li> </ul> </li> <li>不过这是一个目标明确，结构良好的干净的数据集<ul> <li>它一直是QA dataset上最常用和最具竞争力的数据集</li> <li>它也是构建行业系统的一个有用的起点（尽管域内数据总是很有帮助！）</li> <li>并且我们正在使用它</li> </ul> </li> </ul> <h3 id=4-stanford-attentive-reader>4. Stanford Attentive Reader<a class=headerlink href=#4-stanford-attentive-reader title="Permanent link">¶</a></h3> <p>[Chen, Bolton, &amp; Manning 2016] [Chen, Fisch, Weston &amp; Bordes2017] DrQA [Chen 2018]</p> <ul> <li>展示了一个最小的，非常成功的阅读理解和问题回答架构</li> <li>后来被称为 the Stanford Attentive Reader</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561392816776.png><img alt=1561392816776 src=../imgs/1561392816776.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561392907079.png><img alt=1561392907079 src=../imgs/1561392907079.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561393203025.png><img alt=1561393203025 src=../imgs/1561393203025.png></a></p> <p>首先将问题用向量表示</p> <ul> <li>对问题中的每个单词，查找其词嵌入</li> <li>输入到双向LSTM中并将最终的 hidden state 拼接</li> </ul> <p>再处理文章</p> <ul> <li>查找每个单词的词嵌入并输入到双向LSTM中</li> <li>使用双线性注意力，将每个LSTM的表示(LSTM的两个隐藏状态的连接)与问题表示做运算，获得了不同位置的注意力，从而获得答案的开始位置，再以同样方式获得答案的结束位置<ul> <li>为了在文章中找到答案，使用问题的向量表示，来解决答案在什么位置使用注意力</li> </ul> </li> </ul> <p><strong>Stanford Attentive Reader++</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561393371745.png><img alt=1561393371745 src=../imgs/1561393371745.png></a></p> <p>整个模型的所有参数都是端到端训练的，训练的目标是开始位置与结束为止的准确度，优化有两种方式</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561394057773.png><img alt=1561394057773 src=../imgs/1561394057773.png></a></p> <p>问题部分</p> <ul> <li>不止是利用最终的隐藏层状态，而是使用所有隐层状态的加权和<ul> <li>使用一个可学习的向量 <span class=arithmatex>\(\boldsymbol{w}\)</span> 与 每个时间步的隐层状态相乘</li> </ul> </li> <li>深层LSTM</li> </ul> <p>文章部分</p> <p>文章中每个token的向量表示由一下部分连接而成</p> <ul> <li>词嵌入(GloVe300d)</li> <li>词的语言特点:POS &amp;NER 标签，one-hot 向量</li> <li>词频率(unigram概率)</li> <li>精确匹配:这个词是否出现在问题<ul> <li>三个二进制的特征： exact, uncased, lemma</li> </ul> </li> <li>对齐问题嵌入(“车”与“汽车”)</li> </ul> <div class=arithmatex>\[ f_{\text {align}}\left(p_{i}\right)=\sum_{j} a_{i, j} \mathbf{E}\left(q_{j}\right) \quad q_{i, j}=\frac{\exp \left(\alpha\left(\mathbf{E}\left(p_{i}\right)\right) \cdot \alpha\left(\mathbf{E}\left(q_{j}\right)\right)\right)}{\sum_{j^{\prime}} \exp \left(\alpha\left(\mathbf{E}\left(p_{i}\right)\right) \cdot \alpha\left(\mathbf{E}\left(q_{j}^{\prime}\right)\right)\right)} \]</div> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561394606625.png><img alt=1561394606625 src=../imgs/1561394606625.png></a></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561394613275.png><img alt=1561394613275 src=../imgs/1561394613275.png></a></p> <ul> <li>单词相似度的语义匹做得更好</li> </ul> <h3 id=5-bidaf-bi-directional-attention-flow-for-machine-comprehension>5. BiDAF: Bi-Directional Attention Flow for Machine Comprehension<a class=headerlink href=#5-bidaf-bi-directional-attention-flow-for-machine-comprehension title="Permanent link">¶</a></h3> <p>(Seo, Kembhavi, Farhadi, Hajishirzi, ICLR 2017)</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561394729370.png><img alt=1561394729370 src=../imgs/1561394729370.png></a></p> <ul> <li> <p>多年来，BiDAF architecture有许多变体和改进，但其核心思想是 <strong>the Attention Flow layer</strong></p> </li> <li> <p><strong>Idea</strong> ：attention 应该双向流动——从上下文到问题，从问题到上下文</p> </li> <li>令相似矩阵( <span class=arithmatex>\(\boldsymbol{w}\)</span> 的维数为6d)</li> </ul> <div class=arithmatex>\[ \boldsymbol{S}_{i j}=\boldsymbol{w}_{\mathrm{sim}}^{T}\left[\boldsymbol{c}_{i} ; \boldsymbol{q}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{q}_{j}\right] \in \mathbb{R} \]</div> <ul> <li>Context-to-Question (C2Q) 注意力 (哪些查询词与每个上下文词最相关)</li> </ul> <div class=arithmatex>\[ \begin{aligned} \alpha^{i} &amp;=\operatorname{softmax}\left(\boldsymbol{S}_{i, :}\right) \in \mathbb{R}^{M} \quad \forall i \in\{1, \ldots, N\} \\ \boldsymbol{a}_{i} &amp;=\sum_{j=1}^{M} \alpha_{j}^{i} \boldsymbol{q}_{j} \in \mathbb{R}^{2 h} \quad \forall i \in\{1, \ldots, N\} \end{aligned} \]</div> <ul> <li>Question-to-Context (Q2C) 注意力 (上下文中最重要的单词相对于查询的加权和——通过max略有不对称)<ul> <li>通过max取得上下文中的每个单词对于问题的相关度</li> </ul> </li> </ul> <div class=arithmatex>\[ \begin{aligned} \boldsymbol{m}_{i} &amp;=\max _{j} \boldsymbol{S}_{i j} \in \mathbb{R} \quad \forall i \in\{1, \ldots, N\} \\ \beta &amp;=\operatorname{softmax}(\boldsymbol{m}) \in \mathbb{R}^{N} \\ \boldsymbol{c}^{\prime} &amp;=\sum_{i=1}^{N} \beta_{i} \boldsymbol{c}_{i} \in \mathbb{R}^{2 h} \end{aligned} \]</div> <ul> <li>对于文章中的每个位置，BiDAF layer的输出为</li> </ul> <div class=arithmatex>\[ \boldsymbol{b}_{i}=\left[\boldsymbol{c}_{i} ; \boldsymbol{a}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{a}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{c}^{\prime}\right] \in \mathbb{R}^{8 h} \quad \forall i \in\{1, \ldots, N\} \]</div> <ul> <li>然后有“modelling”层<ul> <li>文章通过另一个深(双层)BiLSTM</li> </ul> </li> <li>然后回答跨度选择更为复杂<ul> <li>Start：通过BiDAF 和 modelling 的输出层连接到一个密集的全连接层然后softmax</li> <li>End：把 modelling 的输出 <span class=arithmatex>\(M\)</span> 通过另一个BiLSTM得到 <span class=arithmatex>\(M_2\)</span> ，然后再与BiDAF layer连接，并通过密集的全连接层和softmax</li> </ul> </li> </ul> <h3 id=6-recent-more-advanced-architectures>6. Recent, more advanced architectures<a class=headerlink href=#6-recent-more-advanced-architectures title="Permanent link">¶</a></h3> <p>2016年、2017年和2018年的大部分工作都采用了越来越复杂的架构，其中包含了多种注意力变体——通常可以获得很好的任务收益</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561445385565.png><img alt=1561445385565 src=../imgs/1561445385565.png></a></p> <p>人们一直在尝试不同的 Attention </p> <p><strong>Dynamic CoattentionNetworks for Question Answering</strong></p> <p>(CaimingXiong, Victor Zhong, Richard Socher ICLR 2017) </p> <ul> <li>缺陷：问题具有独立于输入的表示形式</li> <li>一个全面的QA模型需要相互依赖</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561441116979.png><img alt=1561441116979 src=../imgs/1561441116979.png></a></p> <p><strong>Coattention Encoder</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561441125342.png><img alt=1561441125342 src=../imgs/1561441125342.png></a></p> <ul> <li>Coattention layer 再次提供了一个上下文之间的双向关注问题</li> <li>然而，coattention包括两级注意力计算<ul> <li>关注那些本身就是注意力输出的表象</li> </ul> </li> <li>我们使用C2Q注意力分布 <span class=arithmatex>\(\alpha _i\)</span> ，求得Q2C注意力输出 <span class=arithmatex>\(\boldsymbol{b}_j\)</span> 的加权和。这给了我们第二级注意力输出 <span class=arithmatex>\(\boldsymbol{s}_{i}\)</span></li> </ul> <div class=arithmatex>\[ \boldsymbol{s}_{i}=\sum_{j=1}^{M+1} \alpha_{j}^{i} \boldsymbol{b}_{j} \in \mathbb{R}^{l} \quad \forall i \in\{1, \ldots, N\} \]</div> <p><strong>FusionNet(Huang, Zhu, Shen, Chen 2017)</strong></p> <p>Attention functions </p> <p>MLP(Additive)形式</p> <div class=arithmatex>\[ S_{i j}=s^{T} \tanh \left(W_{1} c_{i}+W_{2} q_{j}\right) \]</div> <ul> <li>Space: O(mnk), W is kxd</li> </ul> <p>Bilinear (Product) form</p> <div class=arithmatex>\[ \begin{array}{c}{S_{i j}=c_{i}^{T} W q_{j}} \\ {S_{i j}=c_{i}^{T} U^{T} V q_{j}} \\ {S_{i j}=c_{i}^{T} W^{T} D W q_{j}} \\ S_{i j}=\operatorname{Relu}\left(c_{i}^{T} W^{T}\right) \operatorname{DRelu}\left(W q_{j}\right) \end{array} \]</div> <ul> <li>Smaller space, Non-linearity</li> <li>Space: O((m+n)k)</li> </ul> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561444271130.png><img alt=1561444271130 src=../imgs/1561444271130.png></a></p> <p><strong>Multi-level inter-attention</strong></p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561445320199.png><img alt=1561445320199 src=../imgs/1561445320199.png></a></p> <p>经过多层次的inter-attention，使用RNN、self-attention 和另一个RNN得到上下文的最终表示 <span class=arithmatex>\(\left\{\boldsymbol{u}_{i}^{C}\right\}\)</span></p> <h3 id=7-elmo-and-bert-preview>7. ELMo and BERT preview<a class=headerlink href=#7-elmo-and-bert-preview title="Permanent link">¶</a></h3> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561445488664.png><img alt=1561445488664 src=../imgs/1561445488664.png></a></p> <h2 id=note-07-question-answering>Note 07 Question Answering<a class=headerlink href=#note-07-question-answering title="Permanent link">¶</a></h2> <h3 id=1-dynamic-memory-networks-for-question-answering-over-text-and-images>1 Dynamic Memory Networks for Question Answering over Text and Images<a class=headerlink href=#1-dynamic-memory-networks-for-question-answering-over-text-and-images title="Permanent link">¶</a></h3> <p>QA 系统的概念是直接从文档、对话、在线搜索等中提取信息(有时是段落，或是单词的范围)，以满足用户的信息需求。QA系统不需要用户通读整个文档，而是倾向于给出一个简短的答案。现在，QA系统可以很容易地与其他NLP系统(如聊天机器人)结合起来，有些QA系统甚至超越了文本文档的搜索，可以从一组图片中提取信息。</p> <p>有很多类型的问题，其中最简单的是 Factoid Question Answering 事实类问题回答。它包含的问题看起来像““The symbol for mercuric oxide is?” “Which NFL team represented the AFC at Super Bowl 50?”。当然还有其他类型的问题，如数学问题(“2+3=?”)、逻辑问题，这些问题需要广泛的推理(而且没有背景信息)。然而，我们可以说在人们的日常生活中，寻求信息的事实类问题回答是最常见的问题。</p> <p>事实上，大多数NLP问题都可以看作是一个问答问题，其范式很简单：我们发出一个查询，然后机器提供一个响应。通过阅读文档或一组指令，智能系统应该能够回答各种各样的问题。我们可以要求句子的POS标签，我们可以要求系统用不同的语言来响应。因此，很自然地，我们想设计一个可以用于一般QA的模型。</p> <p>为了实现这一目标，我们面临两大障碍。许多NLP任务使用不同的架构，如TreeLSTM (Tai et al., 2015)用于情绪分析，Memory Network (Weston et al., 2015) 用于回答问题，以及双向LSTM-CRF (Huang et al., 2015) 用于词性标注。第二个问题是全面的多任务学习往往非常困难，迁移学习仍然是当前人工智能领域(计算机视觉、强化学习等)神经网络架构的主要障碍。</p> <p>我们可以使用NLP的共享体系结构来解决第一个问题：动态内存网络(DMN)，这是一种为一般QA任务设计的体系结构。QA很难，部分原因是阅读一段很长的文字很难。即使对于人类，我们也不能在你的工作记忆中存储一个很长的文档。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561450892215.png><img alt=1561450892215 src=../imgs/1561450892215.png></a></p> <h4 id=11-input-module>1.1 Input Module<a class=headerlink href=#11-input-module title="Permanent link">¶</a></h4> <p>将DMN分为多个模块。首先我们来看输入模块。输入模块以单词序列 <span class=arithmatex>\(T_I\)</span> 作为输入，输出事实表示序列 <span class=arithmatex>\(T_C\)</span> 。如果输出是一个单词列表，我们有 <span class=arithmatex>\(T_C = T_I\)</span> 。如果输出是一个句子列表，我们有 <span class=arithmatex>\(T_C\)</span> 作为句子的数量， <span class=arithmatex>\(T_I\)</span> 作为句子中的单词数量。我们使用一个简单的GRU来读取其中的句子，即隐藏状态 <span class=arithmatex>\(h_{t}=\operatorname{GRU}\left(x_{t}, h_{t-1}\right)\)</span> ，其中<span class=arithmatex>\(x_{t}=L\left[w_{t}\right]\)</span> ， <span class=arithmatex>\(L\)</span> 为嵌入矩阵，<span class=arithmatex>\(w_t\)</span> 为 <span class=arithmatex>\(t\)</span> 时刻的单词，我们使用Bi-GRU进一步改进，如下图所示。</p> <p><a class=glightbox data-desc-position=bottom data-height=auto data-width=100% href=../imgs/1561450908124.png><img alt=1561450908124 src=../imgs/1561450908124.png></a></p> <h4 id=12-question-module>1.2 Question Module<a class=headerlink href=#12-question-module title="Permanent link">¶</a></h4> <p>我们也使用标准的GRU来读取问题(使用嵌入矩阵<span class=arithmatex>\(L : q_{t}=\operatorname{GRU}\left(L\left[w_{t}^{Q}\right], q_{t-1}\right)\)</span>)，但是问题模块的输出是问题的编码表示。</p> <h4 id=13-episodic-memory-module>1.3 Episodic Memory Module<a class=headerlink href=#13-episodic-memory-module title="Permanent link">¶</a></h4> <p>动态记忆网络的一个显著特征是情景记忆模块，它在输入序列上运行多次，每次关注输入的不同事实子集。</p> <p>它使用Bi-GRU实现这一点，Bi-GRU接收输入模块传入的句子级别表示的输入，并生成情景记忆表示。</p> <p>我们将情景记忆表征表示为 <span class=arithmatex>\(m^i\)</span> ，情景表征(由注意机制输出)表示为 <span class=arithmatex>\(e^i\)</span> 。情景记忆表示使用 <span class=arithmatex>\(m^0 = q\)</span> 初始化，然后继续使用 <span class=arithmatex>\(\mathrm{GRU} : m^{i}=\mathrm{GRU}\left(e^{i}, m^{i-1}\right)\)</span> 。使用来自输入模块的隐藏状态输出更新情景表征，如下所示，其中 <span class=arithmatex>\(g\)</span> 是注意机制 $$ \begin{aligned} h_{t}^{i} &amp;=g_{t}^{i} \operatorname{GRU}\left(c_{t}, h_{t-1}<sup i=i>{i}\right)+\left(1-g_{t}</sup>\right) h_{t-1}^{i} \ e_{i} &amp;=h_{T_{\mathrm{C}}}^{i} \end{aligned} $$ 注意向量 <span class=arithmatex>\(g\)</span> 的计算方法有很多，但是在原始的DMN论文(Kumar et al. 2016)中，我们发现以下公式是最有效的 $$ \begin{array}{l} g_{t}^{i}=G\left(c_{t}, m^{i-1}, q\right) \ {G(c, m, q)=\sigma\left(W^{(2)} \tanh \left(W^{(1)} z(c, m, q)+b<sup _2_=(2)>{(1)}\right)+b</sup>\right)} \ {z(c, m, q)=\left[c, m, q, c \circ q, c \circ m,|c-q|,|c-m|, c^{T} W^{(b)} q_{,} c^{T} W^{(b)} m\right]}\end{array} $$ 这样，如果句子与问题或记忆有关，这个模块中的门就会被激活。在第 <span class=arithmatex>\(i\)</span> 遍中，如果总结不足以回答问题，我们可以在第 <span class=arithmatex>\(i +1\)</span> 遍中重复输入序列。例如，考虑这样一个问题“Where is the football?”以及输入序列“John kicked the football”和“John was in the ﬁeld”。在这个例子中，John和football可以在一个pass中连接，然后John和field可以在第二个pass中连接，这样网络就可以根据这两个信息进行传递推断。</p> <h4 id=14-answer-module>1.4 Answer Module<a class=headerlink href=#14-answer-module title="Permanent link">¶</a></h4> <p>答案模块是一个简单的GRU解码器，它接收问题模块、情景记忆模块的输出，并输出一个单词(或者通常是一个计算结果)。其工作原理如下: $$ \begin{aligned} y_{t} &amp;=\operatorname{softmax}\left(W^{(a)} a_{t}\right) \ a_{t} &amp;=\operatorname{GRU}\left(\left[y_{t-1}, q\right], a_{t-1}\right) \end{aligned} $$</p> <h4 id=15-experiments>1.5 Experiments<a class=headerlink href=#15-experiments title="Permanent link">¶</a></h4> <p>通过实验可以看出，DMN在babl问答任务中的表现优于MemNN，在情绪分析和词性标注方面也优于其他体系结构。情景记忆需要多少个情景？答案是，任务越难，通过的次数就越多。多次传递还可以让网络真正理解句子，只关注最后一项任务的相关部分，而不是只对单词嵌入的信息做出反应。</p> <p>关键思想是模块化系统，您可以通过更改输入模块来允许不同类型的输入。例如，如果我们用一个基于卷积神经网络的模块替换输入模块，那么这个架构就可以处理一个称为可视化问题回答(VQA)的任务。它也能够在这项任务中胜过其他模型。</p> <h4 id=16-summary>1.6 Summary<a class=headerlink href=#16-summary title="Permanent link">¶</a></h4> <p>自2015年以来，寻找能够解决所有问题的通用体系结构的热情略有减退，但在一个领域进行训练并推广到其他领域的愿望有所增强。要理解更高级的问答模块，读者可以参考动态注意力网络(DCN)。</p> <h2 id=reference>Reference<a class=headerlink href=#reference title="Permanent link">¶</a></h2> <p>以下是学习本课程时的可用参考书籍：</p> <p><a href=https://item.jd.com/12355569.html>《基于深度学习的自然语言处理》</a> （车万翔老师等翻译）</p> <p><a href=https://nndl.github.io/ >《神经网络与深度学习》</a></p> <p>以下是整理笔记的过程中参考的博客：</p> <p><a href=https://zhuanlan.zhihu.com/p/59011576>斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p> <p><a href=https://zhuanlan.zhihu.com/p/31977759>斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> <span class="critic comment">这是针对note部分的翻译</span></p> </article> </div> </div> <a class="md-top md-icon" data-md-component=top hidden href=#> <svg viewbox="0 0 24 24" xmlns=http://www.w3.org/2000/svg><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"></path></svg> 回到页面顶部 </a> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright © 2019 - 2022; Xiao Xu </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ rel=noopener target=_blank> Material for MkDocs </a> </div> <div class=md-social> <a class=md-social__link href=https://github.com/looperXX rel=noopener target=_blank title=github.com> <svg viewbox="0 0 480 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"></path></svg> </a> <a class=md-social__link href=https://twitter.com/looperxx_nlp rel=noopener target=_blank title=twitter.com> <svg viewbox="0 0 512 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg> </a> <a class=md-social__link href=https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/ rel=noopener target=_blank title=www.linkedin.com> <svg viewbox="0 0 448 512" xmlns=http://www.w3.org/2000/svg><!-- Font Awesome Free 6.3.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.annotate", "content.tooltips", "navigation.indexes", "navigation.tracking", "navigation.sections", "navigation.tabs", "navigation.top", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script> <script src=../assets/javascripts/bundle.fc8c2696.min.js></script> <script src=../javascripts/baidu-tongji.js></script> <script src=../javascripts/mathjax.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body> </html>