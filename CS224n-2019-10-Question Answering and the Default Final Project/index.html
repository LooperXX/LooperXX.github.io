


<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LooperXX's homepage">
      
      
      
        <meta name="author" content="Looper - Xiao Xu">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.1">
    
    
      
        <title>10 Question Answering and the Default Final Project - Science is interesting.</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.a676eddb.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-164217558-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    <body dir="">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lecture-10-question-answering-and-the-default-final-project" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="">
    <a href=".." title="Science is interesting." class="md-header-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Science is interesting.
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              10 Question Answering and the Default Final Project
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
          

  

<nav class="md-tabs md-tabs--active" aria-label="" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." class="md-tabs__link">
        Home
      </a>
    
  </li>

      
        
  
  
    
    
  
  
    <li class="md-tabs__item">
      
        <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" class="md-tabs__link">
          NLP
        </a>
      
    </li>
  

  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../Normalization/" class="md-tabs__link">
          ML & DL
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-tabs__link md-tabs__link--active">
          CS224n学习笔记
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../MkDocs_demo/" class="md-tabs__link">
          For MkDocs
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Science is interesting." class="md-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    Science is interesting.
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      NLP
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="NLP" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        NLP
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-1" type="checkbox" id="nav-2-1">
    
    <label class="md-nav__link" for="nav-2-1">
      理论笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="理论笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-1">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        理论笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" title="自然语言处理简介" class="md-nav__link">
      自然语言处理简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%E7%9A%84%E5%B7%A8%E4%BA%BA%E8%82%A9%E8%86%80/" title="NLP的巨人肩膀" class="md-nav__link">
      NLP的巨人肩膀
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Attention/" title="Attention" class="md-nav__link">
      Attention
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2">
    
    <label class="md-nav__link" for="nav-2-2">
      代码学习
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="代码学习" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        代码学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../NCRF%2B%2B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="NCRF++" class="md-nav__link">
      NCRF++
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-3" type="checkbox" id="nav-2-3">
    
    <label class="md-nav__link" for="nav-2-3">
      书籍笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="书籍笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        书籍笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" title="Machine Reading Comprehension" class="md-nav__link">
      Machine Reading Comprehension
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%20Concepts/" title="Some Concepts" class="md-nav__link">
      Some Concepts
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NNDL%20%E4%B9%A0%E9%A2%98/" title="NNDL 习题" class="md-nav__link">
      NNDL 习题
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      ML & DL
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="ML & DL" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        ML & DL
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Normalization/" title="Normalization" class="md-nav__link">
      Normalization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%8A%B1%E4%B9%A6%E7%BB%8F%E9%AA%8C%E6%B3%95%E5%88%99/" title="花书经验法则" class="md-nav__link">
      花书经验法则
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
    
    <label class="md-nav__link" for="nav-4">
      CS224n学习笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="CS224n学习笔记" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        CS224n学习笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" title="CS224n-2019简介" class="md-nav__link">
      CS224n-2019简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-Assignment/" title="CS224n-2019作业笔记" class="md-nav__link">
      CS224n-2019作业笔记
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-01-Introduction%20and%20Word%20Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">
      01 Introduction and Word Vectors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">
      02 Word Vectors 2 and Word Senses
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">
      03 Word Window Classification,Neural Networks, and Matrix Calculus
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">
      04 Backpropagation and Computation Graphs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-nav__link">
      05 Linguistic Structure Dependency Parsing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" title="06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">
      06 The probability of a sentence Recurrent Neural Networks and Language Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" title="07 Vanishing Gradients and Fancy RNNs" class="md-nav__link">
      07 Vanishing Gradients and Fancy RNNs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/" title="08 Machine Translation, Sequence-to-sequence and Attention" class="md-nav__link">
      08 Machine Translation, Sequence-to-sequence and Attention
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" title="09 Practical Tips for Final Projects" class="md-nav__link">
      09 Practical Tips for Final Projects
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        10 Question Answering and the Default Final Project
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="10 Question Answering and the Default Final Project" class="md-nav__link md-nav__link--active">
      10 Question Answering and the Default Final Project
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-10-question-answering-and-the-default-final-project" class="md-nav__link">
    Lecture 10 Question Answering and the Default Final Project
  </a>
  
    <nav class="md-nav" aria-label="Lecture 10 Question Answering and the Default Final Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2-motivation-question-answering" class="md-nav__link">
    2. Motivation: Question answering
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-stanford-question-answering-dataset-squad" class="md-nav__link">
    3. Stanford Question Answering Dataset (SQuAD)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-stanford-attentive-reader" class="md-nav__link">
    4. Stanford Attentive Reader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-bidaf-bi-directional-attention-flow-for-machine-comprehension" class="md-nav__link">
    5. BiDAF: Bi-Directional Attention Flow for Machine Comprehension
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-recent-more-advanced-architectures" class="md-nav__link">
    6. Recent, more advanced architectures
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-elmo-and-bert-preview" class="md-nav__link">
    7. ELMo and BERT preview
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#note-07-question-answering" class="md-nav__link">
    Note 07 Question Answering
  </a>
  
    <nav class="md-nav" aria-label="Note 07 Question Answering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dynamic-memory-networks-for-question-answering-over-text-and-images" class="md-nav__link">
    1 Dynamic Memory Networks for Question Answering over Text and Images
  </a>
  
    <nav class="md-nav" aria-label="1 Dynamic Memory Networks for Question Answering over Text and Images">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-input-module" class="md-nav__link">
    1.1 Input Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-question-module" class="md-nav__link">
    1.2 Question Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-episodic-memory-module" class="md-nav__link">
    1.3 Episodic Memory Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-answer-module" class="md-nav__link">
    1.4 Answer Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-experiments" class="md-nav__link">
    1.5 Experiments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-summary" class="md-nav__link">
    1.6 Summary
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" title="11 ConvNets for NLP" class="md-nav__link">
      11 ConvNets for NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/" title="12 Information from parts of words Subword Models" class="md-nav__link">
      12 Information from parts of words Subword Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/" title="13 Modeling contexts of use Contextual Representations and Pretraining" class="md-nav__link">
      13 Modeling contexts of use Contextual Representations and Pretraining
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/" title="14 Transformers and Self-Attention For Generative Models" class="md-nav__link">
      14 Transformers and Self-Attention For Generative Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-15-Natural%20Language%20Generation/" title="15 Natural Language Generation" class="md-nav__link">
      15 Natural Language Generation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-16-Coreference%20Resolution/" title="16 Coreference Resolution" class="md-nav__link">
      16 Coreference Resolution
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-17-Multitask%20Learning/" title="17 Multitask Learning" class="md-nav__link">
      17 Multitask Learning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/" title="18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment" class="md-nav__link">
      18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/" title="19 Safety, Bias, and Fairness" class="md-nav__link">
      19 Safety, Bias, and Fairness
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/" title="20 The Future of NLP + Deep Learning" class="md-nav__link">
      20 The Future of NLP + Deep Learning
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      For MkDocs
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="For MkDocs" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        For MkDocs
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../MkDocs_demo/" title="Demo" class="md-nav__link">
      Demo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Material%20Theme%20Tutorial/" title="Material Theme Tutorial" class="md-nav__link">
      Material Theme Tutorial
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#lecture-10-question-answering-and-the-default-final-project" class="md-nav__link">
    Lecture 10 Question Answering and the Default Final Project
  </a>
  
    <nav class="md-nav" aria-label="Lecture 10 Question Answering and the Default Final Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#2-motivation-question-answering" class="md-nav__link">
    2. Motivation: Question answering
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-stanford-question-answering-dataset-squad" class="md-nav__link">
    3. Stanford Question Answering Dataset (SQuAD)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-stanford-attentive-reader" class="md-nav__link">
    4. Stanford Attentive Reader
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-bidaf-bi-directional-attention-flow-for-machine-comprehension" class="md-nav__link">
    5. BiDAF: Bi-Directional Attention Flow for Machine Comprehension
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-recent-more-advanced-architectures" class="md-nav__link">
    6. Recent, more advanced architectures
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#7-elmo-and-bert-preview" class="md-nav__link">
    7. ELMo and BERT preview
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#note-07-question-answering" class="md-nav__link">
    Note 07 Question Answering
  </a>
  
    <nav class="md-nav" aria-label="Note 07 Question Answering">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dynamic-memory-networks-for-question-answering-over-text-and-images" class="md-nav__link">
    1 Dynamic Memory Networks for Question Answering over Text and Images
  </a>
  
    <nav class="md-nav" aria-label="1 Dynamic Memory Networks for Question Answering over Text and Images">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-input-module" class="md-nav__link">
    1.1 Input Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-question-module" class="md-nav__link">
    1.2 Question Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-episodic-memory-module" class="md-nav__link">
    1.3 Episodic Memory Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-answer-module" class="md-nav__link">
    1.4 Answer Module
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-experiments" class="md-nav__link">
    1.5 Experiments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-summary" class="md-nav__link">
    1.6 Summary
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/LooperXX/LooperXX.github.io/edit/master/docs/CS224n-2019-10-Question Answering and the Default Final Project.md" title="编辑此页" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71,7.04C21.1,6.65 21.1,6 20.71,5.63L18.37,3.29C18,2.9 17.35,2.9 16.96,3.29L15.12,5.12L18.87,8.87M3,17.25V21H6.75L17.81,9.93L14.06,6.18L3,17.25Z" /></svg>
                  </a>
                
                
                  
                
                
                  <h1>10 Question Answering and the Default Final Project</h1>
                
                <h2 id="lecture-10-question-answering-and-the-default-final-project">Lecture 10 Question Answering and the Default Final Project<a class="headerlink" href="#lecture-10-question-answering-and-the-default-final-project" title="Permanent link">&para;</a></h2>
<p><strong>Lecture Plan</strong></p>
<ol>
<li>Final final project notes, etc. </li>
<li>Motivation/History </li>
<li>The SQuADdataset </li>
<li>The Stanford Attentive Reader model </li>
<li>BiDAF </li>
<li>Recent, more advanced architectures </li>
<li>ELMo and BERT preview</li>
</ol>
<p><strong>Project writeup</strong> </p>
<ul>
<li>Abstract Introduction</li>
<li>Prior related work </li>
<li>Model</li>
<li>Data</li>
<li>Experiments</li>
<li>Results</li>
<li>Analysis &amp; Conclusion</li>
</ul>
<p><img alt="1561387440177" src="../imgs/1561387440177.png" /></p>
<p>在谷歌中检索谁是澳大利亚第三任总理，可以获得答案。</p>
<p>技术说明：这是从web页面中提取的“特性片段”回答，而不是使用(结构化的)谷歌知识图(以前称为Freebase)回答的问题。</p>
<p>我们今天要谈论的就是这样的问题，而不是基于结构化数据存储的问答。</p>
<h3 id="2-motivation-question-answering">2. Motivation: Question answering<a class="headerlink" href="#2-motivation-question-answering" title="Permanent link">&para;</a></h3>
<ul>
<li>拥有大量的全文文档集合，例如网络，简单地返回相关文档的作用是有限的</li>
<li>相反，我们经常想要得到问题的答案</li>
<li>尤其是在移动设备上</li>
<li>或是使用像Alexa、Google assistant这样的数字助理设备。</li>
<li>我们可以把它分解成两部分<ul>
<li>查找(可能)包含答案的文档<ul>
<li>可以通过传统的信息检索/web搜索处理</li>
<li>(下个季度我将讲授cs276，它将处理这个问题</li>
</ul>
</li>
<li>在一段或一份文件中找到答案<ul>
<li>这个问题通常被称为阅读理解</li>
<li>这就是我们今天要关注的</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>A Brief History of Reading Comprehension</strong> </p>
<ul>
<li>许多早期的NLP工作尝试阅读理解<ul>
<li>Schank, Abelson, Lehnert et al. c. 1977 –“Yale A.I. Project” </li>
</ul>
</li>
<li>由Lynette Hirschman在1999年复活<ul>
<li>NLP系统能回答三至六年级学生的人类阅读理解问题吗?简单的方法尝试</li>
</ul>
</li>
<li>Chris Burges于2013年通过 MCTest 又重新复活 RC<ul>
<li>再次通过简单的故事文本回答问题</li>
</ul>
</li>
<li>2015/16年，随着大型数据集的产生，闸门开启，可以建立监督神经系统<ul>
<li>Hermann et al. (NIPS 2015) DeepMind CNN/DM dataset</li>
<li>Rajpurkaret al. (EMNLP 2016) <strong>SQuAD</strong></li>
<li>MS MARCO, TriviaQA, RACE, NewsQA, NarrativeQA, …</li>
</ul>
</li>
</ul>
<p><strong>Machine Comprehension (Burges 2013)</strong> </p>
<p>“一台机器能够理解文本的段落，对于大多数母语使用者能够正确回答的关于文本的任何问题，该机器都能提供一个字符串，这些说话者既能回答该问题，又不会包含与该问题无关的信息。”</p>
<p><strong>MCTestReading Comprehension</strong></p>
<p><img alt="1561388069044" src="../imgs/1561388069044.png" /></p>
<p><img alt="1561388079886" src="../imgs/1561388079886.png" /></p>
<p><strong>A Brief History of Open-domain Question Answering</strong> </p>
<ul>
<li>Simmons et al. (1964) 首先探索了如何基于匹配问题和答案的依赖关系解析，从说明性文本中回答问题</li>
<li>Murax(Kupiec1993) 旨在使用IR和浅层语言处理在在线百科全书上回答问题</li>
<li>NIST TREC QA track 始于1999年，首次严格调查了对大量文档的事实问题的回答</li>
<li>IBM的冒险！System (DeepQA, 2011)提出了一个版本的问题;它使用了许多方法的集合</li>
<li>DrQA(Chen et al. 2016)采用IR结合神经阅读理解，将深度学习引入开放领域的QA</li>
</ul>
<p><strong>Turn-of-the Millennium Full NLP QA</strong></p>
<p>[architecture of LCC (Harabagiu/Moldovan) QA system, circa 2003] 复杂的系统，但他们在“事实”问题上做得相当好</p>
<p><img alt="1561388511187" src="../imgs/1561388511187.png" /></p>
<ul>
<li>非常复杂的多模块多组件的系统<ul>
<li>首先对问题进行解析，使用手写的语义规范化规则，将其转化为更好的语义形式</li>
<li>在通过问题类型分类器，找出问题在寻找的语义类型</li>
<li>信息检索系统找到可能包含答案的段落，排序后进行选择</li>
<li>NER识别候选实体再进行判断</li>
</ul>
</li>
<li>这样的QA系统在特定领域很有效：Factoid Question Answering 针对实体的问答</li>
</ul>
<h3 id="3-stanford-question-answering-dataset-squad">3. Stanford Question Answering Dataset (SQuAD)<a class="headerlink" href="#3-stanford-question-answering-dataset-squad" title="Permanent link">&para;</a></h3>
<p><img alt="1561389617242" src="../imgs/1561389617242.png" /></p>
<ul>
<li>
<p>Passage 是来自维基百科的一段文本，系统需要回答问题，在文章中找出答案</p>
</li>
<li>
<p>答案必须是文章中的一系列单词序列，也就是提取式问答</p>
</li>
<li>100k examples</li>
</ul>
<p><img alt="1561390034569" src="../imgs/1561390034569.png" /></p>
<p><strong>SQuAD evaluation, v1.1</strong> </p>
<ul>
<li>
<p>作者收集了3个黄金答案</p>
</li>
<li>
<p>系统在两个指标上计算得分</p>
<ul>
<li>精确匹配：1/0的准确度，您是否匹配三个答案中的一个</li>
<li>F1：将系统和每个答案都视为词袋，并评估</li>
</ul>
<div>
<div class="MathJax_Preview">
\text{Precision} =\frac{T P}{T P+F P}, \text { Recall }=\frac{T P}{T P+F N}, \text { harmonic mean } \mathrm{F} 1=\frac{2 P R}{P+R}
</div>
<script type="math/tex; mode=display">
\text{Precision} =\frac{T P}{T P+F P}, \text { Recall }=\frac{T P}{T P+F N}, \text { harmonic mean } \mathrm{F} 1=\frac{2 P R}{P+R}
</script>
</div>
<ul>
<li>Precision 和 Recall 的调和平均值</li>
<li>分数是(宏观)平均每题F1分数</li>
</ul>
</li>
<li>
<p>F1测量被视为更可靠的指标，作为主要指标使用</p>
<ul>
<li>它不是基于选择是否和人类选择的跨度完全相同，人类选择的跨度容易受到各种影响，包括换行</li>
<li>在单次级别匹配不同的答案</li>
</ul>
</li>
<li>
<p>这两个指标忽视标点符号和冠词(a, an, the only)</p>
</li>
</ul>
<p><strong>SQuAD2.0</strong> </p>
<ul>
<li>SQuAD1.0的一个缺陷是，所有问题都有答案的段落</li>
<li>系统(隐式地)排名候选答案并选择最好的一个，这就变成了一种排名任务</li>
<li>你不必判断一个span是否回答了这个问题</li>
<li>SQuAD2.0中 &#8531; 的训练问题没有回答，大约 &frac12; 的开发/测试问题没有回答<ul>
<li>对于No Answer examples, no answer 获得的得分为1，对于精确匹配和F1，任何其他响应的得分都为0</li>
</ul>
</li>
<li>SQuAD2.0最简单的系统方法<ul>
<li>对于一个 span 是否回答了一个问题有一个阈值评分</li>
</ul>
</li>
<li>或者您可以有第二个确认回答的组件<ul>
<li>类似 自然语言推理 或者 答案验证</li>
</ul>
</li>
</ul>
<p>Example</p>
<p><img alt="1561391753141" src="../imgs/1561391753141.png" /></p>
<p>得分高的系统并不能真正理解人类语言！</p>
<p><strong>Good systems are great, but still basic NLU errors</strong></p>
<p><img alt="1561392135476" src="../imgs/1561392135476.png" /></p>
<p>系统没有真正了解一切，仍然在做一种匹配问题</p>
<p><strong>SQuAD limitations</strong></p>
<ul>
<li>SQuAD 也有其他一些关键限制<ul>
<li>只有 span-based 答案(没有 yes/no，计数，隐式的为什么)</li>
<li>问题是看着段落构造的<ul>
<li>通常不是真正的信息需求</li>
<li>一般来说，问题和答案之间的词汇和句法匹配比IRL更大</li>
<li>问题与文章高度重叠，无论是单词还是句法结构</li>
</ul>
</li>
<li>除了共同参照，几乎没有任何多事实/句子推理</li>
</ul>
</li>
<li>不过这是一个目标明确，结构良好的干净的数据集<ul>
<li>它一直是QA dataset上最常用和最具竞争力的数据集</li>
<li>它也是构建行业系统的一个有用的起点（尽管域内数据总是很有帮助！）</li>
<li>并且我们正在使用它</li>
</ul>
</li>
</ul>
<h3 id="4-stanford-attentive-reader">4. Stanford Attentive Reader<a class="headerlink" href="#4-stanford-attentive-reader" title="Permanent link">&para;</a></h3>
<p>[Chen, Bolton, &amp; Manning 2016] [Chen, Fisch, Weston &amp; Bordes2017] DrQA [Chen 2018]</p>
<ul>
<li>展示了一个最小的，非常成功的阅读理解和问题回答架构</li>
<li>后来被称为 the Stanford Attentive Reader</li>
</ul>
<p><img alt="1561392816776" src="../imgs/1561392816776.png" /></p>
<p><img alt="1561392907079" src="../imgs/1561392907079.png" /></p>
<p><img alt="1561393203025" src="../imgs/1561393203025.png" /></p>
<p>首先将问题用向量表示</p>
<ul>
<li>对问题中的每个单词，查找其词嵌入</li>
<li>输入到双向LSTM中并将最终的 hidden state 拼接</li>
</ul>
<p>再处理文章</p>
<ul>
<li>查找每个单词的词嵌入并输入到双向LSTM中</li>
<li>使用双线性注意力，将每个LSTM的表示(LSTM的两个隐藏状态的连接)与问题表示做运算，获得了不同位置的注意力，从而获得答案的开始位置，再以同样方式获得答案的结束位置<ul>
<li>为了在文章中找到答案，使用问题的向量表示，来解决答案在什么位置使用注意力</li>
</ul>
</li>
</ul>
<p><strong>Stanford Attentive Reader++</strong></p>
<p><img alt="1561393371745" src="../imgs/1561393371745.png" /></p>
<p>整个模型的所有参数都是端到端训练的，训练的目标是开始位置与结束为止的准确度，优化有两种方式</p>
<p><img alt="1561394057773" src="../imgs/1561394057773.png" /></p>
<p>问题部分</p>
<ul>
<li>不止是利用最终的隐藏层状态，而是使用所有隐层状态的加权和<ul>
<li>使用一个可学习的向量 <span><span class="MathJax_Preview">\boldsymbol{w}</span><script type="math/tex">\boldsymbol{w}</script></span> 与 每个时间步的隐层状态相乘</li>
</ul>
</li>
<li>深层LSTM</li>
</ul>
<p>文章部分</p>
<p>文章中每个token的向量表示由一下部分连接而成</p>
<ul>
<li>词嵌入(GloVe300d)</li>
<li>词的语言特点:POS &amp;NER 标签，one-hot 向量</li>
<li>词频率(unigram概率)</li>
<li>精确匹配:这个词是否出现在问题<ul>
<li>三个二进制的特征： exact, uncased, lemma</li>
</ul>
</li>
<li>对齐问题嵌入(“车”与“汽车”)</li>
</ul>
<div>
<div class="MathJax_Preview">
f_{\text {align}}\left(p_{i}\right)=\sum_{j} a_{i, j} \mathbf{E}\left(q_{j}\right) \quad q_{i, j}=\frac{\exp \left(\alpha\left(\mathbf{E}\left(p_{i}\right)\right) \cdot \alpha\left(\mathbf{E}\left(q_{j}\right)\right)\right)}{\sum_{j^{\prime}} \exp \left(\alpha\left(\mathbf{E}\left(p_{i}\right)\right) \cdot \alpha\left(\mathbf{E}\left(q_{j}^{\prime}\right)\right)\right)}
</div>
<script type="math/tex; mode=display">
f_{\text {align}}\left(p_{i}\right)=\sum_{j} a_{i, j} \mathbf{E}\left(q_{j}\right) \quad q_{i, j}=\frac{\exp \left(\alpha\left(\mathbf{E}\left(p_{i}\right)\right) \cdot \alpha\left(\mathbf{E}\left(q_{j}\right)\right)\right)}{\sum_{j^{\prime}} \exp \left(\alpha\left(\mathbf{E}\left(p_{i}\right)\right) \cdot \alpha\left(\mathbf{E}\left(q_{j}^{\prime}\right)\right)\right)}
</script>
</div>
<p><img alt="1561394606625" src="../imgs/1561394606625.png" /></p>
<p><img alt="1561394613275" src="../imgs/1561394613275.png" /></p>
<ul>
<li>单词相似度的语义匹做得更好</li>
</ul>
<h3 id="5-bidaf-bi-directional-attention-flow-for-machine-comprehension">5. BiDAF: Bi-Directional Attention Flow for Machine Comprehension<a class="headerlink" href="#5-bidaf-bi-directional-attention-flow-for-machine-comprehension" title="Permanent link">&para;</a></h3>
<p>(Seo, Kembhavi, Farhadi, Hajishirzi, ICLR 2017)</p>
<p><img alt="1561394729370" src="../imgs/1561394729370.png" /></p>
<ul>
<li>
<p>多年来，BiDAF architecture有许多变体和改进，但其核心思想是 <strong>the Attention Flow layer</strong></p>
</li>
<li>
<p><strong>Idea</strong> ：attention 应该双向流动——从上下文到问题，从问题到上下文</p>
</li>
<li>令相似矩阵( <span><span class="MathJax_Preview">\boldsymbol{w}</span><script type="math/tex">\boldsymbol{w}</script></span> 的维数为6d)</li>
</ul>
<div>
<div class="MathJax_Preview">
\boldsymbol{S}_{i j}=\boldsymbol{w}_{\mathrm{sim}}^{T}\left[\boldsymbol{c}_{i} ; \boldsymbol{q}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{q}_{j}\right] \in \mathbb{R}
</div>
<script type="math/tex; mode=display">
\boldsymbol{S}_{i j}=\boldsymbol{w}_{\mathrm{sim}}^{T}\left[\boldsymbol{c}_{i} ; \boldsymbol{q}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{q}_{j}\right] \in \mathbb{R}
</script>
</div>
<ul>
<li>Context-to-Question (C2Q) 注意力 (哪些查询词与每个上下文词最相关)</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned} \alpha^{i} &amp;=\operatorname{softmax}\left(\boldsymbol{S}_{i, :}\right) \in \mathbb{R}^{M} \quad \forall i \in\{1, \ldots, N\} \\ \boldsymbol{a}_{i} &amp;=\sum_{j=1}^{M} \alpha_{j}^{i} \boldsymbol{q}_{j} \in \mathbb{R}^{2 h} \quad \forall i \in\{1, \ldots, N\} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \alpha^{i} &=\operatorname{softmax}\left(\boldsymbol{S}_{i, :}\right) \in \mathbb{R}^{M} \quad \forall i \in\{1, \ldots, N\} \\ \boldsymbol{a}_{i} &=\sum_{j=1}^{M} \alpha_{j}^{i} \boldsymbol{q}_{j} \in \mathbb{R}^{2 h} \quad \forall i \in\{1, \ldots, N\} \end{aligned}
</script>
</div>
<ul>
<li>Question-to-Context (Q2C) 注意力 (上下文中最重要的单词相对于查询的加权和——通过max略有不对称)<ul>
<li>通过max取得上下文中的每个单词对于问题的相关度</li>
</ul>
</li>
</ul>
<div>
<div class="MathJax_Preview">
\begin{aligned} \boldsymbol{m}_{i} &amp;=\max _{j} \boldsymbol{S}_{i j} \in \mathbb{R} \quad \forall i \in\{1, \ldots, N\} \\ \beta &amp;=\operatorname{softmax}(\boldsymbol{m}) \in \mathbb{R}^{N} \\ \boldsymbol{c}^{\prime} &amp;=\sum_{i=1}^{N} \beta_{i} \boldsymbol{c}_{i} \in \mathbb{R}^{2 h} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \boldsymbol{m}_{i} &=\max _{j} \boldsymbol{S}_{i j} \in \mathbb{R} \quad \forall i \in\{1, \ldots, N\} \\ \beta &=\operatorname{softmax}(\boldsymbol{m}) \in \mathbb{R}^{N} \\ \boldsymbol{c}^{\prime} &=\sum_{i=1}^{N} \beta_{i} \boldsymbol{c}_{i} \in \mathbb{R}^{2 h} \end{aligned}
</script>
</div>
<ul>
<li>对于文章中的每个位置，BiDAF layer的输出为</li>
</ul>
<div>
<div class="MathJax_Preview">
\boldsymbol{b}_{i}=\left[\boldsymbol{c}_{i} ; \boldsymbol{a}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{a}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{c}^{\prime}\right] \in \mathbb{R}^{8 h} \quad \forall i \in\{1, \ldots, N\}
</div>
<script type="math/tex; mode=display">
\boldsymbol{b}_{i}=\left[\boldsymbol{c}_{i} ; \boldsymbol{a}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{a}_{i} ; \boldsymbol{c}_{i} \circ \boldsymbol{c}^{\prime}\right] \in \mathbb{R}^{8 h} \quad \forall i \in\{1, \ldots, N\}
</script>
</div>
<ul>
<li>然后有“modelling”层<ul>
<li>文章通过另一个深(双层)BiLSTM</li>
</ul>
</li>
<li>然后回答跨度选择更为复杂<ul>
<li>Start：通过BiDAF 和 modelling 的输出层连接到一个密集的全连接层然后softmax</li>
<li>End：把 modelling 的输出 <span><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 通过另一个BiLSTM得到 <span><span class="MathJax_Preview">M_2</span><script type="math/tex">M_2</script></span> ，然后再与BiDAF layer连接，并通过密集的全连接层和softmax</li>
</ul>
</li>
</ul>
<h3 id="6-recent-more-advanced-architectures">6. Recent, more advanced architectures<a class="headerlink" href="#6-recent-more-advanced-architectures" title="Permanent link">&para;</a></h3>
<p>2016年、2017年和2018年的大部分工作都采用了越来越复杂的架构，其中包含了多种注意力变体——通常可以获得很好的任务收益</p>
<p><img alt="1561445385565" src="../imgs/1561445385565.png" /></p>
<p>人们一直在尝试不同的 Attention </p>
<p><strong>Dynamic CoattentionNetworks for Question Answering</strong></p>
<p>(CaimingXiong, Victor Zhong, Richard Socher ICLR 2017) </p>
<ul>
<li>缺陷：问题具有独立于输入的表示形式</li>
<li>一个全面的QA模型需要相互依赖</li>
</ul>
<p><img alt="1561441116979" src="../imgs/1561441116979.png" /></p>
<p><strong>Coattention Encoder</strong></p>
<p><img alt="1561441125342" src="../imgs/1561441125342.png" /></p>
<ul>
<li>Coattention layer 再次提供了一个上下文之间的双向关注问题</li>
<li>然而，coattention包括两级注意力计算<ul>
<li>关注那些本身就是注意力输出的表象</li>
</ul>
</li>
<li>我们使用C2Q注意力分布 <span><span class="MathJax_Preview">\alpha _i</span><script type="math/tex">\alpha _i</script></span> ，求得Q2C注意力输出 <span><span class="MathJax_Preview">\boldsymbol{b}_j</span><script type="math/tex">\boldsymbol{b}_j</script></span> 的加权和。这给了我们第二级注意力输出 <span><span class="MathJax_Preview">\boldsymbol{s}_{i}</span><script type="math/tex">\boldsymbol{s}_{i}</script></span></li>
</ul>
<div>
<div class="MathJax_Preview">
\boldsymbol{s}_{i}=\sum_{j=1}^{M+1} \alpha_{j}^{i} \boldsymbol{b}_{j} \in \mathbb{R}^{l} \quad \forall i \in\{1, \ldots, N\}
</div>
<script type="math/tex; mode=display">
\boldsymbol{s}_{i}=\sum_{j=1}^{M+1} \alpha_{j}^{i} \boldsymbol{b}_{j} \in \mathbb{R}^{l} \quad \forall i \in\{1, \ldots, N\}
</script>
</div>
<p><strong>FusionNet(Huang, Zhu, Shen, Chen 2017)</strong></p>
<p>Attention functions </p>
<p>MLP(Additive)形式</p>
<div>
<div class="MathJax_Preview">
S_{i j}=s^{T} \tanh \left(W_{1} c_{i}+W_{2} q_{j}\right)
</div>
<script type="math/tex; mode=display">
S_{i j}=s^{T} \tanh \left(W_{1} c_{i}+W_{2} q_{j}\right)
</script>
</div>
<ul>
<li>Space: O(mnk), W is kxd</li>
</ul>
<p>Bilinear (Product) form</p>
<div>
<div class="MathJax_Preview">
\begin{array}{c}{S_{i j}=c_{i}^{T} W q_{j}} \\ {S_{i j}=c_{i}^{T} U^{T} V q_{j}} \\ {S_{i j}=c_{i}^{T} W^{T} D W q_{j}} \\ 
S_{i j}=\operatorname{Relu}\left(c_{i}^{T} W^{T}\right) \operatorname{DRelu}\left(W q_{j}\right) \end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{c}{S_{i j}=c_{i}^{T} W q_{j}} \\ {S_{i j}=c_{i}^{T} U^{T} V q_{j}} \\ {S_{i j}=c_{i}^{T} W^{T} D W q_{j}} \\ 
S_{i j}=\operatorname{Relu}\left(c_{i}^{T} W^{T}\right) \operatorname{DRelu}\left(W q_{j}\right) \end{array}
</script>
</div>
<ul>
<li>Smaller space, Non-linearity</li>
<li>Space: O((m+n)k)</li>
</ul>
<p><img alt="1561444271130" src="../imgs/1561444271130.png" /></p>
<p><strong>Multi-level inter-attention</strong></p>
<p><img alt="1561445320199" src="../imgs/1561445320199.png" /></p>
<p>经过多层次的inter-attention，使用RNN、self-attention 和另一个RNN得到上下文的最终表示 <span><span class="MathJax_Preview">\left\{\boldsymbol{u}_{i}^{C}\right\}</span><script type="math/tex">\left\{\boldsymbol{u}_{i}^{C}\right\}</script></span></p>
<h3 id="7-elmo-and-bert-preview">7. ELMo and BERT preview<a class="headerlink" href="#7-elmo-and-bert-preview" title="Permanent link">&para;</a></h3>
<p><img alt="1561445488664" src="../imgs/1561445488664.png" /></p>
<h2 id="note-07-question-answering">Note 07 Question Answering<a class="headerlink" href="#note-07-question-answering" title="Permanent link">&para;</a></h2>
<h3 id="1-dynamic-memory-networks-for-question-answering-over-text-and-images">1 Dynamic Memory Networks for Question Answering over Text and Images<a class="headerlink" href="#1-dynamic-memory-networks-for-question-answering-over-text-and-images" title="Permanent link">&para;</a></h3>
<p>QA 系统的概念是直接从文档、对话、在线搜索等中提取信息(有时是段落，或是单词的范围)，以满足用户的信息需求。QA系统不需要用户通读整个文档，而是倾向于给出一个简短的答案。现在，QA系统可以很容易地与其他NLP系统(如聊天机器人)结合起来，有些QA系统甚至超越了文本文档的搜索，可以从一组图片中提取信息。</p>
<p>有很多类型的问题，其中最简单的是 Factoid Question Answering 事实类问题回答。它包含的问题看起来像““The symbol for mercuric oxide is?” “Which NFL team represented the AFC at Super Bowl 50?”。当然还有其他类型的问题，如数学问题(“2+3=?”)、逻辑问题，这些问题需要广泛的推理(而且没有背景信息)。然而，我们可以说在人们的日常生活中，寻求信息的事实类问题回答是最常见的问题。</p>
<p>事实上，大多数NLP问题都可以看作是一个问答问题，其范式很简单：我们发出一个查询，然后机器提供一个响应。通过阅读文档或一组指令，智能系统应该能够回答各种各样的问题。我们可以要求句子的POS标签，我们可以要求系统用不同的语言来响应。因此，很自然地，我们想设计一个可以用于一般QA的模型。</p>
<p>为了实现这一目标，我们面临两大障碍。许多NLP任务使用不同的架构，如TreeLSTM (Tai et al., 2015)用于情绪分析，Memory Network (Weston et al., 2015) 用于回答问题，以及双向LSTM-CRF  (Huang et al., 2015) 用于词性标注。第二个问题是全面的多任务学习往往非常困难，迁移学习仍然是当前人工智能领域(计算机视觉、强化学习等)神经网络架构的主要障碍。</p>
<p>我们可以使用NLP的共享体系结构来解决第一个问题：动态内存网络(DMN)，这是一种为一般QA任务设计的体系结构。QA很难，部分原因是阅读一段很长的文字很难。即使对于人类，我们也不能在你的工作记忆中存储一个很长的文档。</p>
<p><img alt="1561450892215" src="../imgs/1561450892215.png" /></p>
<h4 id="11-input-module">1.1 Input Module<a class="headerlink" href="#11-input-module" title="Permanent link">&para;</a></h4>
<p>将DMN分为多个模块。首先我们来看输入模块。输入模块以单词序列 <span><span class="MathJax_Preview">T_I</span><script type="math/tex">T_I</script></span> 作为输入，输出事实表示序列 <span><span class="MathJax_Preview">T_C</span><script type="math/tex">T_C</script></span> 。如果输出是一个单词列表，我们有 <span><span class="MathJax_Preview">T_C = T_I</span><script type="math/tex">T_C = T_I</script></span> 。如果输出是一个句子列表，我们有 <span><span class="MathJax_Preview">T_C</span><script type="math/tex">T_C</script></span> 作为句子的数量， <span><span class="MathJax_Preview">T_I</span><script type="math/tex">T_I</script></span> 作为句子中的单词数量。我们使用一个简单的GRU来读取其中的句子，即隐藏状态 <span><span class="MathJax_Preview">h_{t}=\operatorname{GRU}\left(x_{t}, h_{t-1}\right)</span><script type="math/tex">h_{t}=\operatorname{GRU}\left(x_{t}, h_{t-1}\right)</script></span> ，其中<span><span class="MathJax_Preview">x_{t}=L\left[w_{t}\right]</span><script type="math/tex">x_{t}=L\left[w_{t}\right]</script></span> ， <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> 为嵌入矩阵，<span><span class="MathJax_Preview">w_t</span><script type="math/tex">w_t</script></span> 为 <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 时刻的单词，我们使用Bi-GRU进一步改进，如下图所示。</p>
<p><img alt="1561450908124" src="../imgs/1561450908124.png" /></p>
<h4 id="12-question-module">1.2 Question Module<a class="headerlink" href="#12-question-module" title="Permanent link">&para;</a></h4>
<p>我们也使用标准的GRU来读取问题(使用嵌入矩阵<span><span class="MathJax_Preview">L : q_{t}=\operatorname{GRU}\left(L\left[w_{t}^{Q}\right], q_{t-1}\right)</span><script type="math/tex">L : q_{t}=\operatorname{GRU}\left(L\left[w_{t}^{Q}\right], q_{t-1}\right)</script></span>)，但是问题模块的输出是问题的编码表示。</p>
<h4 id="13-episodic-memory-module">1.3 Episodic Memory Module<a class="headerlink" href="#13-episodic-memory-module" title="Permanent link">&para;</a></h4>
<p>动态记忆网络的一个显著特征是情景记忆模块，它在输入序列上运行多次，每次关注输入的不同事实子集。</p>
<p>它使用Bi-GRU实现这一点，Bi-GRU接收输入模块传入的句子级别表示的输入，并生成情景记忆表示。</p>
<p>我们将情景记忆表征表示为 <span><span class="MathJax_Preview">m^i</span><script type="math/tex">m^i</script></span> ，情景表征(由注意机制输出)表示为 <span><span class="MathJax_Preview">e^i</span><script type="math/tex">e^i</script></span> 。情景记忆表示使用 <span><span class="MathJax_Preview">m^0 = q</span><script type="math/tex">m^0 = q</script></span> 初始化，然后继续使用 <span><span class="MathJax_Preview">\mathrm{GRU} : m^{i}=\mathrm{GRU}\left(e^{i}, m^{i-1}\right)</span><script type="math/tex">\mathrm{GRU} : m^{i}=\mathrm{GRU}\left(e^{i}, m^{i-1}\right)</script></span> 。使用来自输入模块的隐藏状态输出更新情景表征，如下所示，其中 <span><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> 是注意机制
$$
\begin{aligned} h_{t}^{i} &amp;=g_{t}^{i} \operatorname{GRU}\left(c_{t}, h_{t-1}<sup>{i}\right)+\left(1-g_{t}</sup>{i}\right) h_{t-1}^{i} \ e_{i} &amp;=h_{T_{\mathrm{C}}}^{i} \end{aligned}
$$
注意向量 <span><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> 的计算方法有很多，但是在原始的DMN论文(Kumar et al. 2016)中，我们发现以下公式是最有效的
$$
\begin{array}{l}
g_{t}^{i}=G\left(c_{t}, m^{i-1}, q\right) \
{G(c, m, q)=\sigma\left(W^{(2)} \tanh \left(W^{(1)} z(c, m, q)+b<sup>{(1)}\right)+b</sup>{(2)}\right)} \ 
{z(c, m, q)=\left[c, m, q, c \circ q, c \circ m,|c-q|,|c-m|, c^{T} W^{(b)} q_{,} c^{T} W^{(b)} m\right]}\end{array}
$$
这样，如果句子与问题或记忆有关，这个模块中的门就会被激活。在第 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 遍中，如果总结不足以回答问题，我们可以在第 <span><span class="MathJax_Preview">i +1</span><script type="math/tex">i +1</script></span> 遍中重复输入序列。例如，考虑这样一个问题“Where is the football?”以及输入序列“John kicked the football”和“John was in the ﬁeld”。在这个例子中，John和football可以在一个pass中连接，然后John和field可以在第二个pass中连接，这样网络就可以根据这两个信息进行传递推断。</p>
<h4 id="14-answer-module">1.4 Answer Module<a class="headerlink" href="#14-answer-module" title="Permanent link">&para;</a></h4>
<p>答案模块是一个简单的GRU解码器，它接收问题模块、情景记忆模块的输出，并输出一个单词(或者通常是一个计算结果)。其工作原理如下:
$$
\begin{aligned} y_{t} &amp;=\operatorname{softmax}\left(W^{(a)} a_{t}\right) \ a_{t} &amp;=\operatorname{GRU}\left(\left[y_{t-1}, q\right], a_{t-1}\right) \end{aligned}
$$</p>
<h4 id="15-experiments">1.5 Experiments<a class="headerlink" href="#15-experiments" title="Permanent link">&para;</a></h4>
<p>通过实验可以看出，DMN在babl问答任务中的表现优于MemNN，在情绪分析和词性标注方面也优于其他体系结构。情景记忆需要多少个情景？答案是，任务越难，通过的次数就越多。多次传递还可以让网络真正理解句子，只关注最后一项任务的相关部分，而不是只对单词嵌入的信息做出反应。</p>
<p>关键思想是模块化系统，您可以通过更改输入模块来允许不同类型的输入。例如，如果我们用一个基于卷积神经网络的模块替换输入模块，那么这个架构就可以处理一个称为可视化问题回答(VQA)的任务。它也能够在这项任务中胜过其他模型。</p>
<h4 id="16-summary">1.6 Summary<a class="headerlink" href="#16-summary" title="Permanent link">&para;</a></h4>
<p>自2015年以来，寻找能够解决所有问题的通用体系结构的热情略有减退，但在一个领域进行训练并推广到其他领域的愿望有所增强。要理解更高级的问答模块，读者可以参考动态注意力网络(DCN)。</p>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<p>以下是学习本课程时的可用参考书籍：</p>
<p><a href="https://item.jd.com/12355569.html">《基于深度学习的自然语言处理》</a> （车万翔老师等翻译）</p>
<p><a href="https://nndl.github.io/">《神经网络与深度学习》</a></p>
<p>以下是整理笔记的过程中参考的博客：</p>
<p><a href="https://zhuanlan.zhihu.com/p/59011576">斯坦福CS224N深度学习自然语言处理2019冬学习笔记目录</a> (课件核心内容的提炼，并包含作者的见解与建议)</p>
<p><a href="https://zhuanlan.zhihu.com/p/31977759">斯坦福大学 CS224n自然语言处理与深度学习笔记汇总</a> <span class="critic comment">这是针对note部分的翻译</span></p>
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>var disqus_config=function(){this.page.url="None",this.page.identifier="None"};!function(){var e=document,i=e.createElement("script");i.src="//https-looperxx-github-io-my-wiki.disqus.com/embed.js",i.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(i)}()</script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="">
        
          <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" title="09 Practical Tips for Final Projects" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                09 Practical Tips for Final Projects
              </div>
            </div>
          </a>
        
        
          <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" title="11 ConvNets for NLP" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  下一页
                </span>
                11 ConvNets for NLP
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 - 2020 Xiao Xu
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="https://github.com/looperXX" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
      </a>
    
      
      
      <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.c51dfa35.min.js"></script>
      <script src="../assets/javascripts/bundle.eaaa3931.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "", "search.config.separator": "[\\uff0c\\u3002]+", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.58d22e8e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
    
  </body>
</html>