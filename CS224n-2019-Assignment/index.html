<!doctype html><html lang="zh" class="no-js"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><meta http-equiv="x-ua-compatible" content="ie=edge"><meta name="description" content="Looper's homepage"><link rel="canonical" href="https://looperxx.github.io/CS224n-2019-Assignment/"><meta name="author" content="Looper - Xiao Xu"><meta name="lang:clipboard.copy" content="å¤åˆ¶"><meta name="lang:clipboard.copied" content="å·²å¤åˆ¶"><meta name="lang:search.language" content="jp"><meta name="lang:search.pipeline.stopwords" content="True"><meta name="lang:search.pipeline.trimmer" content="True"><meta name="lang:search.result.none" content="æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ç»“æœ"><meta name="lang:search.result.one" content="æ‰¾åˆ° 1 ä¸ªç¬¦åˆæ¡ä»¶çš„ç»“æœ"><meta name="lang:search.result.other" content="# ä¸ªç¬¦åˆæ¡ä»¶çš„ç»“æœ"><meta name="lang:search.tokenizer" content="[\uff0c\u3002]+"><link rel="shortcut icon" href="../assets/images/favicon.png"><meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.3.1"><title>CS224n-2019ä½œä¸šç¬”è®° - Science is interesting.</title><link rel="stylesheet" href="../assets/stylesheets/application.4031d38b.css"><script src="../assets/javascripts/modernizr.74668098.js"></script><link href="https://fonts.gstatic.com" rel="preconnect" crossorigin><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono&display=swap"><style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style><link rel="stylesheet" href="../assets/fonts/material-icons.css"></head><body dir="ltr"><svg class="md-svg"><defs><svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg></defs></svg> <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off"> <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off"> <label class="md-overlay" data-md-component="overlay" for="__drawer"></label><a href="#cs224n-2019-assignment" tabindex="1" class="md-skip">è·³è½¬è‡³ </a><header class="md-header" data-md-component="header"><nav class="md-header-nav md-grid"><div class="md-flex"><div class="md-flex__cell md-flex__cell--shrink"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-header-nav__button md-logo"><i class="md-icon">î Œ</i></a></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label></div><div class="md-flex__cell md-flex__cell--stretch"><div class="md-flex__ellipsis md-header-nav__title" data-md-component="title"><span class="md-header-nav__topic">Science is interesting.</span><span class="md-header-nav__topic">CS224n-2019ä½œä¸šç¬”è®°</span></div></div><div class="md-flex__cell md-flex__cell--shrink"><label class="md-icon md-icon--search md-header-nav__button" for="__search"></label><div class="md-search" data-md-component="search" role="dialog"><label class="md-search__overlay" for="__search"></label><div class="md-search__inner" role="search"><form class="md-search__form" name="search"><input type="text" class="md-search__input" name="query" placeholder="æœç´¢" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active"> <label class="md-icon md-search__icon" for="__search"></label> <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">&#xE5CD;</button></form><div class="md-search__output"><div class="md-search__scrollwrap" data-md-scrollfix><div class="md-search-result" data-md-component="result"><div class="md-search-result__meta">é”®å…¥ä»¥å¼€å§‹æœç´¢</div><ol class="md-search-result__list"></ol></div></div></div></div></div></div><div class="md-flex__cell md-flex__cell--shrink"><div class="md-header-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="å‰å¾€ Github ä»“åº“" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div></div></div></nav></header><div class="md-container"><nav class="md-tabs md-tabs--active" data-md-component="tabs"><div class="md-tabs__inner md-grid"><ul class="md-tabs__list"><li class="md-tabs__item"><a href=".." title="Home" class="md-tabs__link">Home</a></li><li class="md-tabs__item"><a href="../Linux/" title="Math & CS & Coding" class="md-tabs__link">Math & CS & Coding</a></li><li class="md-tabs__item"><a href="../Attention/" title="ML & DL" class="md-tabs__link">ML & DL</a></li><li class="md-tabs__item"><a href="../è‡ªç„¶è¯­è¨€å¤„ç†ç®€ä»‹/" title="NLP" class="md-tabs__link">NLP</a></li><li class="md-tabs__item"><a href="../é¢ç»/" title="Interview experience" class="md-tabs__link">Interview experience</a></li><li class="md-tabs__item"><a href="../MkDocs_demo/" title="For MkDocs" class="md-tabs__link">For MkDocs</a></li></ul></div></nav><main class="md-main"><div class="md-main__inner md-grid" data-md-component="container"><div class="md-sidebar md-sidebar--primary" data-md-component="navigation"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--primary" data-md-level="0"><label class="md-nav__title md-nav__title--site" for="__drawer"><a href="https://looperxx.github.io/" title="Science is interesting." class="md-nav__button md-logo"><i class="md-icon">î Œ</i></a>Science is interesting.</label><div class="md-nav__source"><a href="https://github.com/looperxx/looperxx.github.io/" title="å‰å¾€ Github ä»“åº“" class="md-source" data-md-source="github"><div class="md-source__icon"><svg viewBox="0 0 24 24" width="24" height="24"><use xlink:href="#__github" width="24" height="24"></use></svg></div><div class="md-source__repository">looperxx/looperxx.github.io</div></a></div><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href=".." title="Home" class="md-nav__link">Home</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2"><label class="md-nav__link" for="nav-2">Math & CS & Coding</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-2">Math & CS & Coding</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Linux/" title="Linux" class="md-nav__link">Linux</a></li><li class="md-nav__item"><a href="../Coding Knowledge/" title="é‡ç‚¹å†…å®¹" class="md-nav__link">é‡ç‚¹å†…å®¹</a></li><li class="md-nav__item"><a href="../å†å¹´æœºè¯•/" title="å†å¹´æœºè¯•" class="md-nav__link">å†å¹´æœºè¯•</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3"><label class="md-nav__link" for="nav-3">ML & DL</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-3">ML & DL</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Attention/" title="Attention" class="md-nav__link">Attention</a></li><li class="md-nav__item"><a href="../Normalization/" title="Normalization" class="md-nav__link">Normalization</a></li><li class="md-nav__item"><a href="../Concepts/" title="Concepts" class="md-nav__link">Concepts</a></li><li class="md-nav__item"><a href="../èŠ±ä¹¦ç»éªŒæ³•åˆ™/" title="èŠ±ä¹¦ç»éªŒæ³•åˆ™" class="md-nav__link">èŠ±ä¹¦ç»éªŒæ³•åˆ™</a></li><li class="md-nav__item"><a href="../ç»å…¸ç½‘ç»œ/" title="ç»å…¸ç½‘ç»œ" class="md-nav__link">ç»å…¸ç½‘ç»œ</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked><label class="md-nav__link" for="nav-4">NLP</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-4">NLP</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-1" type="checkbox" id="nav-4-1"><label class="md-nav__link" for="nav-4-1">ç®€ä»‹</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-1">ç®€ä»‹</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../è‡ªç„¶è¯­è¨€å¤„ç†ç®€ä»‹/" title="è‡ªç„¶è¯­è¨€å¤„ç†ç®€ä»‹" class="md-nav__link">è‡ªç„¶è¯­è¨€å¤„ç†ç®€ä»‹</a></li><li class="md-nav__item"><a href="../NLPçš„å·¨äººè‚©è†€/" title="NLPçš„å·¨äººè‚©è†€" class="md-nav__link">NLPçš„å·¨äººè‚©è†€</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-2" type="checkbox" id="nav-4-2"><label class="md-nav__link" for="nav-4-2">ä¹¦ç±ç¬”è®°</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-2">ä¹¦ç±ç¬”è®°</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../NLP Concepts/" title="NLP Concepts" class="md-nav__link">NLP Concepts</a></li><li class="md-nav__item"><a href="../Neural Reading Comprehension and beyond/" title="Machine Reading Comprehension" class="md-nav__link">Machine Reading Comprehension</a></li></ul></nav></li><li class="md-nav__item md-nav__item--active md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3" type="checkbox" id="nav-4-3" checked><label class="md-nav__link" for="nav-4-3">è¯¾ç¨‹ç¬”è®°</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-4-3">è¯¾ç¨‹ç¬”è®°</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019 ç®€ä»‹/" title="CS224n-2019ç®€ä»‹" class="md-nav__link">CS224n-2019ç®€ä»‹</a></li><li class="md-nav__item md-nav__item--active"><input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc"><label class="md-nav__link md-nav__link--active" for="__toc">CS224n-2019ä½œä¸šç¬”è®°</label><a href="./" title="CS224n-2019ä½œä¸šç¬”è®°" class="md-nav__link md-nav__link--active">CS224n-2019ä½œä¸šç¬”è®°</a><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">ç›®å½•</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#assignment-01" title="Assignment 01" class="md-nav__link">Assignment 01</a></li><li class="md-nav__item"><a href="#assignment-02" title="Assignment 02" class="md-nav__link">Assignment 02</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-written-understanding-word2vec" title="1  Written: Understanding word2vec" class="md-nav__link">1  Written: Understanding word2vec</a></li><li class="md-nav__item"><a href="#2-coding-implementing-word2vec" title="2 Coding: Implementing word2vec" class="md-nav__link">2 Coding: Implementing word2vec</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#word2vecpy" title="word2vec.py" class="md-nav__link">word2vec.py</a></li><li class="md-nav__item"><a href="#sgdpy" title="sgd.py" class="md-nav__link">sgd.py</a></li><li class="md-nav__item"><a href="#runpy" title="run.py" class="md-nav__link">run.py</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a href="#assignment-03" title="Assignment 03" class="md-nav__link">Assignment 03</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-machine-learning-neural-networks" title="1. Machine Learning &amp; Neural Networks" class="md-nav__link">1. Machine Learning &amp; Neural Networks</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#a-adam-optimizer" title="(a) Adam Optimizer" class="md-nav__link">(a) Adam Optimizer</a></li><li class="md-nav__item"><a href="#b-dropout" title="(b) Dropout" class="md-nav__link">(b) Dropout</a></li></ul></nav></li><li class="md-nav__item"><a href="#2-neural-transition-based-dependency-parsing" title="2. Neural Transition-Based Dependency Parsing" class="md-nav__link">2. Neural Transition-Based Dependency Parsing</a></li></ul></nav></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#reference_1" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="è¯„è®º" class="md-nav__link md-nav__link--active">è¯„è®º</a></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-4-3-3" type="checkbox" id="nav-4-3-3"><label class="md-nav__link" for="nav-4-3-3">CS224n-2019å­¦ä¹ ç¬”è®°</label><nav class="md-nav" data-md-component="collapsible" data-md-level="3"><label class="md-nav__title" for="nav-4-3-3">CS224n-2019å­¦ä¹ ç¬”è®°</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">01 Introduction and Word Vectors</a></li><li class="md-nav__item"><a href="../CS224n-2019-02-Word Vectors 2 and Word Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">02 Word Vectors 2 and Word Senses</a></li><li class="md-nav__item"><a href="../CS224n-2019-03-Word Window Classification,Neural Networks, and Matrix Calculus/" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">03 Word Window Classification,Neural Networks, and Matrix Calculus</a></li><li class="md-nav__item"><a href="../CS224n-2019-04-Backpropagation and Computation Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">04 Backpropagation and Computation Graphs</a></li><li class="md-nav__item"><a href="../CS224n-2019-05-Linguistic Structure Dependency Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-nav__link">05 Linguistic Structure Dependency Parsing</a></li><li class="md-nav__item"><a href="../CS224n-2019-06-The probability of a sentence Recurrent Neural Networks and Language Models/" title="06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">06 The probability of a sentence Recurrent Neural Networks and Language Models</a></li></ul></nav></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5"><label class="md-nav__link" for="nav-5">Interview experience</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-5">Interview experience</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../é¢ç»/" title="æˆ‘çš„é¢ç»" class="md-nav__link">æˆ‘çš„é¢ç»</a></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-5-2" type="checkbox" id="nav-5-2"><label class="md-nav__link" for="nav-5-2">å®è®­ç¬”è®°</label><nav class="md-nav" data-md-component="collapsible" data-md-level="2"><label class="md-nav__title" for="nav-5-2">å®è®­ç¬”è®°</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../Protocol Buffers/" title="Protobuf" class="md-nav__link">Protobuf</a></li><li class="md-nav__item"><a href="../FDBus/" title="FDBus" class="md-nav__link">FDBus</a></li><li class="md-nav__item"><a href="../FDBus API/" title="FDBus API" class="md-nav__link">FDBus API</a></li><li class="md-nav__item"><a href="../FDBuså†…éƒ¨ç»“æ„/" title="FDBuså†…éƒ¨ç»“æ„" class="md-nav__link">FDBuså†…éƒ¨ç»“æ„</a></li><li class="md-nav__item"><a href="../Cross compiler/" title="Cross compiler" class="md-nav__link">Cross compiler</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item md-nav__item--nested"><input class="md-toggle md-nav__toggle" data-md-toggle="nav-6" type="checkbox" id="nav-6"><label class="md-nav__link" for="nav-6">For MkDocs</label><nav class="md-nav" data-md-component="collapsible" data-md-level="1"><label class="md-nav__title" for="nav-6">For MkDocs</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="../MkDocs_demo/" title="Demo" class="md-nav__link">Demo</a></li><li class="md-nav__item"><a href="../Material Theme Tutorial/" title="Material Theme Tutorial" class="md-nav__link">Material Theme Tutorial</a></li></ul></nav></li></ul></nav></div></div></div><div class="md-sidebar md-sidebar--secondary" data-md-component="toc"><div class="md-sidebar__scrollwrap"><div class="md-sidebar__inner"><nav class="md-nav md-nav--secondary"><label class="md-nav__title" for="__toc">ç›®å½•</label><ul class="md-nav__list" data-md-scrollfix><li class="md-nav__item"><a href="#assignment-01" title="Assignment 01" class="md-nav__link">Assignment 01</a></li><li class="md-nav__item"><a href="#assignment-02" title="Assignment 02" class="md-nav__link">Assignment 02</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-written-understanding-word2vec" title="1  Written: Understanding word2vec" class="md-nav__link">1  Written: Understanding word2vec</a></li><li class="md-nav__item"><a href="#2-coding-implementing-word2vec" title="2 Coding: Implementing word2vec" class="md-nav__link">2 Coding: Implementing word2vec</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#word2vecpy" title="word2vec.py" class="md-nav__link">word2vec.py</a></li><li class="md-nav__item"><a href="#sgdpy" title="sgd.py" class="md-nav__link">sgd.py</a></li><li class="md-nav__item"><a href="#runpy" title="run.py" class="md-nav__link">run.py</a></li></ul></nav></li></ul></nav></li><li class="md-nav__item"><a href="#assignment-03" title="Assignment 03" class="md-nav__link">Assignment 03</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#1-machine-learning-neural-networks" title="1. Machine Learning &amp; Neural Networks" class="md-nav__link">1. Machine Learning &amp; Neural Networks</a><nav class="md-nav"><ul class="md-nav__list"><li class="md-nav__item"><a href="#a-adam-optimizer" title="(a) Adam Optimizer" class="md-nav__link">(a) Adam Optimizer</a></li><li class="md-nav__item"><a href="#b-dropout" title="(b) Dropout" class="md-nav__link">(b) Dropout</a></li></ul></nav></li><li class="md-nav__item"><a href="#2-neural-transition-based-dependency-parsing" title="2. Neural Transition-Based Dependency Parsing" class="md-nav__link">2. Neural Transition-Based Dependency Parsing</a></li></ul></nav></li><li class="md-nav__item"><a href="#reference" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#reference_1" title="Reference" class="md-nav__link">Reference</a></li><li class="md-nav__item"><a href="#__comments" title="è¯„è®º" class="md-nav__link md-nav__link--active">è¯„è®º</a></li></ul></nav></div></div></div><div class="md-content"><article class="md-content__inner md-typeset"><a href="https://github.com/looperxx/looperxx.github.io/edit/master/docs/CS224n-2019-Assignment.md" title="ç¼–è¾‘æ­¤é¡µ" class="md-icon md-content__icon">&#xE3C9;</a><h1 id="cs224n-2019-assignment">CS224n-2019 Assignment<a class="headerlink" href="#cs224n-2019-assignment" title="Permanent link">&para;</a></h1>
<p>æœ¬æ–‡æ¡£å°†è®°å½•ä½œä¸šä¸­çš„è¦ç‚¹ä»¥åŠé—®é¢˜çš„ç­”æ¡ˆ</p>
<p>è¯¾ç¨‹ç¬”è®°å‚è§æˆ‘çš„<a href="https://looperxx.github.io/CS224n-2019-01-Introduction%20and%20Word%20Vectors/">åšå®¢</a>ï¼Œå¹¶åœ¨åšå®¢çš„<a href="https://github.com/LooperXX/LooperXX.github.io">Repo</a>ä¸­æä¾›ç¬”è®°æºæ–‡ä»¶çš„ä¸‹è½½</p>
<h2 id="assignment-01">Assignment 01<a class="headerlink" href="#assignment-01" title="Permanent link">&para;</a></h2>
<ul>
<li>é€æ­¥å®Œæˆå…±ç°çŸ©é˜µçš„æ­å»ºï¼Œå¹¶è°ƒç”¨ <code>sklearn.decomposition</code> ä¸­çš„ <code>TruncatedSVD</code> å®Œæˆä¼ ç»Ÿçš„åŸºäºSVDçš„é™ç»´ç®—æ³•</li>
<li>å¯è§†åŒ–å±•ç¤ºï¼Œè§‚å¯Ÿå¹¶åˆ†æå…¶åœ¨äºŒç»´ç©ºé—´ä¸‹çš„èšé›†æƒ…å†µã€‚</li>
<li>è½½å…¥Word2Vecï¼Œå°†å…¶ä¸SVDå¾—åˆ°çš„å•è¯åˆ†å¸ƒæƒ…å†µè¿›è¡Œå¯¹æ¯”ï¼Œåˆ†æä¸¤è€…è¯å‘é‡çš„ä¸åŒä¹‹å¤„ã€‚</li>
<li>å­¦ä¹ ä½¿ç”¨<code>gensim</code>ï¼Œä½¿ç”¨<code>Cosine Similarity</code> åˆ†æå•è¯çš„ç›¸ä¼¼åº¦ï¼Œå¯¹æ¯”å•è¯å’Œå…¶åŒä¹‰è¯ä¸åä¹‰è¯çš„<code>Cosine Distance</code> ï¼Œå¹¶å°è¯•æ‰¾åˆ°æ­£ç¡®çš„ä¸é”™è¯¯çš„ç±»æ¯”æ ·ä¾‹</li>
<li>æ¢å¯»Word2Vecå‘é‡ä¸­å­˜åœ¨çš„ <code>Independent Bias</code> é—®é¢˜</li>
</ul>
<h2 id="assignment-02">Assignment 02<a class="headerlink" href="#assignment-02" title="Permanent link">&para;</a></h2>
<h3 id="1-written-understanding-word2vec">1  Written: Understanding word2vec<a class="headerlink" href="#1-written-understanding-word2vec" title="Permanent link">&para;</a></h3>
<div>
<div class="MathJax_Preview">
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
</div>
<script type="math/tex; mode=display">
P(O=o | C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{w \in \mathrm{Vocab}} \exp \left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}
</script>
</div>
<div>
<div class="MathJax_Preview">
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
</div>
<script type="math/tex; mode=display">
J_{\text { naive-softmax }}\left(v_{c}, o, U\right)=-\log P(O=o | C=c)
</script>
</div>
<p>çœŸå®(ç¦»æ•£)æ¦‚ç‡åˆ†å¸ƒ <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> ä¸å¦ä¸€ä¸ªåˆ†å¸ƒ <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span> çš„äº¤å‰ç†µæŸå¤±ä¸º <span><span class="MathJax_Preview">-\sum_i p_{i} \log \left(q_{i}\right)</span><script type="math/tex">-\sum_i p_{i} \log \left(q_{i}\right)</script></span></p>
<div class="admonition question">
<p class="admonition-title">Question a</p>
<p>Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> and <span><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span>; i.e., show that</p>
<div>
<div class="MathJax_Preview">
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
</div>
<script type="math/tex; mode=display">
-\sum_{w \in Vocab} y_{w} \log \left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)
</script>
</div>
<p>Your answer should be one line.</p>
</div>
<p><strong>Answer a</strong> : </p>
<p>å› ä¸º <span><span class="MathJax_Preview">\textbf{y}</span><script type="math/tex">\textbf{y}</script></span> æ˜¯ç‹¬çƒ­å‘é‡ï¼Œæ‰€ä»¥ <span><span class="MathJax_Preview">-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})</span><script type="math/tex">-\sum_{w \in Vocab} y_{w} \log (\hat{y}_{w})=-y_o\log (\hat{y}_{o}) -\sum_{w \in Vocab,w \neq o} y_{w} \log (\hat{y}_{w}) = -\log (\hat{y}_{o})</script></span> </p>
<div class="admonition question">
<p class="admonition-title">Question b</p>
<p>Compute the partial derivative of <span><span class="MathJax_Preview">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</span><script type="math/tex">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</script></span> with respect to <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span>. Please write your answer in terms of <span><span class="MathJax_Preview">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</span><script type="math/tex">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</script></span>.</p>
</div>
<p><strong>Answer b</strong> : </p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &amp;={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &amp;={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &amp;={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &amp;={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &amp;={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &amp;={U(\hat{y}-y)}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}} &={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial v_{c}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial v_{c}}} 
\\ &={-u_{o}+\frac{1}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} \frac{\partial \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)}{\partial v_{c}}
\\ &={-u_{o}+\sum_{w} \frac{\exp \left(u_{w}^{T} v_{c}\right)u_w}{\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)}} 
\\ &={-u_{o}+\sum_{w} p(O=w | C=c)u_{w}}
\\ &={-u_{o}+\sum_{w} \hat{y_w} u_{w}}
\\ &={U(\hat{y}-y)}
\end{array}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question c</p>
<p>Compute the partial derivatives of <span><span class="MathJax_Preview">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</span><script type="math/tex">J_{\text{naive-softmax}}(v_c, o, \textbf{U})</script></span> with respect to each of the â€˜outside' word vectors, <span><span class="MathJax_Preview">u_w</span><script type="math/tex">u_w</script></span>'s. There will be two cases: when <span><span class="MathJax_Preview">w = o</span><script type="math/tex">w = o</script></span>, the true â€˜outside' word vector, and <span><span class="MathJax_Preview">w \neq o</span><script type="math/tex">w \neq o</script></span>, for all other words. Please write you answer in terms of <span><span class="MathJax_Preview">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</span><script type="math/tex">\textbf{y}, \hat {\textbf{y}}, \textbf{U}</script></span>.</p>
</div>
<p><strong>Answer c</strong> : 
$$
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}<br />
&amp;={-\frac{\partial\left(u_{o}^{T} v_{c}\right)}{\partial u_{w}}+\frac{\partial \left(\log \left(\sum_{w} \exp \left(u_{w}^{T} v_{c}\right)\right)\right)}{\partial u_{w}}} 
\end{array}
$$</p>
<p>When <span><span class="MathJax_Preview">w \neq o</span><script type="math/tex">w \neq o</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&amp;= 0 + p(O=w | C=c) v_{c}
\\ &amp;=\hat{y}_{w} v_{c}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= 0 + p(O=w | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c}
\end{array}
</script>
</div>
<p>When <span><span class="MathJax_Preview">w = o</span><script type="math/tex">w = o</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&amp;= -v_c + p(O=o | C=c) v_{c}
\\ &amp;=\hat{y}_{w} v_{c} - v_c
\\ &amp;=(\hat{y}_{w} - 1)v_c
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial u_{w}}}  
&= -v_c + p(O=o | C=c) v_{c}
\\ &=\hat{y}_{w} v_{c} - v_c
\\ &=(\hat{y}_{w} - 1)v_c
\end{array}
</script>
</div>
<p>Then : </p>
<div>
<div class="MathJax_Preview">
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
</div>
<script type="math/tex; mode=display">
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} = v_c(\hat y - y)^T
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question d</p>
<p>The sigmoid function is given by the follow Equation :</p>
<div>
<div class="MathJax_Preview">
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
</div>
<script type="math/tex; mode=display">
\sigma(x)=\frac{1}{1+e^{-x}}=\frac{e^{x}}{e^{x}+1}
</script>
</div>
<p>Please compute the derivative of <span><span class="MathJax_Preview">\sigma (x)</span><script type="math/tex">\sigma (x)</script></span> with respect to <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, where <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is a vector.</p>
</div>
<p><strong>Answer d</strong> : </p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&amp;=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&amp;= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &amp;=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} &amp; {0} &amp; {\cdots} &amp; {0} \\ {0} &amp; {\sigma^{\prime}\left(x_{2}\right)} &amp; {\cdots} &amp; {0} \\ {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} \\ {0} &amp; {0} &amp; {\cdots} &amp; {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &amp;=\text{diag}(\sigma^\prime(x))
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial \sigma(x_i)}{\partial x_i}
&=\frac{1}{(1+\exp (-x_i))^{2}} \exp (-x_i)=\sigma(x_i)(1-\sigma(x_i)) \\
\frac{\partial \sigma(x)}{\partial x}
&= \left[\frac{\partial \sigma\left(x_{j}\right)}{\partial x_{i}}\right]_{d \times d}
\\ &=\left[\begin{array}{cccc}{\sigma^{\prime}\left(x_{1}\right)} & {0} & {\cdots} & {0} \\ {0} & {\sigma^{\prime}\left(x_{2}\right)} & {\cdots} & {0} \\ {\vdots} & {\vdots} & {\vdots} & {\vdots} \\ {0} & {0} & {\cdots} & {\sigma^{\prime}\left(x_{d}\right)}\end{array}\right]
\\ &=\text{diag}(\sigma^\prime(x))
\end{array}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question e</p>
<p>Now we shall consider the Negative Sampling loss, which is an alternative to the Naive
Softmax loss. Assume that <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> negative samples (words) are drawn from the vocabulary. For simplicity
of notation we shall refer to them as <span><span class="MathJax_Preview">w_{1}, w_{2}, \dots, w_{K}</span><script type="math/tex">w_{1}, w_{2}, \dots, w_{K}</script></span> and their outside vectors as <span><span class="MathJax_Preview">u_{1}, \dots, u_{K}</span><script type="math/tex">u_{1}, \dots, u_{K}</script></span>. Note that
<span><span class="MathJax_Preview">o \notin\left\{w_{1}, \dots, w_{K}\right\}</span><script type="math/tex">o \notin\left\{w_{1}, \dots, w_{K}\right\}</script></span>. For a center word <span><span class="MathJax_Preview">c</span><script type="math/tex">c</script></span> and an outside word <span><span class="MathJax_Preview">o</span><script type="math/tex">o</script></span>, the negative sampling loss function is
given by:</p>
<div>
<div class="MathJax_Preview">
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
</div>
<script type="math/tex; mode=display">
J_{\text { neg-sample }}\left(v_{c}, o, U\right)=-\log \left(\sigma\left(u_{o}^{\top} v_{c}\right)\right)-\sum_{k=1}^{K} \log \left(\sigma\left(-u_{k}^{\top} v_{c}\right)\right)
</script>
</div>
<p>for a sample <span><span class="MathJax_Preview">w_{1}, w_{2}, \dots, w_{K}</span><script type="math/tex">w_{1}, w_{2}, \dots, w_{K}</script></span>, where <span><span class="MathJax_Preview">\sigma(\cdot)</span><script type="math/tex">\sigma(\cdot)</script></span> is the sigmoid function.</p>
<p>Please repeat parts b and c, computing the partial derivatives of <span><span class="MathJax_Preview">J_{\text { neg-sample }}</span><script type="math/tex">J_{\text { neg-sample }}</script></span> respect to <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span>, with
respect to <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span>, and with respect to a negative sample <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span>. Please write your answers in terms of the
vectors <span><span class="MathJax_Preview">u_o, v_c,</span><script type="math/tex">u_o, v_c,</script></span> and <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span>, where <span><span class="MathJax_Preview">k \in[1, K]</span><script type="math/tex">k \in[1, K]</script></span>. After you've done this, describe with one sentence why this
loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able
to use your solution to part (d) to help compute the necessary gradients here.</p>
</div>
<p><strong>Answer e</strong> : </p>
<p>For <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span> :</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&amp;=(\sigma(u_o^T v_c) - 1) u_o   + \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &amp;= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\sigma\left(u_{k}^{T} v_{c}\right) u_{k}
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
\frac{\partial J_{\text {neg-sample}}}{\partial v_c}
&=(\sigma(u_o^T v_c) - 1) u_o   + \sum_{k=1}^{K}\left(1-\sigma\left(-u_{k}^{T} v_{c}\right)\right) u_{k} 
\\ &= (\sigma(u_o^T v_c) - 1) u_o+ \sum_{k=1}^{K}\sigma\left(u_{k}^{T} v_{c}\right) u_{k}
\end{array}
</script>
</div>
<p>For <span><span class="MathJax_Preview">u_o</span><script type="math/tex">u_o</script></span>, Remeber : <span><span class="MathJax_Preview">o \notin\left\{w_{1}, \dots, w_{K}\right\}</span><script type="math/tex">o \notin\left\{w_{1}, \dots, w_{K}\right\}</script></span> <img alt="ğŸ˜¢" class="emojione" src="https://cdn.jsdelivr.net/emojione/assets/4.0/png/64/1f622.png" title=":cry:" /> :</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
</div>
<script type="math/tex; mode=display">
\frac{\partial J_{\text {neg-sample}}}{\partial u_o}=(\sigma(u_o^T v_c) - 1)v_c
</script>
</div>
<p>For <span><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span> :</p>
<div>
<div class="MathJax_Preview">
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
</div>
<script type="math/tex; mode=display">
\frac{\partial J}{\partial \boldsymbol{u}_{k}}=-\left(\sigma\left(-\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} = \sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c}, \quad for\ k=1,2, \ldots, K
</script>
</div>
<p>Why this
loss function is much more efficient to compute than the naive-softmax loss?</p>
<p>For naive softmax loss function:</p>
<div>
<div class="MathJax_Preview">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&amp;={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&amp;= v_c(\hat y - y)^T
\end{array}
</div>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{\partial J\left(v_{c}, o, U\right)}{\partial v_{c}}}  
&={U(\hat{y}-y)}
\\ {\frac{\partial J\left(v_{c}, o, U\right)}{\partial U}} 
&= v_c(\hat y - y)^T
\end{array}
</script>
</div>
<p>For negative sampling loss function:</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &amp;=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k} 
=\sigma\left(-\boldsymbol{u}_{o}^{\top} v_{c}\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k}
\\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &amp;=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} 
= \sigma\left(-\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c} 
\\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &amp;=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \frac{\partial J}{\partial \boldsymbol{v}_{c}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} v_{c}\right)-1\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k} 
=\sigma\left(-\boldsymbol{u}_{o}^{\top} v_{c}\right) \boldsymbol{u}_{o} + \sum_{k=1}^{K}\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{k}
\\ \frac{\partial J}{\partial \boldsymbol{u}_{o}} &=\left(\sigma\left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)-1\right) \boldsymbol{v}_{c} 
= \sigma\left(-\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)\boldsymbol{v}_{c} 
\\ \frac{\partial J}{\partial \boldsymbol{u}_{k}} &=\sigma\left(\boldsymbol{u}_{k}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{v}_{c}, \quad \text { for all } k=1,2, \ldots, K \end{aligned}
</script>
</div>
<p>ä»æ±‚å¾—çš„åå¯¼æ•°ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒåŸå§‹çš„softmaxå‡½æ•°æ¯æ¬¡å¯¹ <span><span class="MathJax_Preview">v_c</span><script type="math/tex">v_c</script></span> è¿›è¡Œåå‘ä¼ æ’­æ—¶ï¼Œéœ€è¦ä¸ output vector matrix è¿›è¡Œå¤§é‡ä¸”å¤æ‚çš„çŸ©é˜µè¿ç®—ï¼Œè€Œè´Ÿé‡‡æ ·ä¸­çš„è®¡ç®—å¤æ‚åº¦åˆ™ä¸å†ä¸è¯è¡¨å¤§å° <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> æœ‰å…³ï¼Œè€Œæ˜¯ä¸é‡‡æ ·æ•°é‡ <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> æœ‰å…³ã€‚</p>
<div class="admonition question">
<p class="admonition-title">Question f</p>
<p>Suppose the center word is <span><span class="MathJax_Preview">c = w_t</span><script type="math/tex">c = w_t</script></span> and the context window is <span><span class="MathJax_Preview">\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]</span><script type="math/tex">\left[w_{t-m}, \ldots, w_{t-1}, w_{t}, w_{t+1}, \dots,w_{t+m} \right]</script></span>, where <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> is the context window size. Recall that for the skip-gram version of <strong>word2vec</strong>, the
total loss for the context window is</p>
<div>
<div class="MathJax_Preview">
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
</div>
<script type="math/tex; mode=display">
J_{\text { skip-gram }}\left(v_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j \neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)
</script>
</div>
<p>Here, <span><span class="MathJax_Preview">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> represents an arbitrary loss term for the center word <span><span class="MathJax_Preview">c = w_t</span><script type="math/tex">c = w_t</script></span> and outside word
<span><span class="MathJax_Preview">w_t+j</span><script type="math/tex">w_t+j</script></span> . <span><span class="MathJax_Preview">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> could be <span><span class="MathJax_Preview">J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J_{\text {naive-softmax}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> or <span><span class="MathJax_Preview">J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">J_{\text {neg-sample}}\left(v_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span>, depending on your
implementation.</p>
<p>Write down three partial derivatives:</p>
<div>
<div class="MathJax_Preview">
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
</div>
<script type="math/tex; mode=display">
\partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U} \\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{c}
\\ \partial \boldsymbol{J}_{\text { skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m}, \ldots w_{t+m}, \boldsymbol{U}\right) / \partial v_{w} \text { when } w \neq c
</script>
</div>
<p>Write your answers in terms of <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{U}</script></span> and <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) / \partial \boldsymbol{v_c}</script></span>. This is very simple -
each solution should be one line.</p>
<p><strong><em>Once you're done</em></strong>: Given that you computed the derivatives of <span><span class="MathJax_Preview">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)</span><script type="math/tex">\partial \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right)</script></span> with respect to all the
model parameters <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> and <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> in parts a to c, you have now computed the derivatives of the full loss
function <span><span class="MathJax_Preview">J_{skip-gram}</span><script type="math/tex">J_{skip-gram}</script></span> with respect to all parameters. You're ready to implement <strong><em>word2vec</em></strong> !</p>
</div>
<p><strong>Answer f</strong> : 
$$
\begin{array}{l} 
\frac{\partial J_{s g}}{\partial U} &amp;= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial U} 
\ \frac{\partial J_{s g}}{\partial v_{c}}&amp;= \sum_{-m \leq j \leq m, j \neq 0} \frac{\partial J\left(v_{c}, w_{t+j}, U\right)}{\partial v_{c}} 
\ \frac{\partial J_{s g}}{\partial v_{w}}&amp;=0(\text { when } w \neq c) \end{array}
$$</p>
<h3 id="2-coding-implementing-word2vec">2 Coding: Implementing word2vec<a class="headerlink" href="#2-coding-implementing-word2vec" title="Permanent link">&para;</a></h3>
<h4 id="word2vecpy">word2vec.py<a class="headerlink" href="#word2vecpy" title="Permanent link">&para;</a></h4>
<p>æœ¬éƒ¨åˆ†è¦æ±‚å®ç° <span><span class="MathJax_Preview">sigmoid, naiveSoftmaxLossAndGradient, negSamplingLossAndGradient, skipgram</span><script type="math/tex">sigmoid, naiveSoftmaxLossAndGradient, negSamplingLossAndGradient, skipgram</script></span>  å››ä¸ªå‡½æ•°ï¼Œä¸»è¦è€ƒå¯Ÿå¯¹ç¬¬ä¸€éƒ¨åˆ†ä¸­åå‘ä¼ æ’­è®¡ç®—ç»“æœçš„å®ç°ã€‚ä»£ç å®ç°ä¸­ï¼Œé€šè¿‡ä¼˜åŒ–åå¯¼æ•°ç»“åˆåå¯¼æ•°è®¡ç®—ç»“æœä¸ <span><span class="MathJax_Preview">\sigma(x) + \sigma(-x) = 1</span><script type="math/tex">\sigma(x) + \sigma(-x) = 1</script></span> å¯¹å…¬å¼è¿›è¡Œè½¬åŒ–ï¼Œä»è€Œå®ç°äº†å…¨çŸ¢é‡åŒ–ã€‚è¿™éƒ¨åˆ†éœ€è¦å¤§å®¶è‡ªè¡Œç»“åˆä»£ç ä¸å…¬å¼è¿›è¡Œæ¨å¯¼ã€‚</p>
<h4 id="sgdpy">sgd.py<a class="headerlink" href="#sgdpy" title="Permanent link">&para;</a></h4>
<p>å®ç° SGD 
$$
\theta^{n e w}=\theta^{o l d} - \alpha \nabla_{\theta} J(\theta)
$$</p>
<h4 id="runpy">run.py<a class="headerlink" href="#runpy" title="Permanent link">&para;</a></h4>
<p>é¦–å…ˆè¦è¯´æ˜çš„æ˜¯ï¼Œè¿™ä¸ªçœŸçš„è¦è·‘å¥½ä¹… <img alt="ğŸ˜…" class="emojione" src="https://cdn.jsdelivr.net/emojione/assets/4.0/png/64/1f605.png" title=":sweat_smile:" /></p>
<div class="admonition question">
<p class="admonition-title">Question</p>
<p>Briefly explain in at most three sentences what you see in the plot.</p>
</div>
<p><img alt="word_vectors" src="../imgs/word_vectors.png" /></p>
<p>ä¸Šå›¾æ˜¯ç»è¿‡è®­ç»ƒçš„è¯å‘é‡çš„å¯è§†åŒ–ã€‚æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ä¸€äº›æ¨¡å¼ï¼š</p>
<ul>
<li>è¿‘ä¹‰è¯è¢«ç»„åˆåœ¨ä¸€èµ·ï¼Œæ¯”å¦‚ amazing å’Œ wonderfulï¼Œwoman å’Œ femaleã€‚<ul>
<li>ä½†æ˜¯ man å’Œ male å´è·ç¦»è¾ƒè¿œ</li>
</ul>
</li>
<li>åä¹‰è¯å¯èƒ½å› ä¸ºç»å¸¸å±äºåŒä¸€ä¸Šä¸‹æ–‡ï¼Œå®ƒä»¬ä¹Ÿä¼šä¸åŒä¹‰è¯ä¸€èµ·å‡ºç°ï¼Œæ¯”å¦‚ enjoyable å’Œ annoyingã€‚</li>
<li><code>man:king::woman:queen</code> ä»¥åŠ <code>queen:king::female:male</code> å½¢æˆçš„ä¸¤æ¡ç›´çº¿åŸºæœ¬å¹³è¡Œ</li>
</ul>
<h2 id="assignment-03">Assignment 03<a class="headerlink" href="#assignment-03" title="Permanent link">&para;</a></h2>
<h3 id="1-machine-learning-neural-networks">1. Machine Learning &amp; Neural Networks<a class="headerlink" href="#1-machine-learning-neural-networks" title="Permanent link">&para;</a></h3>
<h4 id="a-adam-optimizer">(a) Adam Optimizer<a class="headerlink" href="#a-adam-optimizer" title="Permanent link">&para;</a></h4>
<p>å›å¿†ä¸€ä¸‹æ ‡å‡†éšæœºæ¢¯åº¦ä¸‹é™çš„æ›´æ–°è§„åˆ™
$$
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}-\alpha \nabla_{\boldsymbol{\theta}}{J_{\mathrm{minibatch}}(\boldsymbol{\theta})}
$$
å…¶ä¸­ï¼Œ<span><span class="MathJax_Preview">\boldsymbol{\theta}</span><script type="math/tex">\boldsymbol{\theta}</script></span> æ˜¯åŒ…å«æ¨¡å‹æ‰€æœ‰å‚æ•°çš„å‘é‡ï¼Œ<span><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> æ˜¯æŸå¤±å‡½æ•°ï¼Œ<span><span class="MathJax_Preview">\nabla_{\boldsymbol{\theta}} J_{\mathrm{minibatch}}(\boldsymbol{\theta})</span><script type="math/tex">\nabla_{\boldsymbol{\theta}} J_{\mathrm{minibatch}}(\boldsymbol{\theta})</script></span> æ˜¯å…³äºminibatchæ•°æ®ä¸Šå‚æ•°çš„æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼Œ<span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> æ˜¯å­¦ä¹ ç‡ã€‚<a href="https://arxiv.org/pdf/1412.6980.pdf">Adam Optimization</a>ä½¿ç”¨äº†ä¸€ä¸ªæ›´å¤æ‚çš„æ›´æ–°è§„åˆ™ï¼Œå¹¶é™„åŠ äº†ä¸¤ä¸ªæ­¥éª¤ã€‚</p>
<div class="admonition question">
<p class="admonition-title">Question 1.a.i</p>
<p>é¦–å…ˆï¼ŒAdamä½¿ç”¨äº†ä¸€ä¸ªå«åš <span><span class="MathJax_Preview">momentum</span><script type="math/tex">momentum</script></span> **åŠ¨é‡**çš„æŠ€å·§æ¥è·Ÿè¸ªæ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼ <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span></p>
<div>
<div class="MathJax_Preview">
\begin{aligned}
\mathbf{m} &amp; \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ \boldsymbol{\theta} &amp; \leftarrow \boldsymbol{\theta}-\alpha \mathbf{m} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{m} & \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ \boldsymbol{\theta} & \leftarrow \boldsymbol{\theta}-\alpha \mathbf{m} \end{aligned}
</script>
</div>
<p>å…¶ä¸­ï¼Œ<span><span class="MathJax_Preview">\beta_1</span><script type="math/tex">\beta_1</script></span> æ˜¯ä¸€ä¸ª 0 å’Œ 1 ä¹‹é—´çš„è¶…å‚æ•°(é€šå¸¸è¢«è®¾ä¸º0.9)ã€‚ç®€è¦è¯´æ˜(ä¸éœ€è¦ç”¨æ•°å­¦æ–¹æ³•è¯æ˜ï¼Œåªéœ€è¦ç›´è§‚åœ°è¯´æ˜)å¦‚ä½•ä½¿ç”¨mæ¥é˜»æ­¢æ›´æ–°å‘ç”Ÿå¤§çš„å˜åŒ–ï¼Œä»¥åŠæ€»ä½“ä¸Šä¸ºä»€ä¹ˆè¿™ç§å°å˜åŒ–å¯èƒ½æœ‰åŠ©äºå­¦ä¹ ã€‚</p>
</div>
<p><strong>Answer 1.a.i</strong> : </p>
<ul>
<li>
<p>ç”±äºè¶…å‚æ•° <span><span class="MathJax_Preview">\beta _1</span><script type="math/tex">\beta _1</script></span> ä¸€èˆ¬è¢«è®¾ä¸º0.9ï¼Œæ­¤æ—¶å¯¹äºç§»åŠ¨å¹³å‡çš„æ¢¯åº¦å€¼ <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> è€Œè¨€ï¼Œä¸»è¦å—åˆ°çš„æ˜¯ä¹‹å‰æ¢¯åº¦çš„ç§»åŠ¨å¹³å‡å€¼çš„å½±å“ï¼Œè€Œæœ¬æ¬¡è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦å°†ä¼šè¢«ç¼©æ”¾ä¸ºåŸæ¥çš„ <span><span class="MathJax_Preview">{1 - \beta_1}</span><script type="math/tex">{1 - \beta_1}</script></span> å€ï¼Œå³æ—¶æœ¬æ¬¡è®¡ç®—å¾—åˆ°çš„æ¢¯åº¦å¾ˆå¤§ï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼‰ï¼Œè¿™ä¸€å½±å“ä¹Ÿä¼šè¢«å‡è½»ï¼Œä»è€Œé˜»æ­¢æ›´æ–°å‘ç”Ÿå¤§çš„å˜åŒ–ã€‚</p>
</li>
<li>
<p>é€šè¿‡å‡å°æ¢¯åº¦çš„å˜åŒ–ç¨‹åº¦ï¼Œä½¿å¾—æ¯æ¬¡çš„æ¢¯åº¦æ›´æ–°æ›´åŠ ç¨³å®šï¼Œä»è€Œä½¿æ¨¡å‹å­¦ä¹ æ›´åŠ ç¨³å®šï¼Œæ”¶æ•›é€Ÿåº¦æ›´å¿«ï¼Œå¹¶ä¸”è¿™ä¹Ÿå‡æ…¢äº†å¯¹äºè¾ƒå¤§æ¢¯åº¦å€¼çš„å‚æ•°çš„æ›´æ–°é€Ÿåº¦ï¼Œä¿è¯å…¶æ›´æ–°çš„ç¨³å®šæ€§ã€‚</p>
</li>
</ul>
<div class="admonition question">
<p class="admonition-title">Question 1.a.ii</p>
<p>Adamè¿˜é€šè¿‡è·Ÿè¸ªæ¢¯åº¦å¹³æ–¹çš„ç§»åŠ¨å¹³å‡å€¼ <span><span class="MathJax_Preview">v</span><script type="math/tex">v</script></span> æ¥ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡</p>
<div>
<div class="MathJax_Preview">
\begin{aligned} 
\mathbf{m} &amp; \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ 
\mathbf{v} &amp; \leftarrow \beta_{2} \mathbf{v}+\left(1-\beta_{2}\right)\left(\nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \odot \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta})\right) \\ 
\boldsymbol{\theta} &amp; \leftarrow \boldsymbol{\theta}-\alpha \odot \mathbf{m} / \sqrt{\mathbf{v}}  \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} 
\mathbf{m} & \leftarrow \beta_{1} \mathbf{m}+\left(1-\beta_{1}\right) \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \\ 
\mathbf{v} & \leftarrow \beta_{2} \mathbf{v}+\left(1-\beta_{2}\right)\left(\nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta}) \odot \nabla_{\boldsymbol{\theta}} J_{\text { minibatch }}(\boldsymbol{\theta})\right) \\ 
\boldsymbol{\theta} & \leftarrow \boldsymbol{\theta}-\alpha \odot \mathbf{m} / \sqrt{\mathbf{v}}  \end{aligned}
</script>
</div>
<p>å…¶ä¸­ï¼Œ<span><span class="MathJax_Preview">\odot, /</span><script type="math/tex">\odot, /</script></span> åˆ†åˆ«è¡¨ç¤ºé€å…ƒç´ çš„ä¹˜æ³•å’Œé™¤æ³•ï¼ˆæ‰€ä»¥ <span><span class="MathJax_Preview">z \odot z</span><script type="math/tex">z \odot z</script></span> æ˜¯é€å…ƒç´ çš„å¹³æ–¹ï¼‰ï¼Œ<span><span class="MathJax_Preview">\beta_2</span><script type="math/tex">\beta_2</script></span> æ˜¯ä¸€ä¸ª 0 å’Œ 1 ä¹‹é—´çš„è¶…å‚æ•°(é€šå¸¸è¢«è®¾ä¸º0.99)ã€‚å› ä¸ºAdamå°†æ›´æ–°é™¤ä»¥ <span><span class="MathJax_Preview">\sqrt v</span><script type="math/tex">\sqrt v</script></span> ï¼Œé‚£ä¹ˆå“ªä¸ªæ¨¡å‹å‚æ•°ä¼šå¾—åˆ°æ›´å¤§çš„æ›´æ–°ï¼Ÿä¸ºä»€ä¹ˆè¿™å¯¹å­¦ä¹ æœ‰å¸®åŠ©ï¼Ÿ</p>
</div>
<p><strong>Answer 1.a.ii</strong> : </p>
<ul>
<li>ç§»åŠ¨å¹³å‡æ¢¯åº¦æœ€å°çš„æ¨¡å‹å‚æ•°å°†å¾—åˆ°è¾ƒå¤§çš„æ›´æ–°ã€‚</li>
<li>ä¸€æ–¹é¢ï¼Œå°†æ¢¯åº¦è¾ƒå°çš„å‚æ•°çš„æ›´æ–°å˜å¤§ï¼Œå¸®åŠ©å…¶èµ°å‡ºå±€éƒ¨æœ€ä¼˜ç‚¹ï¼ˆéç‚¹ï¼‰ï¼›å¦ä¸€æ–¹é¢ï¼Œå°†æ¢¯åº¦è¾ƒå¤§çš„å‚æ•°çš„æ›´æ–°å˜å°ï¼Œä½¿å…¶æ›´æ–°æ›´åŠ ç¨³å®šã€‚ç»“åˆä»¥ä¸Šä¸¤ä¸ªæ–¹é¢ï¼Œä½¿å­¦ä¹ æ›´åŠ å¿«é€Ÿçš„åŒæ—¶ä¹Ÿæ›´åŠ ç¨³å®šã€‚</li>
</ul>
<h4 id="b-dropout">(b) Dropout<a class="headerlink" href="#b-dropout" title="Permanent link">&para;</a></h4>
<p><a href="https://www.cs.toronto.edu/Ëœhinton/absps/JMLRdropout.pdf">Dropout</a> æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼ŒDropout ä»¥ <span><span class="MathJax_Preview">p_{drop}</span><script type="math/tex">p_{drop}</script></span> çš„æ¦‚ç‡éšæœºè®¾ç½®éšè—å±‚ <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> ä¸­çš„ç¥ç»å…ƒä¸ºé›¶(æ¯ä¸ªminibatchä¸­ dropout ä¸åŒçš„ç¥ç»å…ƒ),ç„¶åå°† <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> ä¹˜ä»¥ä¸€ä¸ªå¸¸æ•° <span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> ã€‚æˆ‘ä»¬å¯ä»¥å†™ä¸º</p>
<div>
<div class="MathJax_Preview">
\mathbf{h}_{\mathrm{drop}}=\gamma \mathbf{d} \circ \mathbf{h}
</div>
<script type="math/tex; mode=display">
\mathbf{h}_{\mathrm{drop}}=\gamma \mathbf{d} \circ \mathbf{h}
</script>
</div>
<p>å…¶ä¸­ï¼Œ<span><span class="MathJax_Preview">d \in \{0,1\}^{D_h}</span><script type="math/tex">d \in \{0,1\}^{D_h}</script></span> ( <span><span class="MathJax_Preview">D_h</span><script type="math/tex">D_h</script></span> æ˜¯ <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> çš„å¤§å°)æ˜¯ä¸€ä¸ªæ©ç å‘é‡ï¼Œå…¶ä¸­æ¯ä¸ªæ¡ç›®éƒ½æ˜¯ä»¥ <span><span class="MathJax_Preview">p_{drop}</span><script type="math/tex">p_{drop}</script></span> çš„æ¦‚ç‡ä¸º 0 ï¼Œä»¥ <span><span class="MathJax_Preview">1 - p_{drop}</span><script type="math/tex">1 - p_{drop}</script></span> çš„æ¦‚ç‡ä¸º 1ã€‚<span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> æ˜¯ä½¿å¾— <span><span class="MathJax_Preview">h_{drop}</span><script type="math/tex">h_{drop}</script></span> çš„æœŸæœ›å€¼ä¸º <span><span class="MathJax_Preview">h</span><script type="math/tex">h</script></span> çš„å€¼</p>
<div>
<div class="MathJax_Preview">
\mathbb{E}_{p_{\text{drop}}}\left[\mathbf{h}_{\text{drop}}\right]_{i}=h_{i}, \text{for all } i \in \{1,\dots,D_h\}
</div>
<script type="math/tex; mode=display">
\mathbb{E}_{p_{\text{drop}}}\left[\mathbf{h}_{\text{drop}}\right]_{i}=h_{i}, \text{for all } i \in \{1,\dots,D_h\}
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question 1.b.i</p>
<p><span><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> å¿…é¡»ç­‰äºä»€ä¹ˆ(ç”¨ <span><span class="MathJax_Preview">p_{drop}</span><script type="math/tex">p_{drop}</script></span> è¡¨ç¤º) ï¼Ÿç®€å•è¯æ˜ä½ çš„ç­”æ¡ˆã€‚</p>
</div>
<p><strong>Answer 1.b.i</strong> : </p>
<div>
<div class="MathJax_Preview">
\gamma = \frac{1}{1 - p_{drop}} \\
</div>
<script type="math/tex; mode=display">
\gamma = \frac{1}{1 - p_{drop}} \\
</script>
</div>
<p>è¯æ˜å¦‚ä¸‹ï¼š</p>
<div>
<div class="MathJax_Preview">
\sum_i (1 -  p_{drop}) h_i = (1 -  p_{drop}) E[h] \\
\sum_i[h_{drop}]_i = \gamma\sum_i (1 -  p_{drop}) h_i = \gamma (1 -  p_{drop}) E[h] = E[h]
</div>
<script type="math/tex; mode=display">
\sum_i (1 -  p_{drop}) h_i = (1 -  p_{drop}) E[h] \\
\sum_i[h_{drop}]_i = \gamma\sum_i (1 -  p_{drop}) h_i = \gamma (1 -  p_{drop}) E[h] = E[h]
</script>
</div>
<div class="admonition question">
<p class="admonition-title">Question 1.b.ii</p>
<p>ä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥åªåœ¨è®­ç»ƒæ—¶ä½¿ç”¨ dropout è€Œåœ¨è¯„ä¼°æ—¶ä¸ä½¿ç”¨ï¼Ÿ</p>
</div>
<p><strong>Answer 1.b.ii</strong> : </p>
<p>å¦‚æœæˆ‘ä»¬åœ¨è¯„ä¼°æœŸé—´åº”ç”¨ dropout ï¼Œé‚£ä¹ˆè¯„ä¼°ç»“æœå°†ä¼šå…·æœ‰éšæœºæ€§ï¼Œå¹¶ä¸èƒ½ä½“ç°æ¨¡å‹çš„çœŸå®æ€§èƒ½ï¼Œè¿èƒŒäº†æ­£åˆ™åŒ–çš„åˆè¡·ã€‚é€šè¿‡åœ¨è¯„ä¼°æœŸé—´ç¦ç”¨ dropoutï¼Œä»è€Œè§‚å¯Ÿæ¨¡å‹çš„æ€§èƒ½ä¸æ­£åˆ™åŒ–çš„æ•ˆæœï¼Œä¿è¯æ¨¡å‹çš„å‚æ•°å¾—åˆ°æ­£ç¡®çš„æ›´æ–°ã€‚</p>
<h3 id="2-neural-transition-based-dependency-parsing">2. Neural Transition-Based Dependency Parsing<a class="headerlink" href="#2-neural-transition-based-dependency-parsing" title="Permanent link">&para;</a></h3>
<p>åœ¨æœ¬èŠ‚ä¸­ï¼Œæ‚¨å°†å®ç°ä¸€ä¸ªåŸºäºç¥ç»ç½‘ç»œçš„ä¾èµ–è§£æå™¨ï¼Œå…¶ç›®æ ‡æ˜¯åœ¨UAS(æœªæ ‡è®°ä¾å­˜è¯„åˆ†)æŒ‡æ ‡ä¸Šæœ€å¤§åŒ–æ€§èƒ½ã€‚</p>
<p>ä¾å­˜è§£æå™¨åˆ†æå¥å­çš„è¯­æ³•ç»“æ„ï¼Œåœ¨ head words å’Œ ä¿®é¥° head words çš„å•è¯ä¹‹é—´å»ºç«‹å…³ç³»ã€‚ä½ çš„å®ç°å°†æ˜¯ä¸€ä¸ªåŸºäºè½¬æ¢çš„è§£æå™¨ï¼Œå®ƒé€æ­¥æ„å»ºä¸€ä¸ªè§£æã€‚æ¯ä¸€æ­¥éƒ½ç»´æŠ¤ä¸€ä¸ªå±€éƒ¨è§£æï¼Œè¡¨ç¤ºå¦‚ä¸‹</p>
<ul>
<li>ä¸€ä¸ªå­˜å‚¨æ­£åœ¨è¢«å¤„ç†çš„å•è¯çš„ æ ˆ </li>
<li>ä¸€ä¸ªå­˜å‚¨å°šæœªå¤„ç†çš„å•è¯çš„ ç¼“å­˜</li>
<li>ä¸€ä¸ªè§£æå™¨é¢„æµ‹çš„ ä¾èµ– çš„åˆ—è¡¨</li>
</ul>
<p>æœ€åˆ,æ ˆåªåŒ…å« ROOT ï¼Œä¾èµ–é¡¹åˆ—è¡¨æ˜¯ç©ºçš„ï¼Œè€Œç¼“å­˜åˆ™åŒ…å«äº†è¿™ä¸ªå¥å­çš„æ‰€æœ‰å•è¯ã€‚åœ¨æ¯ä¸€ä¸ªæ­¥éª¤ä¸­,è§£æå™¨å°†å¯¹éƒ¨åˆ†è§£æä½¿ç”¨ä¸€ä¸ªè½¬æ¢,ç›´åˆ°å®ƒçš„é­‚æ‘æ˜¯ç©ºçš„ï¼Œå¹¶ä¸”æ ˆå¤§å°ä¸º1ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è½¬æ¢ï¼š</p>
<ul>
<li>SHIFTï¼šå°†bufferä¸­çš„ç¬¬ä¸€ä¸ªè¯ç§»å‡ºå¹¶æ”¾åˆ°stackä¸Šã€‚</li>
<li>LEFT-ARCï¼šå°†ç¬¬äºŒä¸ª(æœ€è¿‘æ·»åŠ çš„ç¬¬äºŒ)é¡¹æ ‡è®°ä¸ºæ ˆé¡¶å…ƒç´ çš„ä¾èµ–ï¼Œå¹¶ä»å †æ ˆä¸­åˆ é™¤ç¬¬äºŒé¡¹</li>
<li>RIGHT-ARCï¼šå°†ç¬¬ä¸€ä¸ª(æœ€è¿‘æ·»åŠ çš„ç¬¬ä¸€)é¡¹æ ‡è®°ä¸ºæ ˆä¸­ç¬¬äºŒé¡¹çš„ä¾èµ–ï¼Œå¹¶ä»å †æ ˆä¸­åˆ é™¤ç¬¬ä¸€é¡¹</li>
</ul>
<p>åœ¨æ¯ä¸ªæ­¥éª¤ä¸­ï¼Œè§£æå™¨å°†ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œåˆ†ç±»å™¨åœ¨ä¸‰ä¸ªè½¬æ¢ä¸­å†³å®šã€‚</p>
<div class="admonition question">
<p class="admonition-title">Question 2.a</p>
<p>æ±‚è§£è§£æå¥å­ â€œI parsed this sentence correctlyâ€ æ‰€éœ€çš„è½¬æ¢é¡ºåºã€‚è¿™å¥è¯çš„ä¾èµ–æ ‘å¦‚ä¸‹æ‰€ç¤ºã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œç»™å‡º stack å’Œ buffer çš„ç»“æ„ï¼Œä»¥åŠæœ¬æ­¥éª¤åº”ç”¨äº†ä»€ä¹ˆè½¬æ¢ï¼Œå¹¶æ·»åŠ æ–°çš„ä¾èµ–(å¦‚æœæœ‰çš„è¯)ã€‚ä¸‹é¢æä¾›äº†ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ã€‚</p>
<p><img alt="1560871900131" src="../imgs/1560871900131.png" /></p>
</div>
<p><strong>Answer 2.a</strong> : </p>
<table>
<thead>
<tr>
<th>Stack</th>
<th>Buffer</th>
<th>New dependency</th>
<th>Transition</th>
</tr>
</thead>
<tbody>
<tr>
<td>[ROOT]</td>
<td>[I, parsed, this, sentence, correctly]</td>
<td></td>
<td>Initial Conï¬guration</td>
</tr>
<tr>
<td>[ROOT, I]</td>
<td>[parsed, this, sentence, correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, I, parsed]</td>
<td>[this, sentence, correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed]</td>
<td>[this, sentence, correctly]</td>
<td>parsed <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> I</td>
<td>LEFT-ARC</td>
</tr>
<tr>
<td>[ROOT, parsed, this]</td>
<td>[sentence, correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed, this, sentence]</td>
<td>[correctly]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed, sentence]</td>
<td>[correctly]</td>
<td>sentence <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> this</td>
<td>LEFT-ARC</td>
</tr>
<tr>
<td>[ROOT, parsed]</td>
<td>[correctly]</td>
<td>parsed <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> sentence</td>
<td>RIGHT-ARC</td>
</tr>
<tr>
<td>[ROOT, parsed, correctly]</td>
<td>[]</td>
<td></td>
<td>SHIFT</td>
</tr>
<tr>
<td>[ROOT, parsed]</td>
<td>[]</td>
<td>parsed <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> correctly</td>
<td>RIGHT-ARC</td>
</tr>
<tr>
<td>[ROOT]</td>
<td>[]</td>
<td>ROOT <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> parsed</td>
<td>RIGHT-ARC</td>
</tr>
</tbody>
</table>
<div class="admonition question">
<p class="admonition-title">Question 2.b</p>
<p>ä¸€ä¸ªåŒ…å« <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> ä¸ªå•è¯çš„å¥å­éœ€è¦å¤šå°‘æ­¥(ç”¨ <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> è¡¨ç¤º)æ‰èƒ½è¢«è§£æï¼Ÿç®€è¦è§£é‡Šä¸ºä»€ä¹ˆã€‚</p>
</div>
<p><strong>Answer 2.b</strong> : </p>
<p>åŒ…å«<span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>ä¸ªå•è¯çš„å¥å­éœ€è¦ <span><span class="MathJax_Preview">2 \times n</span><script type="math/tex">2 \times n</script></span> æ­¥æ‰èƒ½å®Œæˆè§£æã€‚å› ä¸ºéœ€è¦è¿›è¡Œ <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> æ­¥çš„ <span><span class="MathJax_Preview">SHIFT</span><script type="math/tex">SHIFT</script></span> æ“ä½œå’Œ å…±è®¡$n æ­¥çš„ LEFT-ARC æˆ– RIGHT-ARC æ“ä½œï¼Œæ‰èƒ½å®Œæˆè§£æã€‚ï¼ˆæ¯ä¸ªå•è¯éƒ½éœ€è¦ä¸€æ¬¡SHIFTå’ŒARCçš„æ“ä½œï¼Œåˆå§‹åŒ–æ­¥éª¤ä¸è®¡ç®—åœ¨å†…ï¼‰</p>
<p><strong>Question 2.c</strong></p>
<p>å®ç°è§£æå™¨å°†ä½¿ç”¨çš„è½¬æ¢æœºåˆ¶</p>
<p><strong>Question 2.d</strong></p>
<p>æˆ‘ä»¬çš„ç½‘ç»œå°†é¢„æµ‹å“ªäº›è½¬æ¢åº”è¯¥åº”ç”¨äºéƒ¨åˆ†è§£æã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å®ƒæ¥è§£æä¸€ä¸ªå¥å­ï¼Œé€šè¿‡åº”ç”¨é¢„æµ‹å‡ºçš„è½¬æ¢æ“ä½œï¼Œç›´åˆ°è§£æå®Œæˆã€‚ç„¶è€Œï¼Œåœ¨å¯¹å¤§é‡æ•°æ®è¿›è¡Œé¢„æµ‹æ—¶ï¼Œç¥ç»ç½‘ç»œçš„è¿è¡Œé€Ÿåº¦è¦é«˜å¾—å¤š(å³åŒæ—¶é¢„æµ‹äº†å¯¹ä»»ä½•ä¸åŒéƒ¨åˆ†è§£æçš„ä¸‹ä¸€ä¸ªè½¬æ¢)ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„ç®—æ³•æ¥è§£æå°æ‰¹æ¬¡çš„å¥å­</p>
<p><img alt="1560906831993" src="../imgs/1560906831993.png" /></p>
<p>å®ç°minibatchçš„è§£æå™¨</p>
<p>æˆ‘ä»¬ç°åœ¨å°†è®­ç»ƒä¸€ä¸ªç¥ç»ç½‘ç»œæ¥é¢„æµ‹ï¼Œè€ƒè™‘åˆ°æ ˆã€ç¼“å­˜å’Œä¾èµ–é¡¹é›†åˆçš„çŠ¶æ€ï¼Œä¸‹ä¸€æ­¥åº”è¯¥åº”ç”¨å“ªä¸ªè½¬æ¢ã€‚é¦–å…ˆï¼Œæ¨¡å‹æå–äº†ä¸€ä¸ªè¡¨ç¤ºå½“å‰çŠ¶æ€çš„ç‰¹å¾å‘é‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŸç¥ç»ä¾èµ–è§£æè®ºæ–‡ä¸­çš„ç‰¹å¾é›†åˆï¼š<a href="http://cs.stanford.edu/people/danqi/papers/emnlp2014.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a>ã€‚è¿™ä¸ªç‰¹å¾å‘é‡ç”±æ ‡è®°åˆ—è¡¨(ä¾‹å¦‚åœ¨æ ˆä¸­çš„æœ€åä¸€ä¸ªè¯ï¼Œç¼“å­˜ä¸­çš„ç¬¬ä¸€ä¸ªè¯ï¼Œæ ˆä¸­ç¬¬äºŒåˆ°æœ€åä¸€ä¸ªå­—çš„ä¾èµ–(å¦‚æœæœ‰))ç»„æˆã€‚å®ƒä»¬å¯ä»¥è¢«è¡¨ç¤ºä¸ºæ•´æ•°çš„åˆ—è¡¨<span><span class="MathJax_Preview">[w_1,w_2,\dots,w_m]</span><script type="math/tex">[w_1,w_2,\dots,w_m]</script></span>ï¼Œmæ˜¯ç‰¹å¾çš„æ•°é‡ï¼Œæ¯ä¸ª <span><span class="MathJax_Preview">0 \leq w_i \lt |V|</span><script type="math/tex">0 \leq w_i \lt |V|</script></span> æ˜¯è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªtokençš„ç´¢å¼•(<span><span class="MathJax_Preview">| V |</span><script type="math/tex">| V |</script></span>æ˜¯è¯æ±‡é‡)ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çš„ç½‘ç»œæŸ¥æ‰¾æ¯ä¸ªå•è¯çš„åµŒå…¥ï¼Œå¹¶å°†å®ƒä»¬è¿æ¥æˆä¸€ä¸ªè¾“å…¥å‘é‡ï¼š</p>
<div>
<div class="MathJax_Preview">
\mathbf{x}=\left[\mathbf{E}_{w_{1}}, \dots, \mathbf{E}_{w_{m}}\right] \in \mathbb{R}^{d m}
</div>
<script type="math/tex; mode=display">
\mathbf{x}=\left[\mathbf{E}_{w_{1}}, \dots, \mathbf{E}_{w_{m}}\right] \in \mathbb{R}^{d m}
</script>
</div>
<p>å…¶ä¸­ <span><span class="MathJax_Preview">\mathbf{E} \in \mathbb{R}^{|V| \times d}</span><script type="math/tex">\mathbf{E} \in \mathbb{R}^{|V| \times d}</script></span> æ˜¯åµŒå…¥çŸ©é˜µï¼Œæ¯ä¸€è¡Œ <span><span class="MathJax_Preview">\mathbf{E}_w</span><script type="math/tex">\mathbf{E}_w</script></span> æ˜¯ä¸€ä¸ªç‰¹å®šçš„å•è¯ <span><span class="MathJax_Preview">w</span><script type="math/tex">w</script></span> çš„å‘é‡ã€‚æ¥ç€æˆ‘ä»¬å¯ä»¥è®¡ç®—æˆ‘ä»¬çš„é¢„æµ‹ï¼š</p>
<div>
<div class="MathJax_Preview">
\mathbf h = \text{ReLU}(\mathbf{xW+b_1}) \\
\mathbf l = \text{ReLU}(\mathbf{hU+b_2}) \\
\mathbf {\hat y} = \text{softmax}(l) 
</div>
<script type="math/tex; mode=display">
\mathbf h = \text{ReLU}(\mathbf{xW+b_1}) \\
\mathbf l = \text{ReLU}(\mathbf{hU+b_2}) \\
\mathbf {\hat y} = \text{softmax}(l) 
</script>
</div>
<p>å…¶ä¸­ï¼Œ <span><span class="MathJax_Preview">\mathbf{h}</span><script type="math/tex">\mathbf{h}</script></span> æŒ‡çš„æ˜¯éšè—å±‚ï¼Œ<span><span class="MathJax_Preview">\mathbf{l}</span><script type="math/tex">\mathbf{l}</script></span> æ˜¯å…¶åˆ†æ•°ï¼Œ<span><span class="MathJax_Preview">\mathbf{\hat y}</span><script type="math/tex">\mathbf{\hat y}</script></span> æŒ‡çš„æ˜¯é¢„æµ‹ç»“æœï¼Œ <span><span class="MathJax_Preview">\text{ReLU(z)}=max(z,0)</span><script type="math/tex">\text{ReLU(z)}=max(z,0)</script></span> ã€‚æˆ‘ä»¬ä½¿ç”¨æœ€å°åŒ–äº¤å‰ç†µæŸå¤±æ¥è®­ç»ƒæ¨¡å‹</p>
<div>
<div class="MathJax_Preview">
J(\theta) = CE(\mathbf y,\mathbf{\hat y}) = -\sum^3_{i=1}y_i\log\hat y_i
</div>
<script type="math/tex; mode=display">
J(\theta) = CE(\mathbf y,\mathbf{\hat y}) = -\sum^3_{i=1}y_i\log\hat y_i
</script>
</div>
<p>è®­ç»ƒé›†çš„æŸå¤±ä¸ºæ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ <span><span class="MathJax_Preview">J(\theta)</span><script type="math/tex">J(\theta)</script></span> çš„å¹³å‡å€¼ã€‚</p>
<p>Question 2.e</p>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<h2 id="reference_1">Reference<a class="headerlink" href="#reference_1" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="https://my.oschina.net/findbill/blog/535044">ä»SVDåˆ°PCAâ€”â€”å¥‡å¦™çš„æ•°å­¦æ¸¸æˆ</a></li>
</ul><h2 id="__comments">è¯„è®º</h2><div id="disqus_thread"></div><script>var disqus_config = function () {
      this.page.url = "https://looperxx.github.io/CS224n-2019-Assignment/";
      this.page.identifier =
        "CS224n-2019-Assignment/";
    };
    (function() {
      var d = document, s = d.createElement("script");
      s.src = "//https-looperxx-github-io-my-wiki.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();</script></article></div></div></main><footer class="md-footer"><div class="md-footer-nav"><nav class="md-footer-nav__inner md-grid"><a href="../CS224n-2019 ç®€ä»‹/" title="CS224n-2019ç®€ä»‹" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev"><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-back md-footer-nav__button"></i></div><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">åé€€</span>CS224n-2019ç®€ä»‹</span></div></a><a href="../CS224n-2019-01-Introduction and Word Vectors/" title="01 Introduction and Word Vectors" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next"><div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title"><span class="md-flex__ellipsis"><span class="md-footer-nav__direction">å‰è¿›</span>01 Introduction and Word Vectors</span></div><div class="md-flex__cell md-flex__cell--shrink"><i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i></div></a></nav></div><div class="md-footer-meta md-typeset"><div class="md-footer-meta__inner md-grid"><div class="md-footer-copyright"><div class="md-footer-copyright__highlight">Copyright &copy; 2019 - 2020 Looper Xiao Xu</div>powered by <a href="https://www.mkdocs.org">MkDocs</a> and <a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a></div><div class="md-footer-social"><link rel="stylesheet" href="../assets/fonts/font-awesome.css"> <a href="https://github.com/looperXX" class="md-footer-social__link fa fa-github"></a>  <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" class="md-footer-social__link fa fa-linkedin"></a> </div></div></div></footer></div><script src="../assets/javascripts/application.b260a35d.js"></script><script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script><script>app.initialize({version:"1.0.4",url:{base:".."}})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script><script src="../js/baidu-tongji.js"></script></body></html>