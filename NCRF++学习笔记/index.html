


<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LooperXX's homepage">
      
      
      
        <meta name="author" content="Looper - Xiao Xu">
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1, mkdocs-material-5.1.1">
    
    
      
        <title>NCRF++ - Science is interesting.</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.a676eddb.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
        
<link rel="preconnect dns-prefetch" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-164217558-1","auto"),ga("set","anonymizeIp",!0),ga("send","pageview"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){if(this.value){var e=document.location.pathname;ga("send","pageview",e+"?q="+this.value)}})}),document.addEventListener("DOMContentSwitch",function(){ga("send","pageview")})</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
  
    <body dir="">
  
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ncrf" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="">
    <a href=".." title="Science is interesting." class="md-header-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,6H21V8H3V6M3,11H21V13H3V11M3,16H21V18H3V16Z" /></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Science is interesting.
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              NCRF++
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5,3A6.5,6.5 0 0,1 16,9.5C16,11.11 15.41,12.59 14.44,13.73L14.71,14H15.5L20.5,19L19,20.5L14,15.5V14.71L13.73,14.44C12.59,15.41 11.11,16 9.5,16A6.5,6.5 0 0,1 3,9.5A6.5,6.5 0 0,1 9.5,3M9.5,5C7,5 5,7 5,9.5C5,12 7,14 9.5,14C12,14 14,12 14,9.5C14,7 12,5 9.5,5Z" /></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z" /></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            键入以开始搜索
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
          

  

<nav class="md-tabs md-tabs--active" aria-label="" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." class="md-tabs__link">
        Home
      </a>
    
  </li>

      
        
  
  
    
    
  
  
    <li class="md-tabs__item">
      
        <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" class="md-tabs__link">
          NLP
        </a>
      
    </li>
  

  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../Normalization/" class="md-tabs__link">
          ML & DL
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-tabs__link">
          CS224n学习笔记
        </a>
      
    </li>
  

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../MkDocs_demo/" class="md-tabs__link">
          For MkDocs
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Science is interesting." class="md-nav__button md-logo" aria-label="Science is interesting.">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12,8A3,3 0 0,0 15,5A3,3 0 0,0 12,2A3,3 0 0,0 9,5A3,3 0 0,0 12,8M12,11.54C9.64,9.35 6.5,8 3,8V19C6.5,19 9.64,20.35 12,22.54C14.36,20.35 17.5,19 21,19V8C17.5,8 14.36,9.35 12,11.54Z" /></svg>

    </a>
    Science is interesting.
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      NLP
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="NLP" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        NLP
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-1" type="checkbox" id="nav-2-1">
    
    <label class="md-nav__link" for="nav-2-1">
      理论笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="理论笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-1">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        理论笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B/" title="自然语言处理简介" class="md-nav__link">
      自然语言处理简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%E7%9A%84%E5%B7%A8%E4%BA%BA%E8%82%A9%E8%86%80/" title="NLP的巨人肩膀" class="md-nav__link">
      NLP的巨人肩膀
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Attention/" title="Attention" class="md-nav__link">
      Attention
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-2" type="checkbox" id="nav-2-2" checked>
    
    <label class="md-nav__link" for="nav-2-2">
      代码学习
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="代码学习" data-md-level="2">
      <label class="md-nav__title" for="nav-2-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        代码学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        NCRF++
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3,9H17V7H3V9M3,13H17V11H3V13M3,17H17V15H3V17M19,17H21V15H19V17M19,7V9H21V7H19M19,13H21V11H19V13Z" /></svg>
        </span>
      </label>
    
    <a href="./" title="NCRF++" class="md-nav__link md-nav__link--active">
      NCRF++
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    文档学习
  </a>
  
    <nav class="md-nav" aria-label="文档学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#readmemd" class="md-nav__link">
    Readme.md
  </a>
  
    <nav class="md-nav" aria-label="Readme.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#requirement" class="md-nav__link">
    Requirement
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages" class="md-nav__link">
    Advantages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    Usage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-format" class="md-nav__link">
    Data Format
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance" class="md-nav__link">
    Performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add-handcrafted-features" class="md-nav__link">
    Add Handcrafted Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speed" class="md-nav__link">
    Speed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#n-best-decoding" class="md-nav__link">
    N best Decoding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reproduce-paper-results-and-hyperparameter-tuning" class="md-nav__link">
    Reproduce Paper Results and Hyperparameter Tuning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configurationmd" class="md-nav__link">
    Configuration.md
  </a>
  
    <nav class="md-nav" aria-label="Configuration.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#io" class="md-nav__link">
    I/O
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networkconfiguration" class="md-nav__link">
    NetworkConfiguration
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trainingsetting" class="md-nav__link">
    TrainingSetting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparameters" class="md-nav__link">
    Hyperparameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extensionmd" class="md-nav__link">
    Extension.md
  </a>
  
    <nav class="md-nav" aria-label="Extension.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#module-extension" class="md-nav__link">
    Module Extension.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparameter_tuningmd" class="md-nav__link">
    Hyperparameter_tuning.md
  </a>
  
    <nav class="md-nav" aria-label="Hyperparameter_tuning.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperparamter-tuning-on-conll-2003-english-ner-task" class="md-nav__link">
    Hyperparamter tuning on CoNLL 2003 English NER task
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    源码学习
  </a>
  
    <nav class="md-nav" aria-label="源码学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    架构梳理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    流程梳理
  </a>
  
    <nav class="md-nav" aria-label="流程梳理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#crf" class="md-nav__link">
    CRF 笔记
  </a>
  
    <nav class="md-nav" aria-label="CRF 笔记">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#crf_1" class="md-nav__link">
    什么样的问题需要CRF模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    随机场，马尔科夫随机场，条件随机场
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-crf" class="md-nav__link">
    Softmax 与 CRF
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    公式推导
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-api" class="md-nav__link">
    Pytorch API
  </a>
  
    <nav class="md-nav" aria-label="Pytorch API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gather" class="md-nav__link">
    gather
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scatter" class="md-nav__link">
    scatter
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked_select" class="md-nav__link">
    masked_select
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked_scatter_" class="md-nav__link">
    masked_scatter_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked_fill_" class="md-nav__link">
    masked_fill_
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crf_2" class="md-nav__link">
    CRF 源码解析
  </a>
  
    <nav class="md-nav" aria-label="CRF 源码解析">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_calculate_pzself-feats-mask" class="md-nav__link">
    _calculate_PZ(self, feats, mask)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_score_sentenceself-scores-mask-tags" class="md-nav__link">
    _score_sentence(self, scores, mask, tags)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_viterbi_decodeself-feats-mask" class="md-nav__link">
    _viterbi_decode(self, feats, mask)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_viterbi_decode_nbestself-feats-mask-nbest" class="md-nav__link">
    _viterbi_decode_nbest(self, feats, mask, nbest)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
      </ul>
    </nav>
  </li>

        
          
          
          


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2-3" type="checkbox" id="nav-2-3">
    
    <label class="md-nav__link" for="nav-2-3">
      书籍笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="书籍笔记" data-md-level="2">
      <label class="md-nav__title" for="nav-2-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        书籍笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" title="Machine Reading Comprehension" class="md-nav__link">
      Machine Reading Comprehension
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NLP%20Concepts/" title="Some Concepts" class="md-nav__link">
      Some Concepts
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../NNDL%20%E4%B9%A0%E9%A2%98/" title="NNDL 习题" class="md-nav__link">
      NNDL 习题
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      ML & DL
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="ML & DL" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        ML & DL
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../Normalization/" title="Normalization" class="md-nav__link">
      Normalization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../%E8%8A%B1%E4%B9%A6%E7%BB%8F%E9%AA%8C%E6%B3%95%E5%88%99/" title="花书经验法则" class="md-nav__link">
      花书经验法则
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4">
    
    <label class="md-nav__link" for="nav-4">
      CS224n学习笔记
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="CS224n学习笔记" data-md-level="1">
      <label class="md-nav__title" for="nav-4">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        CS224n学习笔记
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" title="CS224n-2019简介" class="md-nav__link">
      CS224n-2019简介
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-Assignment/" title="CS224n-2019作业笔记" class="md-nav__link">
      CS224n-2019作业笔记
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-01-Introduction%20and%20Word%20Vectors/" title="01 Introduction and Word Vectors" class="md-nav__link">
      01 Introduction and Word Vectors
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" title="02 Word Vectors 2 and Word Senses" class="md-nav__link">
      02 Word Vectors 2 and Word Senses
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/" title="03 Word Window Classification,Neural Networks, and Matrix Calculus" class="md-nav__link">
      03 Word Window Classification,Neural Networks, and Matrix Calculus
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" title="04 Backpropagation and Computation Graphs" class="md-nav__link">
      04 Backpropagation and Computation Graphs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" title="05 Linguistic Structure Dependency Parsing" class="md-nav__link">
      05 Linguistic Structure Dependency Parsing
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" title="06 The probability of a sentence Recurrent Neural Networks and Language Models" class="md-nav__link">
      06 The probability of a sentence Recurrent Neural Networks and Language Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" title="07 Vanishing Gradients and Fancy RNNs" class="md-nav__link">
      07 Vanishing Gradients and Fancy RNNs
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/" title="08 Machine Translation, Sequence-to-sequence and Attention" class="md-nav__link">
      08 Machine Translation, Sequence-to-sequence and Attention
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" title="09 Practical Tips for Final Projects" class="md-nav__link">
      09 Practical Tips for Final Projects
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/" title="10 Question Answering and the Default Final Project" class="md-nav__link">
      10 Question Answering and the Default Final Project
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" title="11 ConvNets for NLP" class="md-nav__link">
      11 ConvNets for NLP
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/" title="12 Information from parts of words Subword Models" class="md-nav__link">
      12 Information from parts of words Subword Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/" title="13 Modeling contexts of use Contextual Representations and Pretraining" class="md-nav__link">
      13 Modeling contexts of use Contextual Representations and Pretraining
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/" title="14 Transformers and Self-Attention For Generative Models" class="md-nav__link">
      14 Transformers and Self-Attention For Generative Models
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-15-Natural%20Language%20Generation/" title="15 Natural Language Generation" class="md-nav__link">
      15 Natural Language Generation
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-16-Coreference%20Resolution/" title="16 Coreference Resolution" class="md-nav__link">
      16 Coreference Resolution
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-17-Multitask%20Learning/" title="17 Multitask Learning" class="md-nav__link">
      17 Multitask Learning
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/" title="18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment" class="md-nav__link">
      18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/" title="19 Safety, Bias, and Fairness" class="md-nav__link">
      19 Safety, Bias, and Fairness
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/" title="20 The Future of NLP + Deep Learning" class="md-nav__link">
      20 The Future of NLP + Deep Learning
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-5" type="checkbox" id="nav-5">
    
    <label class="md-nav__link" for="nav-5">
      For MkDocs
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59,16.58L13.17,12L8.59,7.41L10,6L16,12L10,18L8.59,16.58Z" /></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="For MkDocs" data-md-level="1">
      <label class="md-nav__title" for="nav-5">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
        </span>
        For MkDocs
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../MkDocs_demo/" title="Demo" class="md-nav__link">
      Demo
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Material%20Theme%20Tutorial/" title="Material Theme Tutorial" class="md-nav__link">
      Material Theme Tutorial
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    文档学习
  </a>
  
    <nav class="md-nav" aria-label="文档学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#readmemd" class="md-nav__link">
    Readme.md
  </a>
  
    <nav class="md-nav" aria-label="Readme.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#requirement" class="md-nav__link">
    Requirement
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages" class="md-nav__link">
    Advantages
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    Usage
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-format" class="md-nav__link">
    Data Format
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance" class="md-nav__link">
    Performance
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add-handcrafted-features" class="md-nav__link">
    Add Handcrafted Features
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#speed" class="md-nav__link">
    Speed
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#n-best-decoding" class="md-nav__link">
    N best Decoding
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reproduce-paper-results-and-hyperparameter-tuning" class="md-nav__link">
    Reproduce Paper Results and Hyperparameter Tuning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configurationmd" class="md-nav__link">
    Configuration.md
  </a>
  
    <nav class="md-nav" aria-label="Configuration.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#io" class="md-nav__link">
    I/O
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#networkconfiguration" class="md-nav__link">
    NetworkConfiguration
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#trainingsetting" class="md-nav__link">
    TrainingSetting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparameters" class="md-nav__link">
    Hyperparameters
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extensionmd" class="md-nav__link">
    Extension.md
  </a>
  
    <nav class="md-nav" aria-label="Extension.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#module-extension" class="md-nav__link">
    Module Extension.
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hyperparameter_tuningmd" class="md-nav__link">
    Hyperparameter_tuning.md
  </a>
  
    <nav class="md-nav" aria-label="Hyperparameter_tuning.md">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hyperparamter-tuning-on-conll-2003-english-ner-task" class="md-nav__link">
    Hyperparamter tuning on CoNLL 2003 English NER task
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    源码学习
  </a>
  
    <nav class="md-nav" aria-label="源码学习">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    架构梳理
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    流程梳理
  </a>
  
    <nav class="md-nav" aria-label="流程梳理">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#crf" class="md-nav__link">
    CRF 笔记
  </a>
  
    <nav class="md-nav" aria-label="CRF 笔记">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#crf_1" class="md-nav__link">
    什么样的问题需要CRF模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    随机场，马尔科夫随机场，条件随机场
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-crf" class="md-nav__link">
    Softmax 与 CRF
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    公式推导
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pytorch-api" class="md-nav__link">
    Pytorch API
  </a>
  
    <nav class="md-nav" aria-label="Pytorch API">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gather" class="md-nav__link">
    gather
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scatter" class="md-nav__link">
    scatter
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked_select" class="md-nav__link">
    masked_select
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked_scatter_" class="md-nav__link">
    masked_scatter_
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked_fill_" class="md-nav__link">
    masked_fill_
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#crf_2" class="md-nav__link">
    CRF 源码解析
  </a>
  
    <nav class="md-nav" aria-label="CRF 源码解析">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_calculate_pzself-feats-mask" class="md-nav__link">
    _calculate_PZ(self, feats, mask)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_score_sentenceself-scores-mask-tags" class="md-nav__link">
    _score_sentence(self, scores, mask, tags)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_viterbi_decodeself-feats-mask" class="md-nav__link">
    _viterbi_decode(self, feats, mask)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_viterbi_decode_nbestself-feats-mask-nbest" class="md-nav__link">
    _viterbi_decode_nbest(self, feats, mask, nbest)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reference" class="md-nav__link">
    Reference
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/LooperXX/LooperXX.github.io/edit/master/docs/NCRF++学习笔记.md" title="编辑此页" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71,7.04C21.1,6.65 21.1,6 20.71,5.63L18.37,3.29C18,2.9 17.35,2.9 16.96,3.29L15.12,5.12L18.87,8.87M3,17.25V21H6.75L17.81,9.93L14.06,6.18L3,17.25Z" /></svg>
                  </a>
                
                
                  
                
                
                <h1 id="ncrf">NCRF++学习笔记<a class="headerlink" href="#ncrf" title="Permanent link">&para;</a></h1>
<h2 id="_1">文档学习<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<h3 id="readmemd">Readme.md<a class="headerlink" href="#readmemd" title="Permanent link">&para;</a></h3>
<h4 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h4>
<p>序列标记模型在许多NLP任务中非常流行，例如命名实体识别（NER），词性（POS）标记和分词。最先进的序列标记模型主要利用具有输入字特征的CRF结构。 LSTM（或双向LSTM）是序列标记任务中流行的基于深度学习的特征提取器。并且由于更快的计算，也可以使用CNN。此外，单词中的特征对于表示单词也是有用的，可以通过字符LSTM或字符CNN结构或人类定义的神经特征来捕获单词。</p>
<p>NCRF ++是一个基于PyTorch的框架，可灵活选择输入特征和输出结构。使用NCRF ++的神经序列标记模型的设计可通过配置文件完全配置，该配置文件不需要任何代码工作。 NCRF ++可以被视为 <a href="http://taku910.github.io/crfpp/">CRF++</a> 的神经网络版本，这是一个著名的统计CRF框架。</p>
<p>该框架已被 <a href="https://arxiv.org/abs/1806.05626">ACL 2018</a> 接受为 demo 论文。使用NCRF ++的详细实验报告和分析已被 <a href="https://arxiv.org/abs/1806.04470">COLING 2018</a> 接受为最佳论文。</p>
<p>NCRF ++支持三个级别的不同结构组合：character sequence layer; word sequence layer and inference layer. </p>
<ul>
<li>字符序列表示：字符LSTM，字符GRU，字符CNN和手工制作的单词特征。</li>
<li>单词序列表示：单词LSTM，单词GRU，单词CNN。</li>
<li>推理层：Softmax，CRF。</li>
</ul>
<h4 id="requirement">Requirement<a class="headerlink" href="#requirement" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>Python: 2 or 3  
PyTorch: 1.0 
</code></pre></div>

<p><a href="https://github.com/jiesutd/NCRFpp/tree/PyTorch0.3">PyTorch 0.3 compatible version is here.</a></p>
<h4 id="advantages">Advantages<a class="headerlink" href="#advantages" title="Permanent link">&para;</a></h4>
<ul>
<li>完全可配置：可以使用配置文件设置所有神经模型结构。</li>
<li>最先进的系统性能：与最先进的模型相比，基于NCRF ++的模型可以提供相当或更好的结果。</li>
<li>灵活的特征：用户可以定义自己的特征和预训练的特征嵌入。</li>
<li>
<p>快速运行速度：NCRF ++利用完全批量操作，在GPU的帮助下使系统高效（&gt; 1000sent / s用于训练，&gt; 2000sents / s用于解码）</p>
</li>
<li>
<p>N best output: NCRF++ 支持 <code>nbest</code> decoding (with their probabilities).</p>
</li>
</ul>
<h4 id="usage">Usage<a class="headerlink" href="#usage" title="Permanent link">&para;</a></h4>
<p>NCRF ++支持通过配置文件设计神经网络结构。该程序可以运行两种状态 <strong>训练和解码</strong>。 （示例配置和数据已包含在此存储库中）</p>
<p>In <strong><em>training</em></strong> status: <code>python main.py --config demo.train.config</code></p>
<p>In <strong><em>decoding</em></strong> status: <code>python main.py --config demo.decode.config</code></p>
<p>配置文件控制网络结构，I / O，训练设置和超参数。</p>
<p><strong><em>Detail configurations and explanations are listed <a href="https://github.com/jiesutd/NCRFpp/blob/master/readme/Configuration.md">here</a>.</em></strong></p>
<p>NCRF ++设计为三层（如下所示）：字符序列层;单词序列层和推理层。通过使用配置文件，可以轻松复制大多数最先进的模型而无需编码。另一方面，用户可以通过设计自己的模块来扩展每一层（例如，他们可能想要设计除CNN / LSTM / GRU之外的其他神经结构）。我们的层设计使模块扩展方便，模块扩展的指令可以在 <a href="https://github.com/jiesutd/NCRFpp/blob/master/readme/Extension.md">这里</a> 找到。</p>
<p><img alt="alt text" src="../imgs/architecture.png" /></p>
<ul>
<li>图中的红色圆圈指word embedding，黄色指 charCNN/RNN 生成的 word embedding，灰色是自定义的 handcrafted 特征。</li>
</ul>
<h4 id="data-format">Data Format<a class="headerlink" href="#data-format" title="Permanent link">&para;</a></h4>
<ul>
<li>You can refer the data format in <a href="https://github.com/jiesutd/NCRFpp/blob/master/sample_data">sample_data</a>.</li>
<li>NCRF++ supports both BIO and BIOES(BMES) tag scheme.</li>
<li>Notice that IOB format (<strong><em>different</em></strong> from BIO) is currently not supported, because this tag scheme is old and works worse than other schemes <a href="https://arxiv.org/pdf/1707.06799.pdf">Reimers and Gurevych, 2017</a>.</li>
<li>The difference among these three tag schemes is explained in this <a href="https://arxiv.org/pdf/1707.06799.pdf">paper</a>.</li>
<li>I have written a <a href="https://github.com/jiesutd/NCRFpp/blob/master/utils/tagSchemeConverter.py">script</a> which converts the tag scheme among IOB/BIO/BIOES. Welcome to have a try.</li>
</ul>
<h4 id="performance">Performance<a class="headerlink" href="#performance" title="Permanent link">&para;</a></h4>
<p>Results on CONLL 2003 English NER task are better or comparable with SOTA results with the same structures.</p>
<p>CharLSTM+WordLSTM+CRF: 91.20 vs 90.94 of <a href="http://www.aclweb.org/anthology/N/N16/N16-1030.pdf">Lample .etc, NAACL16</a>;</p>
<p>CharCNN+WordLSTM+CRF: 91.35 vs 91.21 of <a href="http://www.aclweb.org/anthology/P/P16/P16-1101.pdf">Ma .etc, ACL16</a>.</p>
<p>By default, <code>LSTM</code> is bidirectional LSTM.</p>
<table>
<thead>
<tr>
<th>ID</th>
<th>Model</th>
<th>Nochar</th>
<th>CharLSTM</th>
<th>CharCNN</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>WordLSTM</td>
<td>88.57</td>
<td>90.84</td>
<td>90.73</td>
</tr>
<tr>
<td>2</td>
<td>WordLSTM+CRF</td>
<td>89.45</td>
<td><strong>91.20</strong></td>
<td><strong>91.35</strong></td>
</tr>
<tr>
<td>3</td>
<td>WordCNN</td>
<td>88.56</td>
<td>90.46</td>
<td>90.30</td>
</tr>
<tr>
<td>4</td>
<td>WordCNN+CRF</td>
<td>88.90</td>
<td>90.70</td>
<td>90.43</td>
</tr>
</tbody>
</table>
<p>We have compared twelve neural sequence labeling models (<code>{charLSTM, charCNN, None} x {wordLSTM, wordCNN} x {softmax, CRF}</code>) on three benchmarks (POS, Chunking, NER) under statistical experiments, detail results and comparisons can be found in our COLING 2018 paper <a href="https://arxiv.org/abs/1806.04470">Design Challenges and Misconceptions in Neural Sequence Labeling</a>.</p>
<h4 id="add-handcrafted-features">Add Handcrafted Features<a class="headerlink" href="#add-handcrafted-features" title="Permanent link">&para;</a></h4>
<p>NCRF++ 集成了最先进的几个神经特征序列特征提取器：CNN (<a href="http://www.aclweb.org/anthology/P/P16/P16-1101.pdf">Ma .etc, ACL16</a>), LSTM (<a href="http://www.aclweb.org/anthology/N/N16/N16-1030.pdf">Lample .etc, NAACL16</a>) and GRU (<a href="https://arxiv.org/pdf/1703.06345.pdf">Yang .etc, ICLR17</a>). 此外，手工制作的特征已被证明在序列标记任务中很重要。 NCRF ++允许用户设计自己的特征，如大写，POS标签或任何其他特征（上图中的灰色圆圈）。用户可以通过配置文件配置自定义特征（特征嵌入大小，预训练特征嵌入等）。样本输入数据格式在 <a href="https://github.com/jiesutd/NCRFpp/blob/master/sample_data/train.cappos.bmes">train.cappos.bmes</a> 中，其中包括两个人为定义的特征[POS]和[Cap]（[POS]和[Cap]是两个示例，您可以为您的特征提供所需的任何名称，只需按照格式[xx]并在配置文件中配置相同名称的特征。）用户可以配置配置文件中的每个特征，通过使用</p>
<div class="highlight"><pre><span></span><code>feature=[POS] emb_size=20 emb_dir=%your_pretrained_POS_embedding
feature=[Cap] emb_size=20 emb_dir=%your_pretrained_Cap_embedding
</code></pre></div>

<p>没有预训练嵌入的特征将被随机初始化。</p>
<h4 id="speed">Speed<a class="headerlink" href="#speed" title="Permanent link">&para;</a></h4>
<p>NCRF ++使用完全批量计算实现，使其在模型训练和解码方面都非常有效。在GPU（Nvidia GTX 1080）和大批量大小的帮助下，使用NCRF ++构建的LSTMCRF模型分别在训练和解码状态下可达到1000个sents / s和2000个sents / s。</p>
<p><img alt="alt text" src="../imgs/speed.png" /></p>
<h4 id="n-best-decoding">N best Decoding<a class="headerlink" href="#n-best-decoding" title="Permanent link">&para;</a></h4>
<p>传统的CRF结构仅解码具有最大可能性的一个标签序列（即1个最佳输出）。而NCRF ++可以提供大量选择，它可以解码具有 top n 概率的n个标签序列（即n-best output）。nbest 解码已得到几个流行的 <strong>统计</strong> CRF框架的支持。然而据我们所知，NCRF ++是神经CRF模型中唯一支持nbest解码的工具包。</p>
<p>在我们的实现中，当 nbest = 10 时，在NCRF ++中构建的CharCNN + WordLSTM + CRF模型在CoNLL 2003 NER任务上可以给出97.47％的oracle F1值（当nbest = 1时F1 = 91.35％）。</p>
<p><img alt="alt text" src="../imgs/nbest.png" /></p>
<h4 id="reproduce-paper-results-and-hyperparameter-tuning">Reproduce Paper Results and Hyperparameter Tuning<a class="headerlink" href="#reproduce-paper-results-and-hyperparameter-tuning" title="Permanent link">&para;</a></h4>
<p>To reproduce the results in our COLING 2018 paper, you only need to set the <code>iteration=1</code> as <code>iteration=100</code> in configuration file <code>demo.train.config</code> and configure your file directory in this configuration file. The default configuration file describes the <code>Char CNN + Word LSTM + CRF</code> model, you can build your own model by modifing the configuration accordingly. The parameters in this demo configuration file are the same in our paper. (Notice the <code>Word CNN</code> related models need slightly different parameters, details can be found in our COLING paper.)</p>
<p>If you want to use this framework in new tasks or datasets, here are some tuning <a href="https://github.com/jiesutd/NCRFpp/blob/master/readme/hyperparameter_tuning.md">tips</a> by @Victor0118.</p>
<h3 id="configurationmd">Configuration.md<a class="headerlink" href="#configurationmd" title="Permanent link">&para;</a></h3>
<p>本文档与 <code>demo.train.config</code> 相对应</p>
<h4 id="io">I/O<a class="headerlink" href="#io" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>指令</th>
<th>解释</th>
<th align="left">备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>train_dir=xx</td>
<td>string (necessary in training). Set training file directory.</td>
<td align="left">训练集位置</td>
</tr>
<tr>
<td>dev_dir=xx</td>
<td>string (necessary in training). Set dev file directory.</td>
<td align="left">开发集位置</td>
</tr>
<tr>
<td>test_dir=xx</td>
<td>string . Set test file directory.</td>
<td align="left">测试集位置</td>
</tr>
<tr>
<td>model_dir=xx</td>
<td>string (optional). Set saved model file directory.</td>
<td align="left">输出 模型存储位置 <code>model_dir=lstmcrf</code> 则 model 保存为<code>lstm.0.model</code> ，相关参数词典为<code>lstm.dset</code></td>
</tr>
<tr>
<td>word_emb_dir=xx</td>
<td>string (optional). Set pretrained word embedding file directory.</td>
<td align="left">预训练词嵌入的位置</td>
</tr>
<tr>
<td>raw_dir=xx</td>
<td>string (optional). Set input raw file directory.</td>
<td align="left">原始数据文件的位置</td>
</tr>
<tr>
<td>decode_dir=xx</td>
<td>string (necessary in decoding). Set decoded file directory.</td>
<td align="left">输出 解码结果的位置</td>
</tr>
<tr>
<td>dset_dir=xx</td>
<td>string (necessary). Set saved model file directory.</td>
<td align="left">模型参数词典保存的位置</td>
</tr>
<tr>
<td>load_model_dir=xx</td>
<td>string (necessary in decoding). Set loaded model file directory. (when decoding)</td>
<td align="left">模型的位置</td>
</tr>
<tr>
<td>char_emb_dir=xx</td>
<td>string (optional). Set pretrained character embedding file directory.</td>
<td align="left">预训练的字符嵌入文件的位置</td>
</tr>
<tr>
<td>norm_word_emb=False</td>
<td>boolen. If normalize the pretrained word embedding.</td>
<td align="left">是否对预训练的词嵌入标准化</td>
</tr>
<tr>
<td>norm_char_emb=False</td>
<td>boolen. If normalize the pretrained character embedding.</td>
<td align="left">是否对预训练的字符嵌入标准化</td>
</tr>
<tr>
<td>number_normalized=True</td>
<td>boolen. If normalize the digit into <code>0</code> for input files.</td>
<td align="left">是否将数字标准化为输入文件的“0”</td>
</tr>
<tr>
<td>seg=True</td>
<td>boolen. If task is segmentation like, tasks with token accuracy evaluation (e.g. POS, CCG) is False; tasks with F-value evaluation(e.g. Word Segmentation, NER, Chunking) is True .</td>
<td align="left">如果任务是 segmentation，令牌准确度评估的任务（例如POS，CCG）为 <code>False</code> ; F值评估的任务（例如，Word Segmentation, NER, Chunking）为 <code>True</code> 。</td>
</tr>
<tr>
<td>word_emb_dim=50</td>
<td>int. Word embedding dimension, if model use pretrained word embedding, word_emb_dim will be reset as the same dimension as pretrained embedidng.</td>
<td align="left">词嵌入维度 (如果模型使用预训练单词嵌入，<code>word_emb_dim</code>将被重置为与预训练<code>embedidng</code>相同的维度)</td>
</tr>
<tr>
<td>char_emb_dim=30</td>
<td>int. Character embedding dimension, if model use pretrained character embedding, char_emb_dim will be reset as the same dimension as pretrained embedidng.</td>
<td align="left">字符嵌入维度 (如果模型使用预训练单词嵌入，<code>char_emb_dim</code>将被重置为与预训练<code>embedidng</code>相同的维度)</td>
</tr>
</tbody>
</table>
<h4 id="networkconfiguration">NetworkConfiguration<a class="headerlink" href="#networkconfiguration" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>指令</th>
<th>解释</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>use_crf=True</td>
<td>boolen (necessary in training). Flag of if using CRF layer. If it is set as False, then Softmax is used in inference layer.</td>
<td><code>True</code> : CRF; <code>False</code> : Softmax</td>
</tr>
<tr>
<td>use_char=True</td>
<td>boolen (necessary in training). Flag of if using character sequence layer.</td>
<td><code>True</code> : 使用字符序列层</td>
</tr>
<tr>
<td>word_seq_feature=XX</td>
<td>boolen (necessary in training): CNN/LSTM/GRU. Neural structure selection for word sequence.</td>
<td>词序列特征提取模型：CNN/LSTM/GRU</td>
</tr>
<tr>
<td>char_seq_feature=CNN</td>
<td>boolen (necessary in training): CNN/LSTM/GRU. Neural structure selection for character sequence, it only be used when use_char=True.</td>
<td>字符特征提取模型：CNN/LSTM/GRU</td>
</tr>
<tr>
<td>feature=[POS] emb_size=20 emb_dir=xx emb_norm=false</td>
<td>feature configuration. It includes the feature prefix [POS], pretrained feature embedding file and the embedding size.</td>
<td>特征配置：<code>feature=[特征名称] emb_size=20 emb_dir=xx emb_norm=false</code></td>
</tr>
<tr>
<td>feature=[Cap] emb_size=20 emb_dir=xx emb_norm=false</td>
<td>feature configuration. Another feature [Cap].</td>
<td>同上</td>
</tr>
<tr>
<td>nbest=1</td>
<td>int (necessary in decoding). Set the nbest size during decoding.</td>
<td>设置 nbest 的 size</td>
</tr>
</tbody>
</table>
<h4 id="trainingsetting">TrainingSetting<a class="headerlink" href="#trainingsetting" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>指令</th>
<th>解释</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>status=train</td>
<td>string: train or decode. Set the program running in training or decoding mode.</td>
<td>train / decode 模式</td>
</tr>
<tr>
<td>optimizer=SGD</td>
<td>string: SGD/Adagrad/AdaDelta/RMSprop/Adam. optimizer selection.</td>
<td>优化器选择</td>
</tr>
<tr>
<td>iteration=1</td>
<td>int. Set the iteration number of training.</td>
<td>迭代次数</td>
</tr>
<tr>
<td>batch_size=10</td>
<td>int. Set the batch size of training or decoding.</td>
<td>批量大小</td>
</tr>
<tr>
<td>ave_batch_loss=False</td>
<td>boolen. Set average the batched loss during training.</td>
<td>是否设置 loss 为批量损失的均值</td>
</tr>
</tbody>
</table>
<h4 id="hyperparameters">Hyperparameters<a class="headerlink" href="#hyperparameters" title="Permanent link">&para;</a></h4>
<table>
<thead>
<tr>
<th>指令</th>
<th>解释</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>cnn_layer=4</td>
<td>int. CNN layer number for word sequence layer.</td>
<td>词序列层的CNN 层的深度，从而更好地学习长距离的依赖关系</td>
</tr>
<tr>
<td>char_hidden_dim=50</td>
<td>int. Character hidden vector dimension for character sequence layer.</td>
<td>字符序列层的字符隐层向量的维度</td>
</tr>
<tr>
<td>hidden_dim=200</td>
<td>int. Word hidden vector dimension for word sequence layer.</td>
<td>词序列层的词隐层向量的维度</td>
</tr>
<tr>
<td>dropout=0.5</td>
<td>float. Dropout probability.</td>
<td>Dropout 概率</td>
</tr>
<tr>
<td>lstm_layer=1</td>
<td>int. LSTM layer number for word sequence layer.</td>
<td>LSTM 层数</td>
</tr>
<tr>
<td>bilstm=True</td>
<td>boolen. If use bidirection lstm for word seuquence layer.</td>
<td>是否在词序列层中使用双向 LSTM</td>
</tr>
<tr>
<td>learning_rate=0.015</td>
<td>float. Learning rate.</td>
<td>学习率</td>
</tr>
<tr>
<td>lr_decay=0.05</td>
<td>float. Learning rate decay rate, only works when optimizer=SGD.</td>
<td>SGD 中的学习率衰减概率</td>
</tr>
<tr>
<td>momentum=0</td>
<td>float. Momentum</td>
<td>动量值</td>
</tr>
<tr>
<td>l2=1e-8</td>
<td>float. L2-regulization.</td>
<td>L2 正则化参数</td>
</tr>
<tr>
<td>gpu=True</td>
<td>boolen. If use GPU, generally it depends on the hardward environment.</td>
<td>是否使用 GPU</td>
</tr>
<tr>
<td>clip=</td>
<td>float. Clip the gradient which is larger than the setted number.</td>
<td>梯度裁剪的设定数值</td>
</tr>
</tbody>
</table>
<h3 id="extensionmd">Extension.md<a class="headerlink" href="#extensionmd" title="Permanent link">&para;</a></h3>
<h4 id="module-extension">Module Extension.<a class="headerlink" href="#module-extension" title="Permanent link">&para;</a></h4>
<p>If you want to extend character sequence layer: please refer to the file <a href="model/charlstm.py">charlstm.py</a>.</p>
<p>If you want to extend word sequence layer: please refer to the file <a href="model/wordsequence.py">wordsequence.py</a>.</p>
<p>More details will be updated soon.</p>
<h3 id="hyperparameter_tuningmd">Hyperparameter_tuning.md<a class="headerlink" href="#hyperparameter_tuningmd" title="Permanent link">&para;</a></h3>
<h4 id="hyperparamter-tuning-on-conll-2003-english-ner-task">Hyperparamter tuning on CoNLL 2003 English NER task<a class="headerlink" href="#hyperparamter-tuning-on-conll-2003-english-ner-task" title="Permanent link">&para;</a></h4>
<ol>
<li>如果您使用大批量（例如batch_size&gt; 100），您最好设置 <code>avg_batch_loss = True</code> 以获得稳定的训练过程。对于小批量，<code>avg_batch_loss = True</code> 将更快收敛，有时会提供更好的性能（例如CoNLL 2003 NER）。</li>
<li>如果使用100-d预训练单词向量 <a href="https://nlp.stanford.edu/projects/glove/">此处</a> 而不是50-d预训练单词向量，则可以在CoNLL 2003英语数据集上获得更好的性能。</li>
<li>如果要编写脚本来调整超参数，可以使用 <code>main_parse.py</code> 在命令行参数中设置超参数</li>
<li>模型性能对 <code>lr</code> 敏感，需要在不同结构下仔细调整：<ul>
<li>Word level LSTM models (e.g. char LSTM + word LSTM + CRF) would prefer a <code>lr</code> around 0.015.</li>
<li>Word level CNN models (e.g. char LSTM + word CNN + CRF) would prefer a <code>lr</code> around 0.005 and with more iterations.</li>
<li>You can refer the COLING paper "<a href="https://arxiv.org/pdf/1806.04470.pdf">Design Challenges and Misconceptions in Neural Sequence Labeling</a>" for more hyperparameter settings.</li>
</ul>
</li>
</ol>
<h2 id="_2">源码学习<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="_3">架构梳理<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p><img alt="alt text" src="../imgs/architecture.png" /></p>
<p>框架首先将所有的数据处理的部分，都放在了模型之外，模型的输入均为处理后的 word / char 的 index</p>
<p>模型分为三层：</p>
<ul>
<li>char sequence layer : 支持预训练以及随机初始化两种方式，获得 char-level embedding ，再得到 word embedding。方法包括 charRNN, charCNN，并且可以同时使用 (concatenate embedding_dim 即可)</li>
<li>word sequence layer : 支持预训练以及随机初始化两种方式，获得 word-level embedding，而后与 char sequence layer 得到的 word embedding 以及 handcrafted feature (如 POS ) 的embedding（同样支持两种方式）进行 concatenate，最后得到 hybrid 的 word embedding，再经过 多层 CNN / LSTM / GRU ，得到最终的 hidden_state</li>
<li>inference layer : 支持 CRF 与 Softmax 两种方式</li>
</ul>
<h3 id="_4">流程梳理<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h3>
<p>框架首先读取用户定义好的 config 文件，其中包括了 I/O, Network, Training, Hyperparameters 四部分的参数设置。用户可以通过修改 config 文件，或是以 <code>--parameter value</code> 的形式在运行时修改。运行指令如下：</p>
<ul>
<li><code>python main.py --config train.config</code> </li>
</ul>
<p><strong>training status</strong></p>
<p>首先读取初始化运行参数，读取配置覆盖初始参数，而后对数据进行初步处理并读取/初始化所需的 embedding ，包括</p>
<ul>
<li>建立 feature 的字母表 <strong>alphabet</strong> 和 训练集 / 开发集 / 测试集 的 word / char / label / feature_list 的字母表<ul>
<li>字母表主要是包括存储 instance 的 list 以及 <span><span class="MathJax_Preview">\{ \mathbf{key:instance}, \mathbf{value:index}\}</span><script type="math/tex">\{ \mathbf{key:instance}, \mathbf{value:index}\}</script></span> 的 dict</li>
</ul>
</li>
<li>建立 <strong>instance_text</strong> 和 <strong>instance_Ids</strong> 两个 <strong>list</strong> ，其每一项为 <code>[words, features, chars, labels]</code> 和 <code>[word_Ids, feature_Ids, char_Ids, label_Ids]</code>  （以 NER 任务为例，特征使用的是 POS），分别是 sequence 中的所有单词构成的 <code>[‘good’]</code> ，所有特征构成的 <code>[‘JJ’]</code> ，所有字符构成的 <code>[[‘g’, ‘o’, ’o’, ‘d’]]</code> ，所有 label 构成的 <code>['O']</code> ，以及上述内容在各自的字母表中对应的 index 构成的 <code>xxx_Ids</code></li>
<li>对建立好的 word / char / label / feature_list 的字母表，遍历字母表中的所有 instance ，分别建立 Embedding 矩阵。读取 word / char / feature 的 pretrain embedding，如果未指定 pretrain embedding 的文件位置，那么均随机初始化，否则会分为以下三种情况分别处理<ul>
<li>perfect match : 如果在 pretrain embedding 中找到了字母表里的 instance ，则将其 embedding 对应赋值，否则进行下一步</li>
<li>case_match : 如果在 pretrain embedding 中找到了字母表里的 instance 的 lower 版本的 embedding，则同样对应赋值，否则进行下一步</li>
<li>not_match : 如果在 pretrain embedding 中没有找到字母表里的 instance 的 raw / lower 版本，则随机初始化</li>
</ul>
</li>
</ul>
<p>接着处理模型各层的输入数据（ batch_size 自定义），包括 <code>batch_word, batch_features, batch_wordlen, batch_wordrecover, batch_char, batch_charlen, batch_charrecover, batch_label, mask</code> </p>
<ul>
<li>将 <strong>instance_Ids</strong> 提取出 batch 个 item，每个 item 都是 <code>[words, features, chars, labels]</code> 构成的嵌套 list </li>
<li>将 batch 数据中四类数据分别抽取建立为 list ，包括 <code>words, features, chars, labels</code> ，而后建立起对应的 zeros Tensor 以及 mask Tensor，并统计最长句子长度等信息</li>
<li>分别在 zeros Tensor 中填充 <code>words, labels, mask, features</code> 的 Tensor</li>
</ul>
<p>Pytorch 中提供了 <code>torch.nn.utils.rnn.PackedSequence</code> 的相关 API，输入时需要将 sequence 按照真实长度降序排列，并输入 sequence 以及其对应的真实长度，于是：</p>
<ul>
<li><code>python
    # 统计 batch 中每个 sequence 的长度
    word_seq_lengths = torch.LongTensor(list(map(len, words)))
    # 降序排序后获得其排序结果以及该结果中的对应每一项在原序列中的 index
    word_seq_lengths, word_perm_idx = word_seq_lengths.sort(0, descending=True)</code></li>
</ul>
<p>举个例子：</p>
<ul>
<li>原序列 word_seq_lengths = [9, 7, 11]，那么排序后的返回值为<ul>
<li>word_seq_lengths = [11, 9, 7] ，word_perm_idx = [2, 0, 1]</li>
</ul>
</li>
</ul>
<p>那么我们就可以将 length 对应的 size 为 <span><span class="MathJax_Preview">\text{batch_size} \times \text{max_seq_len}</span><script type="math/tex">\text{batch_size} \times \text{max_seq_len}</script></span> 的 word_seq_tensor 以及 label_seq_tensor 等进行重排列，即
-   <code>word_seq_tensor = word_seq_tensor[word_perm_idx]</code> 那么现在的 word / label / length 就是一一对应的了，feature, mask 也是同理操作</p>
<p>如上操作后，sequence 中的 word-level 的 padding 以及排序工作就完成了，随后进行 char-level 的 padding 以及排序工作</p>
<ul>
<li>这里的 word-level 的 padding，是对每个 batch 的每个句子，都填充为最长的句子长度</li>
</ul>
<p>首先将原本的 chars 列表进行 padding ：</p>
<ul>
<li>chars list 中的chars[3] 如下图所示</li>
</ul>
<p><img alt="image-20190719141036595" src="../imgs/image-20190719141036595.png" /></p>
<ul>
<li>可以看到 chars[3] 中的为一个 sequence ，其中每一项均为 sequence 中的某一个单词，如 chars[3] 的第 8 项即为一个长度为 2 的单词，由字符 15 和 40 组成</li>
<li>先对每个 sequence 进行 padding，保证 sequence 的长度均为 max_seq_len<ul>
<li>即对 chars[3] 这一 sequence 末尾，添加[[0], [0], [0], …… , [0]]</li>
</ul>
</li>
<li>而后对本 batch 内的所有 sequence 中的所有 word 统计得到最长单词长度 max_word_len</li>
<li>初始化 size 为 <code>batch_size, max_seq_len, max_word_len</code> 的 zeros Tensor <code>char_seq_tensor</code> ，统计当前 chars 中所有单词的长度，获得 size 为 <code>batch_size, max_seq_len</code> 的 char_seq_lengths</li>
<li>将每个 sequence 的每个 word 的每个 char_Id 填入 <code>char_seq_tensor</code> 中这样就完成了  char-level 的 padding 工作</li>
</ul>
<p>接着我们我们对数据做进一步的处理</p>
<ul>
<li>首先通过 word_perm_idx 将序列的顺序调整的与 word 和 label 的顺序一致，然后是对 char_seq_tensor, char_seq_lengths 进行压实操作，将其 size 调整为 <code>(batch_size * max_seq_len, max_word_len)</code> 和 <code>(batch_size * max_seq_len,)</code> </li>
<li>
<p>接着进行同样的排序与调序操作，将 word 的顺序调整为按照 word_len 降序排列</p>
<ul>
<li>这是我们应该注意到，单词的顺序在 char sequence layer 中被打乱了，我们在经过charCNN / charRNN 的处理后，得到的输出需要输入到 word sequence layer 中，顺序是不一致的，需要在输出时将顺序调整回去</li>
</ul>
</li>
<li>
<p>回到之前的例子：</p>
<ul>
<li>原序列 word_seq_lengths = [9, 7, 11]，那么排序后的返回值为 word_seq_lengths = [11, 9, 7] ，word_perm_idx = [2, 0, 1]</li>
<li>我们对 word_perm_idx 进行升序排列，得到返回值为 [0, 1, 2] 和 [1, 2, 0]，这时候令 <code>word_seq_lengths = word_seq_lengths[1, 2, 0]</code> ，我们就会发现 word_seq_lengths 变回了原序列</li>
<li>当 char sequence layer 将其获得的每个 word 的 embedding 输出至 word sequence layer 时，需要使用之前保存的 char_seq_recover 将 word 的顺序还原</li>
</ul>
</li>
</ul>
<p>至此，各层所需的输入数据都准备完毕。</p>
<p><img alt="image-20190719145901069" src="../imgs/image-20190719145901069.png" /></p>
<p>运行时，模型首先需要完成对 char sequence layer 的运算，获得 char-level 的 word embedding，然后与 word embedding 和 feature embedding 进行 concatenate，得到最终的 word embedding，再通过多层的 LSTM / GRU / CNN 得到最终的 size 为 <code>batch_size, seq_len, classes</code> 的特征向量</p>
<p>最后的 inference layer 可以有两种选择：CRF / Softmax ，这里我们先讨论 Softmax 的方式。对特征向量的最后一维做 log_softmax ，而后计算其 NLLLoss 并将其最大值对应的 class 作为分类结果。</p>
<h4 id="crf">CRF 笔记<a class="headerlink" href="#crf" title="Permanent link">&para;</a></h4>
<p>条件随机场(Conditional Random Fields, 以下简称CRF)是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型。其特点是假设输出随机变量构成马尔科夫随机场。条件随机场可以用于不同的预测问题，本文讨论的是其在标注问题中的应用，因此主要讲述线性链条件随机场。这时，问题变成由输入序列对输出序列预测的判别模型，形式为对数线性模型，其学习方法通常是极大似然估计或正则化的极大似然估计。</p>
<h5 id="crf_1">什么样的问题需要CRF模型<a class="headerlink" href="#crf_1" title="Permanent link">&para;</a></h5>
<p>对于 A 的一天从早到晚的一系列照片，我们想要知道每张照片对应的活动，如果我们用传统的分类思路去做，即对每张照片分别预测其活动类别，这就忽略了活动之间的关联性和内在约束，比如对于一张 A 闭着嘴巴的照片，如果前一张照片是 A 在吃东西的照片，那么此时 A 就是在咀嚼；如果前一张照片是 A 在唱歌的照片，那么此时 A 就是在唱歌。</p>
<p>这就需要我们考虑 <strong>相邻数据的标记信息</strong> 。自然语言处理中的**词性标注**(Part-Of-Speech Tagging)正是此类问题的经典任务。</p>
<h5 id="_5">随机场，马尔科夫随机场，条件随机场<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h5>
<ul>
<li>随机场：当给每一个位置中按照某种分布随机赋予相空间的一个值之后，其全体就叫做随机场。我们不妨拿种地来打个比方。其中有两个概念：位置（site），相空间（phase space）。“位置”好比是一亩亩农田；“相空间”好比是种的各种庄稼。我们可以给不同的地种上不同的庄稼，这就好比给随机场的每个“位置”，赋予相空间里不同的值。所以，俗气点说，随机场就是在哪块地里种什么庄稼的事情。</li>
<li>马尔科夫随机场：马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。<ul>
<li>马尔科夫性质：它指的是一个随机变量序列按时间先后关系依次排开的时候，第N+1时刻的分布特性，与N时刻以前的随机变量的取值无关。拿天气来打个比方。如果我们假定天气是马尔可夫的，其意思就是我们假设今天的天气仅仅与昨天的天气存在概率上的关联，而与前天及前天以前的天气没有关系。其它如传染病和谣言的传播规律，就是马尔可夫的。</li>
</ul>
</li>
<li>CRF是马尔科夫随机场的特例，它假设马尔科夫随机场中只有X和Y两种变量，X一般是给定的，而Y一般是在给定X的条件下的输出。这样马尔科夫随机场就特化成了条件随机场。设X与Y是随机变量，P(Y|X)是给定X时Y的条件概率分布，若随机变量Y构成的是一个马尔科夫随机场，则称条件概率分布P(Y|X)是条件随机场。</li>
</ul>
<h5 id="softmax-crf">Softmax 与 CRF<a class="headerlink" href="#softmax-crf" title="Permanent link">&para;</a></h5>
<p><img alt="image-20190802135238183" src="../imgs/image-20190802135238183.png" /></p>
<p>上图为用 CNN 或者 RNN 对序列进行编码后，使用 Softmax 作为分类器，完成 POS 任务。</p>
<p>以 RNN 为例，对于 t 时刻来说，输出层 yt 受到隐层 ht（包含上下文信息）和输入层 xt（当前的输入）的影响，但是yt和其他时刻的yt'是相互独立的，并没有直接考虑输出的上下文关系。</p>
<p><img alt="image-20190802135618452" src="../imgs/image-20190802135618452.png" /></p>
<p>上图为使用 CRF 进行 POS 时，由 CRF 对输出层的上下文关系进行直接关联，即 CRF 在输出端显式地考虑了上下文关联。</p>
<h5 id="_6">公式推导<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h5>
<p>CRF 的真正精巧的地方，是它以路径为单位，考虑的是路径的概率。 </p>
<p>假如一个输入有 n 帧，每一帧的标签有 k 种可能性，那么理论上就有 <span><span class="MathJax_Preview">k^n</span><script type="math/tex">k^n</script></span> 中不同的结果标签序列。我们可以将它用如下的网络图进行简单的可视化。在下图中，每个点代表一个标签的可能性，点之间的连线表示标签之间的关联，而每一种标注结果，都对应着图上的一条完整的路径。</p>
<p><img alt="image-20190802140002173" src="../imgs/image-20190802140002173.png" /></p>
<p>而在序列标注任务中，我们的正确答案是一般是唯一的。比如“今天天气不错”，如果对应的分词结果是“今天/天气/不/错”，那么目标输出序列就是 bebess，除此之外别的路径都不符合要求。</p>
<p>换言之，在序列标注任务中，我们的研究的基本单位应该是路径，我们要做的事情，是从 <span><span class="MathJax_Preview">k^n</span><script type="math/tex">k^n</script></span> 条路径选出正确的一条，那就意味着，如果将它视为一个分类问题，那么将是 <span><span class="MathJax_Preview">k^n</span><script type="math/tex">k^n</script></span> 类中选一类的分类问题。</p>
<p>逐帧 softmax 和 CRF 的根本区别</p>
<ul>
<li><strong>前者将序列标注看成是 n 个 k 分类问题，后者将序列标注看成是 1 个 k^n 分类问题</strong></li>
</ul>
<p>具体来讲，在 CRF 的序列标注问题中，我们要计算的是条件概率：</p>
<div>
<div class="MathJax_Preview">
P\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{n}\right)=P\left(y_{1}, \ldots, y_{n} | x\right), \quad x=\left(x_{1}, \ldots, x_{n}\right)
</div>
<script type="math/tex; mode=display">
P\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{n}\right)=P\left(y_{1}, \ldots, y_{n} | x\right), \quad x=\left(x_{1}, \ldots, x_{n}\right)
</script>
</div>
<p>为了得到这个概率的估计，CRF 做了两个假设：</p>
<p><strong>假设一：该分布是指数族分布</strong></p>
<p>这个假设意味着存在函数 <span><span class="MathJax_Preview">f(y_1,…,y_n;x)</span><script type="math/tex">f(y_1,…,y_n;x)</script></span>，使得</p>
<div>
<div class="MathJax_Preview">
P\left(y_{1}, \ldots, y_{n} | \mathbf{x}\right)=\frac{1}{Z(\mathbf{x})} \exp \left(f\left(y_{1}, \ldots, y_{n} ; \mathbf{x}\right)\right)
</div>
<script type="math/tex; mode=display">
P\left(y_{1}, \ldots, y_{n} | \mathbf{x}\right)=\frac{1}{Z(\mathbf{x})} \exp \left(f\left(y_{1}, \ldots, y_{n} ; \mathbf{x}\right)\right)
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">Z(x)</span><script type="math/tex">Z(x)</script></span> 是归一化因子，因为这个是条件分布，所以归一化因子跟 x 有关。这个 f 函数可以视为一个打分函数，打分函数取指数并归一化后就得到概率分布。 </p>
<p><strong>假设二：输出之间的关联仅发生在相邻位置，并且关联是指数加性的</strong></p>
<p>下式为链式结构的条件概率（即 x 与 y 的结构相同）</p>
<div>
<div class="MathJax_Preview">
p(\mathbf{y} | \mathbf{x}, \theta)=\frac{1}{Z(\mathbf{x}, \theta)} \exp \left(\sum_{t=1}^{T} \theta_{1}^{\mathrm{T}} f_{1}\left(\mathbf{x}, y_{t}\right)+\sum_{t=1}^{T-1} \theta_{2}^{\mathrm{T}} f_{2}\left(\mathbf{x}, y_{t}, y_{t+1}\right)\right)
</div>
<script type="math/tex; mode=display">
p(\mathbf{y} | \mathbf{x}, \theta)=\frac{1}{Z(\mathbf{x}, \theta)} \exp \left(\sum_{t=1}^{T} \theta_{1}^{\mathrm{T}} f_{1}\left(\mathbf{x}, y_{t}\right)+\sum_{t=1}^{T-1} \theta_{2}^{\mathrm{T}} f_{2}\left(\mathbf{x}, y_{t}, y_{t+1}\right)\right)
</script>
</div>
<p>其中，<span><span class="MathJax_Preview">f_{1}\left(\mathbf{x}, y_{t}\right)</span><script type="math/tex">f_{1}\left(\mathbf{x}, y_{t}\right)</script></span> 为状态特征，与位置 <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> 相关；<span><span class="MathJax_Preview">f_{2}\left(\mathbf{x}, y_{t}, y_{t+1}\right)</span><script type="math/tex">f_{2}\left(\mathbf{x}, y_{t}, y_{t+1}\right)</script></span> 为转移特征。</p>
<p><strong>损失函数</strong></p>
<p>极大似然估计
$$
L(x) = -\log P\left(y_{1}, \ldots, y_{n} | x\right) \
= log(Z(x)) -\left(\sum_{t=1}^{T} \theta_{1}^{\mathrm{T}} f_{1}\left(\mathbf{x}, y_{t}\right)+\sum_{t=1}^{T-1} \theta_{2}^{\mathrm{T}} f_{2}\left(\mathbf{x}, y_{t}, y_{t+1}\right)\right)
$$
<strong>预测问题</strong></p>
<p>模型训练完成后，如何根据输入找出最优路径？同样的，这也是一个从 <span><span class="MathJax_Preview">k^n</span><script type="math/tex">k^n</script></span> 条路径中选最优的问题，而因为马尔可夫假设的存在，它可以转化为一个动态规划问题，用 viterbi 算法解决，计算量正比于 n。 </p>
<p><strong>递归思想就是：一条最优路径切成两段，那么每一段都是一条（局部）最优路径</strong></p>
<h4 id="pytorch-api">Pytorch API<a class="headerlink" href="#pytorch-api" title="Permanent link">&para;</a></h4>
<p>我们先来介绍几个需要用到的 API</p>
<h5 id="gather">gather<a class="headerlink" href="#gather" title="Permanent link">&para;</a></h5>
<p>理解：将 input 按照 dim 指定的维度以及 index 中指定的顺序，取出的对应值</p>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sparse_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="err">→</span> <span class="n">Tensor</span>
</code></pre></div>

<p>Gathers values along an axis specified by dim.</p>
<p>For a 3-D tensor the output is specified by:</p>
<div class="highlight"><pre><span></span><code><span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span>  <span class="c1"># if dim == 2</span>
</code></pre></div>

<p>If <code>input</code> is an n-dimensional tensor with size <span><span class="MathJax_Preview">(x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})</span><script type="math/tex">(x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})</script></span> and <code>dim = i</code>, then <code>index</code> must be an n-dimensional tensor with size <span><span class="MathJax_Preview">(x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})</span><script type="math/tex">(x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})</script></span> where <span><span class="MathJax_Preview">y \geq 1</span><script type="math/tex">y \geq 1</script></span> and <code>out</code> will  <strong>have the same size</strong> as <code>index</code>.</p>
<p>Parameters</p>
<ul>
<li><strong>input</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>) – the source tensor</li>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – the axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to gather</li>
<li><strong>out</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the destination tensor</li>
<li><strong>sparse_grad</strong> (<a href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,optional</em>) – If <code>True</code>, gradient w.r.t. <code>input</code> will be a sparse tensor.</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]]))</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">4</span><span class="p">,</span>  <span class="mi">3</span><span class="p">]])</span>
</code></pre></div>

<h5 id="scatter">scatter<a class="headerlink" href="#scatter" title="Permanent link">&para;</a></h5>
<p>将 src 的值按照 dim  和 index 的要求填入 self 中</p>
<div class="highlight"><pre><span></span><code><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">src</span><span class="p">)</span> <span class="err">→</span> <span class="n">Tensor</span>
</code></pre></div>

<p>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor. For each value in <code>src</code>, its output index is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p>
<p>For a 3-D tensor, <code>self</code> is updated as:</p>
<div class="highlight"><pre><span></span><code><span class="bp">self</span><span class="p">[</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 0</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]][</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 1</span>
<span class="bp">self</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">index</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]]</span> <span class="o">=</span> <span class="n">src</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>  <span class="c1"># if dim == 2</span>
</code></pre></div>

<p>This is the reverse operation of the manner described in <a href="https://pytorch.org/docs/stable/tensors.html?highlight=scatter#torch.Tensor.gather"><code>gather()</code></a>.</p>
<p><code>self</code>, <code>index</code> and <code>src</code> (if it is a Tensor) should have same number of dimensions. It is also required that <code>index.size(d) &lt;= src.size(d)</code> for all dimensions <code>d</code>, and that <code>index.size(d) &lt;= self.size(d)</code> for all dimensions <code>d != dim</code>.</p>
<p>Moreover, as for <a href="https://pytorch.org/docs/stable/tensors.html?highlight=scatter#torch.Tensor.gather"><code>gather()</code></a>, the values of <code>index</code> must be between <code>0</code> and <code>self.size(dim) - 1</code> inclusive, and all values in a row along the specified dimension <a href="https://pytorch.org/docs/stable/tensors.html?highlight=scatter#torch.Tensor.dim"><code>dim</code></a> must be unique.</p>
<p>Parameters</p>
<ul>
<li><strong>dim</strong> (<a href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – the axis along which to index</li>
<li><strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter, can be either empty or the same size of src. When empty, the operation returns identity</li>
<li><strong>src</strong> (<a href="https://pytorch.org/docs/stable/tensors.html?highlight=scatter#torch.Tensor"><em>Tensor</em></a>) – the source element(s) to scatter, incase value is not specified</li>
<li><strong>value</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><em>float</em></a>) – the source element(s) to scatter, incase src is not specified</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.3992</span><span class="p">,</span>  <span class="mf">0.2908</span><span class="p">,</span>  <span class="mf">0.9044</span><span class="p">,</span>  <span class="mf">0.4850</span><span class="p">,</span>  <span class="mf">0.6004</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5735</span><span class="p">,</span>  <span class="mf">0.9006</span><span class="p">,</span>  <span class="mf">0.6797</span><span class="p">,</span>  <span class="mf">0.4152</span><span class="p">,</span>  <span class="mf">0.1732</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]),</span> <span class="n">x</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.3992</span><span class="p">,</span>  <span class="mf">0.9006</span><span class="p">,</span>  <span class="mf">0.6797</span><span class="p">,</span>  <span class="mf">0.4850</span><span class="p">,</span>  <span class="mf">0.6004</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.2908</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.4152</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.5735</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.9044</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.1732</span><span class="p">]])</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]),</span> <span class="mf">1.23</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">z</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.2300</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">0.0000</span><span class="p">,</span>  <span class="mf">1.2300</span><span class="p">]])</span>
</code></pre></div>

<h5 id="masked_select">masked_select<a class="headerlink" href="#masked_select" title="Permanent link">&para;</a></h5>
<p>从 input 中选出 mask 中为 1 的对应位置的值，拼为新的 Tensor 并返回</p>
<div class="highlight"><pre><span></span><code><span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">mask</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="err">→</span> <span class="n">Tensor</span>
</code></pre></div>

<p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to the binary mask <code>mask</code> which is a ByteTensor.</p>
<p>The shapes of the <code>mask</code> tensor and the <code>input</code> tensor don’t need to match, but they must be <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics">broadcastable</a>.</p>
<p>NOTE</p>
<p>The returned tensor does <strong>not</strong> use the same storage as the original tensor</p>
<p>Parameters</p>
<ul>
<li><strong>input</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>) – the input data</li>
<li><strong>mask</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor"><em>ByteTensor</em></a>) – the tensor containing the binary mask to index with</li>
<li><strong>out</strong> (<a href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a><em>,</em> <em>optional</em>) – the output tensor</li>
</ul>
<p>Example:</p>
<div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">x</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">0.3552</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3825</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8297</span><span class="p">,</span>  <span class="mf">0.3477</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.2035</span><span class="p">,</span>  <span class="mf">1.2252</span><span class="p">,</span>  <span class="mf">0.5002</span><span class="p">,</span>  <span class="mf">0.6248</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">0.1307</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.0608</span><span class="p">,</span>  <span class="mf">0.1244</span><span class="p">,</span>  <span class="mf">2.0139</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mask</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>  <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">masked_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">1.2252</span><span class="p">,</span>  <span class="mf">0.5002</span><span class="p">,</span>  <span class="mf">0.6248</span><span class="p">,</span>  <span class="mf">2.0139</span><span class="p">])</span>
</code></pre></div>

<h5 id="masked_scatter_">masked_scatter_<a class="headerlink" href="#masked_scatter_" title="Permanent link">&para;</a></h5>
<p>将 mask 为 1 的对应 source 中的值复制到 self 中</p>
<div class="highlight"><pre><span></span><code><span class="n">masked_scatter_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span> 
</code></pre></div>

<p>Copies elements from <code>source</code> into <code>self</code> tensor at positions where the <code>mask</code> is one. The shape of <code>mask</code>must be <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the shape of the underlying tensor. The <code>source</code> should have at least as many elements as the number of ones in <code>mask</code></p>
<p>Parameters</p>
<ul>
<li><strong>mask</strong> (<a href="https://pytorch.org/docs/stable/tensors.html?highlight=masked_scatter#torch.ByteTensor"><em>ByteTensor</em></a>) – the binary mask</li>
<li><strong>source</strong> (<a href="https://pytorch.org/docs/stable/tensors.html?highlight=masked_scatter#torch.Tensor"><em>Tensor</em></a>) – the tensor to copy from</li>
</ul>
<p>NOTE</p>
<p>The <code>mask</code> operates on the <code>self</code> tensor, not on the given <code>source</code> tensor.</p>
<h5 id="masked_fill_">masked_fill_<a class="headerlink" href="#masked_fill_" title="Permanent link">&para;</a></h5>
<p>将 mask 为 1 的 self 中的对应位置的值改为 value</p>
<div class="highlight"><pre><span></span><code><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</code></pre></div>

<p>Fills elements of <code>self</code> tensor with <code>value</code> where <code>mask</code> is one. The shape of <code>mask</code> must be <a href="https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics">broadcastable</a> with the shape of the underlying tensor.</p>
<p>Parameters</p>
<ul>
<li><strong>mask</strong> (<a href="https://pytorch.org/docs/stable/tensors.html?highlight=masked_scatter#torch.ByteTensor"><em>ByteTensor</em></a>) – the binary mask</li>
<li><strong>value</strong> (<a href="https://docs.python.org/3/library/functions.html#float"><em>float</em></a>) – the value to fill in with</li>
</ul>
<h4 id="crf_2">CRF 源码解析<a class="headerlink" href="#crf_2" title="Permanent link">&para;</a></h4>
<p>现在来看 <strong>CRF</strong> ，CRF 考虑的不是每次转移时的最优概率，考虑的整体序列的可能性。</p>
<p>Previous_to <span><span class="MathJax_Preview">\to</span><script type="math/tex">\to</script></span> current_from</p>
<blockquote>
<p>CRF 的核心是它不像LSTM等模型，能够考虑长远的上下文信息，<strong>它更多考虑的是整个句子的局部特征的线性加权组合（通过特征模版去扫描整个句子）</strong>。关键的一点是，CRF的模型为p(y | x, w)，注意这里y和x都是序列，它有点像list wise，优化的是一个序列y = (y1, y2, …, yn)，而不是某个时刻的yt，即找到一个概率最高的序列y = (y1, y2, …, yn)使得p(y1, y2, …, yn| x, w)最高，它计算的是一种联合概率，优化的是整个序列（最终目标），而不是将每个时刻的最优拼接起来，在这一点上CRF要优于LSTM。</p>
</blockquote>
<p>转移矩阵的 <span><span class="MathJax_Preview">[i, j]</span><script type="math/tex">[i, j]</script></span> 代表的是由状态 <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> 转移到状态 <span><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> 的可能性</p>
<p>注意，CRF 中的 tag_size 是真实的 tag_size + 2 </p>
<h5 id="_calculate_pzself-feats-mask">_calculate_PZ(self, feats, mask)<a class="headerlink" href="#_calculate_pzself-feats-mask" title="Permanent link">&para;</a></h5>
<p>首先我们将获得的 size 为 <code>batch_size, seq_len, tag_size</code> 的特征向量调整为<code>seq_len, batch_size, tag_size</code> ，方便随后依时间步访问序列。</p>
<p>而后再调整为<code>seq_len * batch_size, 1, tag_size</code>，再扩展成为 <code>seq_len * batch_size, tag_size, tag_size</code> 与转移矩阵相加得到 scores 。</p>
<ul>
<li>可以理解成是将通过 model 获得到的每个时间步的所有的 tag 的可能性都加到转移矩阵之上，即当我们按时间步遍历每个时间步上的 size 为<code>batch_size, tag_size, tag_size</code> 的 cur_values 矩阵时，这里的矩阵是由原始的转移矩阵 + 之前 model 得到的每一时间步上的 word 的每种 tag 的可能性。这里我们将特征向量 feature 从一个行向量(不看 batch_size )扩展为一个矩阵，其实就是不管 start 状态是什么，转移到 j 状态的可能性都会加上 扩展前的 feature[j]。</li>
</ul>
<p>而在每次随时间步的迭代中，我们都会将前一时间步传来的 size 为 <code>batch_size, tag_size, 1</code> 的 partition 数组，扩展为 <code>batch_size, tag_size, tag_size</code> 并加上 cur_values 。</p>
<ul>
<li>第一次迭代中，partition 是由 <code>inivalues[:, START_TAG, :].clone().view(batch_size, tag_size, 1</code> 得到的，其含义是 start_tag 之后的下一个 tag 的概率值，也就是当前时间步对应的 word 的 tag 的可能性。将这一 <strong>列向量</strong> 扩展后并与 cur_values 相加后，相当于每一行的各个数值都加上了同一值，也就是从状态 i 到其他任何状态都加上了 inivalues[START_TAG, i] (不看 batch_size )。<ul>
<li>这是因为这一数值的含义是从 start_tag 到状态 i 的可能性，也就是当前状态为 i 的可能性。</li>
<li>那么 cur_values 需要将由状态 i 出发的所有状态 j 的可能性都增加这一数值，即cur_values[i] = inivalues[START_TAG, i].view(1, tag_size) + cur_values[i]。对每行分别处理一次，经过这样的 tag_size 次运算，我们就可以得到一个新的、考虑到前一状态转移矩阵的，新的状态转移矩阵。</li>
</ul>
</li>
<li>接下来我们需要对这一矩阵进行处理，得到新的 partition 传递给下一次的迭代。我们先计算矩阵每一列的最大值，构成一个行向量 max_value ，max_value[j] 含义是下一状态为 j 的最大转移可能性， 将其拓展为和输入的 partition 一样的 size 后用 partition - max_value，矩阵的所有值都是非正数，逐元素作用 exp 函数将其按列 sum (第 i 列的和的意义是下一状态为 i 的可能性之和) ，逐元素作用 log 函数，最终得到的新的矩阵 temp 是一个行向量(不看 batch_size )，temp[j] 代表的是转移到状态 j 的可能性之和。再将其与 max_score相加，得到最终的 cur_partition。<ul>
<li>需要注意的是，上述过程未提及 mask 步骤，实际操作中需要使用 mask 操作完成对 partition 的更新</li>
<li>即，将 cur_partition 中对应 mask 为 1 的取出并拼接为一维的 masked_cur_partition ，再将masked_cur_partition 中 更新到 partition 的对应位置，获得新的 partition</li>
</ul>
</li>
<li>遍历完序列后，得到 <code>final_partition = cur_partition[:, STOP_TAG]</code> ，即各个状态转移到 stop_tag 的可能性，求得其 sum 并返回 sum 与 scores</li>
</ul>
<h5 id="_score_sentenceself-scores-mask-tags">_score_sentence(self, scores, mask, tags)<a class="headerlink" href="#_score_sentenceself-scores-mask-tags" title="Permanent link">&para;</a></h5>
<p>而今我们已经获得了 <code>forward_score, scores</code> ，接下来继续计算 gold_score</p>
<ul>
<li>tags 的 size 为 (batch, seq_len) ，存放的是每个 batch 内每个 sequence 的对应位置的 tag 类型。为了能够从 scores 中取出对应的 tg_energy，做了如下处理<ul>
<li>首先将 size 为 (seq_len, batch, tag_size, tag_size) 的 scores 调整为 (seq_len, batch, tag_size * tag_size)</li>
<li>接着将 tags 的数值调整为 <code>new_tags[:, idx] = tags[:, idx - 1] * tag_size + tags[:, idx]</code> ，此时的矩阵存储的数值正和 scores 矩阵压缩后的对应位置一致，即将 from_tag &amp; to_tag 的信息存储在了这一数值中，new_tags 的 size 为 (batch_size, seq_len)</li>
<li>将 new_tags 交换 0 1 维并调整为 (seq_len, batch_size, 1)，然后从 scores 中取出对应的 tg_energy ，size 为 (seq_len, batch_size) </li>
<li>再使用 mask 矩阵过滤一遍</li>
</ul>
</li>
<li>取出转移矩阵中所有 tag 变为 STOP_TAG 的概率值并扩展为 (batch_size, tag_size) 作为 end_transition</li>
<li>通过对 size 为 (batch, seq_len) 的 mask 的每行求和，得到每个 sequence 的 length，再从原 tags 中取出每个 sequence 的最后一个 tag 的 ID 形成 size 为 (batch_size, 1) 的 end_ids</li>
<li>接着从 end_transition 中取出 end_ids 对应的 end_energy</li>
<li>对 tg_energy 和 end_energy 分别求和并相加，得到 gold_score</li>
</ul>
<h5 id="_viterbi_decodeself-feats-mask">_viterbi_decode(self, feats, mask)<a class="headerlink" href="#_viterbi_decodeself-feats-mask" title="Permanent link">&para;</a></h5>
<p>现在我们来看 viterbi_decode </p>
<ul>
<li>首先和之前 _calculate_PZ 的计算过程类似，得到 scores 矩阵后进行遍历。不同之处在于，需要记录所有的 partition 以及 cur_bp 分别保存在 partition_history 和 back_points ，并且计算方式为 <code>partition, cur_bp = torch.max(cur_values, 1)</code> ，此外 mask 需要 reverse ，即 <code>mask = (1 - mask.long()).byte()</code> ，此时 mask 中为 1 的部分表示 padding<ul>
<li>partition 为每一时间步上的各个 to_target 的最大可能性</li>
<li>cur_bp 为 partition 的每个值的 from_target</li>
<li>再通过 mask 将 cur_bp 中mask 为 1 的对应位置赋值为 0</li>
</ul>
</li>
<li>接着，通过 length_mask 获得 last_position 即每个 sequence 的结束位置的 index，再从 partition_history 中取出 last_partition ，与转移矩阵 transitions 相加，得到序列的结尾处的转移矩阵 last_values ，而后求得其每列的最大值对应的 index ，再取出 STOP_TAG 对应的列，就获得了 size 为 (batch_size) 的pointer ，即最有可能转移至 STOP_TAG 的 from_target 。再将这一 from_target 放到 back_points 的末尾</li>
<li>最后计算 decode_idx，用于在 decode 阶段解析得到 decoded sequence<ul>
<li>pointer 是 decode_idx 的最后一项，因为其保存的是最有可能转移至 STOP_TAG 的 from_target ，即 end_id</li>
<li>倒序解码时，后一时间步的 pointer 即为当前时间步的 to_target 了，所以对应从 back_points 中取得其 from_target 并保存在 decode_idx 中，以此类推</li>
</ul>
</li>
</ul>
<h5 id="_viterbi_decode_nbestself-feats-mask-nbest">_viterbi_decode_nbest(self, feats, mask, nbest)<a class="headerlink" href="#_viterbi_decode_nbestself-feats-mask-nbest" title="Permanent link">&para;</a></h5>
<ul>
<li>首先和 _calculate_PZ 的计算过程类似，获得 scores 矩阵，再取出 start_tag 之后的下一个 tag 的概率值，并扩展为 (batch_size, tag_size, nbest) ，在添加到 partition_history 中，此时 idx 为 0。此外 mask 需要 reverse ，即 <code>mask = (1 - mask.long()).byte()</code> ，此时 mask 中为 1 的部分表示 padding</li>
<li>接着遍历 scores<ul>
<li>第一次遍历时<ul>
<li>idx 已经为 1 , 将 cur_values 与 partition 相加获得 size 为 (batch_size, tag_size, tag_size) 的 新 cur_values</li>
<li>接着取出 cur_values 中的 topk dim=1 的排序结果作为新的 partition 以及对应的 index -- cur_bp，即获得了下一状态的 topk 的可能性以及对应的 from_target ，size 均为 (batch_size, nbest, tag_size)</li>
<li>将 cur_bp 的所有值都 * nbest ，与后续的 index 一致</li>
<li>将 partition 和 cur_bp 的 1 2 维对调，size 变为  (batch_size, tag_size, nbest)，并将 partition 保存在 partition_history 中</li>
<li>再通过 mask 将 cur_bp 中mask 为 0 的对应位置赋值为 0，并将 cur_bp 保存在 back_points 中</li>
</ul>
</li>
<li>其他时候<ul>
<li><span><span class="MathJax_Preview">\text{idx} \gt 1</span><script type="math/tex">\text{idx} \gt 1</script></span> ，将 cur_values 调整为 (batch_size, tag_size, 1, tag_size) 并扩展为 (batch_size, tag_size, nbest, tag_size) ，将 partition 调整为 (batch_size, tag_size, nbest, 1) 并扩展为 (batch_size, tag_size, nbest, tag_size) ，再将两者相加，获得新的 cur_values 并调整size 为 (batch_size, tag_size * nbest, tag_size)<ul>
<li>这里的两次扩展：对于 nbest 而言，cur_values 是不变的，故可以直接拓展，而 partition 调整为 (batch_size, tag_size, nbest, 1) 后的含义是上一时间步作为 to_target 的每种 tag 的 topk 的概率值，即当前时间步的每一种 tag 作为 from_target 的 topk 的概率值，扩展后相当于 topk 中每个 topn 对应的 (tag_size, tag_size) 的转移矩阵，这里的转移矩阵是由 (tag_size, 1) 的列向量扩展而来的，相当于无论当前时间步的 to_target 是什么， 特定 from_target 的转移概率是一样的</li>
<li>相加后调整为 (batch_size, tag_size * nbest, tag_size) </li>
</ul>
</li>
<li>接着取出 cur_values 中的 topk dim=1 的排序结果作为新的 partition 以及对应的 index -- cur_bp，即获得了下一状态的 topk 的可能性以及对应的 from_target ，size 均为 (batch_size, nbest, tag_size) <ul>
<li>由于已经将 size 调整为 (batch_size, tag_size * nbest, tag_size) ，所以 topk 的排序范围是 tag_size * nbest ，即对记录的 tag_size 个 size 为 [nbest, tag_size] 的转移矩阵纵向连接形成 (tag_size * nbest, tag_size) 的矩阵，而后对每列的 topk 取出数值与 index</li>
<li>这里的 index 的范围是 [0, tag_size * nbest - 1]，index / nbest 就可以得到范围为 [0, tag_size - 1] 的真实的 index ，即 from_target</li>
<li>例如from_target 为 5 的 top2 的 nbest 下的 index 就为 nbest * 5 + 2， index / nbest 后即可得到 from_target = 5</li>
</ul>
</li>
<li>将 partition 和 cur_bp 的 1 2 维对调，size 变为  (batch_size, tag_size, nbest)，并将 partition 保存在 partition_history 中</li>
<li>再通过 mask 将 cur_bp 中mask 为 0 的对应位置赋值为 0，并将 cur_bp 保存在 back_points 中</li>
</ul>
</li>
</ul>
</li>
<li>获得了 size 为 (batch_size, seq_len, nbest, tag_size) 的 partition_history，通过 length_mask 获得 size 为 (batch_size, 1, tag_size, nbest) 的 last_position，再从 partition_history 取出 sequence 末尾得到 last_partition 并调整其 size 为 (batch_size, tag_size, nbest, 1)，再扩展为 (batch_size, tag_size, nbest, tag_size) 并与转移矩阵 transitions 相加，得到序列的结尾处的转移矩阵 last_values </li>
<li>接着与遍历 scores 时类似，取出压缩后的 end_partition 和 end_bp 并取出 STOP_TAG 对应的矩阵，就获得了 size 为 (batch_size, nbest) 的pointer ，即最有可能转移至 STOP_TAG 的 from_target 。再将这一 from_target 放到 back_points 的末尾</li>
<li>最后计算 decode_idx，用于在 decode 阶段解析得到 decoded sequence<ul>
<li>pointer 是 decode_idx 的最后一项，因为其保存的是最有可能转移至 STOP_TAG 的 from_target ，即 end_id ，但由于之前已将 index 压缩，所以 <code>decode_idx[-1] = pointer.data / nbest</code> ，从而获得真正的 index</li>
<li>倒序解码时，后一时间步的 pointer 即为当前时间步的 to_target 了，所以对应从 back_points 中取得其 from_target 并 / nbest，然后保存在 decode_idx 中，以此类推</li>
<li>注意，此处的 pointer 需要保存 <code>the last end nbest ids for non longest</code><ul>
<li>所有的 back_points 保存的都是经过 mask 处理的 tag_id ，padding 的 tag_id 均为 0</li>
<li>对于 _viterbi_decode 的解码过程，pointer 扩展为 (batch_size, 1, tag_size) 后成为 insert_last 并放在 sequence 的 last_position 的位置，所以对于某一 sequence 而言，不管到达这一位置的 pointer 的对应位置是几，只要到达了该 sequence 的最后一个位置，都会得到其 tag，从而完成对应的解码</li>
<li>对于 nbest 时的解码过程，需要考虑 n 种情况，在解码的第一次更新 pointer 时，所有长度小于最大长度的 sequence 的 pointer 都会变为 0，而由于 back_points[idx] 被调整为 (batch_size, tag_size * nbest)， 无法对每个 nbest 都访问 0 以获得其 last end nbest ids ，压缩后所有的 0 都只能访问到同一个值，这只是 top1 的 last end id，那么就会丢失 last end nbest ids。所以需要将会被 0 覆盖掉的 last end nbest ids 保存并重新覆盖</li>
<li>使用的第一个 pointer 是所有 sequence 的最后一个 tag_id，即使 tag_id 并不是 back_points[idx] 的 to_target 也没关系，取得的 from_target 仍然会因为之前的 mask 处理 而为 0；随后的 pointer 亦然</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="reference">Reference<a class="headerlink" href="#reference" title="Permanent link">&para;</a></h2>
<p>《统计学习方法》</p>
<p><a href="http://www.cs.columbia.edu/~mcollins/crf.pdf">CRFS</a></p>
<p><a href="http://www.tensorinfinity.com/paper_170.html">理解条件随机场</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/29989121">条件随机场CRF</a></p>
<p><a href="https://www.jiqizhixin.com/articles/2018-05-23-3">简明条件随机场CRF介绍 | 附带纯Keras实现</a></p>
<p><a href="https://www.cnblogs.com/jiangxinyang/p/9368482.html">自然语言处理之序列标注问题</a></p>
                
              
              
                


  <h2 id="__comments">评论</h2>
  <div id="disqus_thread"></div>
  <script>var disqus_config=function(){this.page.url="None",this.page.identifier="None"};!function(){var e=document,i=e.createElement("script");i.src="//https-looperxx-github-io-my-wiki.disqus.com/embed.js",i.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(i)}()</script>

              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="">
        
          <a href="../Attention/" title="Attention" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20,11V13H8L13.5,18.5L12.08,19.92L4.16,12L12.08,4.08L13.5,5.5L8,11H20Z" /></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                Attention
              </div>
            </div>
          </a>
        
        
          <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" title="Machine Reading Comprehension" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  下一页
                </span>
                Machine Reading Comprehension
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4,11V13H16L10.5,18.5L11.92,19.92L19.84,12L11.92,4.08L10.5,5.5L16,11H4Z" /></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 - 2020 Xiao Xu
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="https://github.com/looperXX" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
      </a>
    
      
      
      <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.c51dfa35.min.js"></script>
      <script src="../assets/javascripts/bundle.eaaa3931.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "", "search.config.separator": "[\\uff0c\\u3002]+", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ["tabs"],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.58d22e8e.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
    
  </body>
</html>