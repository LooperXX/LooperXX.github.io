
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="LooperXX's homepage">
      
      
        <meta name="author" content="Looper - Xiao Xu">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.3">
    
    
      
        <title>[Presentation]LSTM - Science is interesting.</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.edf004c2.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.e6a45f82.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="preference" data-md-color-primary="" data-md-color-accent="">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-language-models" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href=".." title="Science is interesting." class="md-header__button md-logo" aria-label="Science is interesting." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Science is interesting.
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              [Presentation]LSTM
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href=".." class="md-tabs__link">
      Homepage
    </a>
  </li>

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../Attention/" class="md-tabs__link">
        Notes
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-tabs__link">
        Notes on CS224n
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../MkDocs_demo/" class="md-tabs__link">
        For MkDocs
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Science is interesting." class="md-nav__button md-logo" aria-label="Science is interesting." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Science is interesting.
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/LooperXX/LooperXX.github.io/" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    LooperXX/LooperXX.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Notes
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Notes" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Notes
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          Theory
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Theory" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          Theory
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Attention/" class="md-nav__link">
        Attention
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Normalization/" class="md-nav__link">
        Normalization
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          Code
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Code" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          Code
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Notes%20on%20NCRF%2B%2B/" class="md-nav__link">
        NCRF++
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_3">
          Book
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Book" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          Book
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Neural%20Reading%20Comprehension%20and%20beyond/" class="md-nav__link">
        Machine Reading Comprehension
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../NLP%20Concepts/" class="md-nav__link">
        Some Concepts
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../NNDL%20exercise/" class="md-nav__link">
        NNDL exercise
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Notes on CS224n
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Notes on CS224n" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Notes on CS224n
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019%20%E7%AE%80%E4%BB%8B/" class="md-nav__link">
        CS224n-2019 Introduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-Assignment/" class="md-nav__link">
        CS224n-2019 Assignment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-01-Introduction%20and%20Word%20Vectors/" class="md-nav__link">
        01 Introduction and Word Vectors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-02-Word%20Vectors%202%20and%20Word%20Senses/" class="md-nav__link">
        02 Word Vectors 2 and Word Senses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-03-Word%20Window%20Classification%2CNeural%20Networks%2C%20and%20Matrix%20Calculus/" class="md-nav__link">
        03 Word Window Classification,Neural Networks, and Matrix Calculus
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-04-Backpropagation%20and%20Computation%20Graphs/" class="md-nav__link">
        04 Backpropagation and Computation Graphs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-05-Linguistic%20Structure%20Dependency%20Parsing/" class="md-nav__link">
        05 Linguistic Structure Dependency Parsing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-06-The%20probability%20of%20a%20sentence%20Recurrent%20Neural%20Networks%20and%20Language%20Models/" class="md-nav__link">
        06 The probability of a sentence Recurrent Neural Networks and Language Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-07-Vanishing%20Gradients%20and%20Fancy%20RNNs/" class="md-nav__link">
        07 Vanishing Gradients and Fancy RNNs
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-08-Machine%20Translation%2C%20Sequence-to-sequence%20and%20Attention/" class="md-nav__link">
        08 Machine Translation, Sequence-to-sequence and Attention
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-09-Practical%20Tips%20for%20Final%20Projects/" class="md-nav__link">
        09 Practical Tips for Final Projects
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-10-Question%20Answering%20and%20the%20Default%20Final%20Project/" class="md-nav__link">
        10 Question Answering and the Default Final Project
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-11-ConvNets%20for%20NLP/" class="md-nav__link">
        11 ConvNets for NLP
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-12-Information%20from%20parts%20of%20words%20Subword%20Models/" class="md-nav__link">
        12 Information from parts of words Subword Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-13-Modeling%20contexts%20of%20use%20Contextual%20Representations%20and%20Pretraining/" class="md-nav__link">
        13 Modeling contexts of use Contextual Representations and Pretraining
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-14-Transformers%20and%20Self-Attention%20For%20Generative%20Models/" class="md-nav__link">
        14 Transformers and Self-Attention For Generative Models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-15-Natural%20Language%20Generation/" class="md-nav__link">
        15 Natural Language Generation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-16-Coreference%20Resolution/" class="md-nav__link">
        16 Coreference Resolution
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-17-Multitask%20Learning/" class="md-nav__link">
        17 Multitask Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-18-Tree%20Recursive%20Neural%20Networks%2C%20Constituency%20Parsing%2C%20and%20Sentiment/" class="md-nav__link">
        18 Tree Recursive Neural Networks, Constituency Parsing, and Sentiment
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-19-Safety%2C%20Bias%2C%20and%20Fairness/" class="md-nav__link">
        19 Safety, Bias, and Fairness
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../CS224n-2019-20-The%20Future%20of%20NLP%20%2B%20Deep%20Learning/" class="md-nav__link">
        20 The Future of NLP + Deep Learning
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          For MkDocs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="For MkDocs" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          For MkDocs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../MkDocs_demo/" class="md-nav__link">
        Demo
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Material%20Theme%20Tutorial/" class="md-nav__link">
        Material Theme Tutorial
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-language-models" class="md-nav__link">
    1 Language Models
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-recurrent-neural-networks-rnn" class="md-nav__link">
    2 Recurrent Neural Networks (RNN)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-long-short-term-memory-lstm" class="md-nav__link">
    3 Long Short-Term Memory (LSTM)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-recap" class="md-nav__link">
    4 Recap
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/LooperXX/LooperXX.github.io/edit/master/docs/[Presentation]LSTM.md" title="编辑此页" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


  <h1>[Presentation]LSTM</h1>

<h3 id="1-language-models">1 Language Models<a class="headerlink" href="#1-language-models" title="Permanent link">&para;</a></h3>
<p><strong>1.1 Introduction</strong></p>
<p>语言模型计算特定序列中多个单词的出现概率。一个 m 个单词的序列 <span class="arithmatex"><span class="MathJax_Preview">\left\{w_{1}, \dots, w_{m}\right\}</span><script type="math/tex">\left\{w_{1}, \dots, w_{m}\right\}</script></span> 的概率定义为 <span class="arithmatex"><span class="MathJax_Preview">P\left(w_{1}, \dots, w_{m}\right)</span><script type="math/tex">P\left(w_{1}, \dots, w_{m}\right)</script></span> 。单词 <span class="arithmatex"><span class="MathJax_Preview">w_i</span><script type="math/tex">w_i</script></span> 前有一定数量的单词，其特性会根据它在文档中的位置而改变， <span class="arithmatex"><span class="MathJax_Preview">P\left(w_{1}, \dots, w_{m}\right)</span><script type="math/tex">P\left(w_{1}, \dots, w_{m}\right)</script></span> 一般只考虑前 <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> 个单词而不是考虑全部之前的单词。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(w_{1}, \ldots, w_{m}\right)=\prod_{i=1}^{i=m} P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right) \approx \prod_{i=1}^{i=m} P\left(w_{i} | w_{i-n}, \ldots, w_{i-1}\right)
</div>
<script type="math/tex; mode=display">
P\left(w_{1}, \ldots, w_{m}\right)=\prod_{i=1}^{i=m} P\left(w_{i} | w_{1}, \ldots, w_{i-1}\right) \approx \prod_{i=1}^{i=m} P\left(w_{i} | w_{i-n}, \ldots, w_{i-1}\right)
</script>
</div>
<p>上面的公式在语音识别和机器翻译系统对判断一个词序列是否为一个输入句子的准确翻译起到了重要的作用。在现有的机器翻译系统中，对每个短语/句子翻译，系统生成一些候选的词序列（例如，{I have,I has,I had,me have,me had}），并对其评分以确定最可能的翻译序列。</p>
<p>在机器翻译中，对一个输入短语，通过评判每个候选输出词序列的得分的高低，来选出最好的词顺序。为此，模型可以在不同的单词排序或单词选择之间进行选择。它将通过一个概率函数运行所有单词序列候选项，并为每个候选项分配一个分数，从而实现这一目标。最高得分的序列就是翻译结果。例如，相比 small is the cat，翻译系统会给 the cat is small 更高的得分；相比 walking house after school，翻译系统会给 walking home after school 更高的得分。</p>
<p><strong>1.2 n-gram Language Models</strong></p>
<p>为了计算这些概率，每个 n-gram 的计数将与每个单词的频率进行比较，这个称为 n-gram 语言模型。例如，如果选择 bi-gram 模型，每一个 bi-gram 的频率，通过将单词与其前一个单词相结合进行计算，然后除以对应的 uni-gram 的频率。下面的两个公式展示了 bi-gram 模型和 tri-gram 模型的区别。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned} p\left(w_{2} | w_{1}\right) &amp;=\frac{\operatorname{count}\left(w_{1}, w_{2}\right)}{\operatorname{count}\left(w_{1}\right)} \\ p\left(w_{3} | w_{1}, w_{2}\right) &amp;=\frac{\operatorname{count}\left(w_{1}, w_{2}, w_{3}\right)}{\operatorname{count}\left(w_{1}, w_{2}\right)} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} p\left(w_{2} | w_{1}\right) &=\frac{\operatorname{count}\left(w_{1}, w_{2}\right)}{\operatorname{count}\left(w_{1}\right)} \\ p\left(w_{3} | w_{1}, w_{2}\right) &=\frac{\operatorname{count}\left(w_{1}, w_{2}, w_{3}\right)}{\operatorname{count}\left(w_{1}, w_{2}\right)} \end{aligned}
</script>
</div>
<p>上式 tri-gram 模型的关系主要是基于一个固定的上下文窗口(即前n个单词)预测下一个单词。一般 n 的取值为多大才好呢？在某些情况下，前面的连续的 n 个单词的窗口可能不足以捕获足够的上下文信息。例如，考虑句子（类似完形填空，预测下一个最可能的单词）</p>
<p>“Asthe proctor started the clock, the students opened their ____”。如果窗口只是基于前面的三个单词“the students opened their”，那么急于这些语料计算的下划线中最有可能出现的单词就是为“books”——但是如果 n 足够大，能包括全部的上下文，那么下划线中最优可能出现的单词会是“exam”。</p>
<p>这就引出了 n-gram 语言模型的两个主要问题：稀疏性和存储。</p>
<p><strong>Sparsity problems with n-gram Language models</strong></p>
<p>n-gram 语言模型的问题源于两个问题。</p>
<p>首先，注意公式中的分子。如果 <span class="arithmatex"><span class="MathJax_Preview">w_1,w_2.w_3</span><script type="math/tex">w_1,w_2.w_3</script></span> 在语料中从未出现过，那么 <span class="arithmatex"><span class="MathJax_Preview">w_3</span><script type="math/tex">w_3</script></span> 的概率就是 0。为了解决这个问题，在每个单词计数后面加上一个很小的 <span class="arithmatex"><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> ，这就是平滑操作。</p>
<p>然后，考虑公式中的分母。 如果 <span class="arithmatex"><span class="MathJax_Preview">w_1,w_2</span><script type="math/tex">w_1,w_2</script></span> 在语料中从未出现过，那么 <span class="arithmatex"><span class="MathJax_Preview">w_3</span><script type="math/tex">w_3</script></span> 的概率将会无法计算。为了解决这个问题，这里可以只是单独考虑 <span class="arithmatex"><span class="MathJax_Preview">w_2</span><script type="math/tex">w_2</script></span> ，这就是“backoff”操作。</p>
<p>增加 n 会让稀疏问题更加严重，所以一般 <span class="arithmatex"><span class="MathJax_Preview">n \leq 5</span><script type="math/tex">n \leq 5</script></span> 。</p>
<p><strong>Storage problems with n-gram Language models</strong></p>
<p>我们知道需要存储在语料库中看到的所有 n-gram 的统计数。随着 n 的增加(或语料库大小的增加)，模型的大小也会增加。</p>
<p><strong>1.3 Window-based Neural Language Model</strong></p>
<p>Bengio 的论文《A Neural Probabilistic Language Model》中首次解决了上面所说的“维度灾难”，这篇论文提出一个自然语言处理的大规模的深度学习模型，这个模型能够通过学习单词的分布式表示，以及用这些表示来表示单词的概率函数。下图展示了对应的神经网络结构，在这个模型中，输入向量在隐藏层和输出层中都被使用。</p>
<p><img alt="1560997851163" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1560997851163.png" /></p>
<p>下面公式展示了由标准 tanh 函数（即隐藏层）组成的 softmax 函数的参数以及线性函数 <span class="arithmatex"><span class="MathJax_Preview">W^{(3)} x+b^{(3)}</span><script type="math/tex">W^{(3)} x+b^{(3)}</script></span> ，捕获所有前面 n 个输入词向量。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{y}=\operatorname{softmax}\left(W^{(2)} \tanh \left(W^{(1)}x+b^{(1)}\right)+W^{(3)} x+b^{(3)}\right)
</div>
<script type="math/tex; mode=display">
\hat{y}=\operatorname{softmax}\left(W^{(2)} \tanh \left(W^{(1)}x+b^{(1)}\right)+W^{(3)} x+b^{(3)}\right)
</script>
</div>
<p>注意权重矩阵 <span class="arithmatex"><span class="MathJax_Preview">W^{(1)}</span><script type="math/tex">W^{(1)}</script></span> 是应用在词向量上（上图中的绿色实线箭头）， <span class="arithmatex"><span class="MathJax_Preview">W^{(2)}</span><script type="math/tex">W^{(2)}</script></span> 是应用在隐藏层（也是绿色实线箭头）和 <span class="arithmatex"><span class="MathJax_Preview">W^{(3)}</span><script type="math/tex">W^{(3)}</script></span> 是应用在词向量（绿色虚线箭头）。</p>
<p>这个模型的简化版本如下图所示，其中蓝色的层表示输入单词的 embedding 拼接： <span class="arithmatex"><span class="MathJax_Preview">e=\left[e^{(1)} ; e^{(2)} ; e^{(3)} ; e^{(4)}\right]</span><script type="math/tex">e=\left[e^{(1)} ; e^{(2)} ; e^{(3)} ; e^{(4)}\right]</script></span> ，红色的层表示隐藏层： <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{h}=f\left(\boldsymbol{W} e+\boldsymbol{b}_{1}\right)</span><script type="math/tex">\boldsymbol{h}=f\left(\boldsymbol{W} e+\boldsymbol{b}_{1}\right)</script></span> ，绿色的输出分布是对词表的一个 softmax 概率分布： <span class="arithmatex"><span class="MathJax_Preview">\hat{y}=\operatorname{softmax}\left(U h+b_{2}\right)</span><script type="math/tex">\hat{y}=\operatorname{softmax}\left(U h+b_{2}\right)</script></span> 。</p>
<p><img alt="1560998151143" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1560998151143.png" /></p>
<h3 id="2-recurrent-neural-networks-rnn">2 Recurrent Neural Networks (RNN)<a class="headerlink" href="#2-recurrent-neural-networks-rnn" title="Permanent link">&para;</a></h3>
<p>传统的翻译模型只能以有限窗口大小的前 n 个单词作为条件进行语言模型建模，循环神经网络与其不同，RNN 有能力以语料库中所有前面的单词为条件进行语言模型建模。</p>
<p>下图展示的 RNN 的架构，其中矩形框是在一个时间步的一个隐藏层 t。</p>
<p><img alt="1560998193031" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1560998193031.png" /></p>
<p>每个这样的隐藏层都有若干个神经元，每个神经元对输入向量用一个线性矩阵运算然后通过非线性变化（例如 tanh 函数）得到输出。在每一个时间步，隐藏层都有两个输入：前一个时间步的隐藏层 <span class="arithmatex"><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 和当前时间步的输入 <span class="arithmatex"><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> ，前一个时间步的隐藏层 <span class="arithmatex"><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 通过和权重矩阵 <span class="arithmatex"><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> 相乘和当前时间步的输入 <span class="arithmatex"><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 和权重矩阵 <span class="arithmatex"><span class="MathJax_Preview">W^{(hx)}</span><script type="math/tex">W^{(hx)}</script></span> 相乘得到当前时间步的隐藏层 <span class="arithmatex"><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> ，然后再将 <span class="arithmatex"><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 和权重矩阵 <span class="arithmatex"><span class="MathJax_Preview">W^{(S)}</span><script type="math/tex">W^{(S)}</script></span> 相乘，接着对整个词表通过 softmax 计算得到下一个单词的预测结果 <span class="arithmatex"><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span> ，如下面公式所示：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned} h_{t} &amp;=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{[t]}\right) \\ \hat{y} &amp;=\operatorname{softmax}\left(W^{(S)} h_{t}\right) \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} h_{t} &=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{[t]}\right) \\ \hat{y} &=\operatorname{softmax}\left(W^{(S)} h_{t}\right) \end{aligned}
</script>
</div>
<p>每个神经元的输入和输出如下图所示：</p>
<p><img alt="1560998383025" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1560998383025.png" /></p>
<p>在这里一个有意思的地方是在每一个时间步使用相同的权重 <span class="arithmatex"><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">W^{(hx)}</span><script type="math/tex">W^{(hx)}</script></span> 。这样模型需要学习的参数就变少了，这与输入序列的长度无关——这从而解决了维度灾难。</p>
<p>以下是网络中每个参数相关的详细信息：</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">x_{1}, \dots, x_{t-1}, x_{t}, x_{t+1}, \dots x_{T}</span><script type="math/tex">x_{1}, \dots, x_{t-1}, x_{t}, x_{t+1}, \dots x_{T}</script></span> ：含有 T 个单词的语料库对应的词向量。</li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{t}=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{t}\right)</span><script type="math/tex">h_{t}=\sigma\left(W^{(h h)} h_{t-1}+W^{(h x)} x_{t}\right)</script></span> ：每个时间步 t 的隐藏层的输出特征的计算关系</li>
<li><span class="arithmatex"><span class="MathJax_Preview">x_{t} \in \mathbb{R}^{d}</span><script type="math/tex">x_{t} \in \mathbb{R}^{d}</script></span> ：在时间步 t 的输入词向量。</li>
<li><span class="arithmatex"><span class="MathJax_Preview">W^{h x} \in \mathbb{R}^{D_{h} \times d}</span><script type="math/tex">W^{h x} \in \mathbb{R}^{D_{h} \times d}</script></span> ：输入词向量 <span class="arithmatex"><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 对应的权重矩阵。</li>
<li><span class="arithmatex"><span class="MathJax_Preview">W^{h h} \in \mathbb{R}^{D_{h} \times D_{h}}</span><script type="math/tex">W^{h h} \in \mathbb{R}^{D_{h} \times D_{h}}</script></span> ：上一个时间步的输出 <span class="arithmatex"><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> 对应的权重矩阵。</li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{t-1} \in \mathbb{R}^{D_{h}}</span><script type="math/tex">h_{t-1} \in \mathbb{R}^{D_{h}}</script></span> ：上一个时间步 t-1 的非线性函数输出。 <span class="arithmatex"><span class="MathJax_Preview">h_{0} \in \mathbb{R}^{D_{h}}</span><script type="math/tex">h_{0} \in \mathbb{R}^{D_{h}}</script></span> 是在时间步 t=0 的隐藏层的一个初始化向量。</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> ：非线性函数（这里是 sigmoid 函数）。</p>
</li>
<li>
<p><span class="arithmatex"><span class="MathJax_Preview">\hat{y}=\operatorname{softmax}\left(W^{(S)} h_{t}\right)</span><script type="math/tex">\hat{y}=\operatorname{softmax}\left(W^{(S)} h_{t}\right)</script></span> ：在每个时间步 t 全部单词的概率分布输出。本质上， <span class="arithmatex"><span class="MathJax_Preview">\hat y</span><script type="math/tex">\hat y</script></span> 是给定文档上下文分数（例如 <span class="arithmatex"><span class="MathJax_Preview">h_{t-1}</span><script type="math/tex">h_{t-1}</script></span> ）和最后观测的词向量 <span class="arithmatex"><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> ，对一个出现单词的预测。这里， <span class="arithmatex"><span class="MathJax_Preview">W^{(S)} \in \mathbb{R}^{|V| \times D_{h}}</span><script type="math/tex">W^{(S)} \in \mathbb{R}^{|V| \times D_{h}}</script></span> ， <span class="arithmatex"><span class="MathJax_Preview">\hat{y} \in \mathbb{R}^{|V|}</span><script type="math/tex">\hat{y} \in \mathbb{R}^{|V|}</script></span> ，其中 <span class="arithmatex"><span class="MathJax_Preview">|V|</span><script type="math/tex">|V|</script></span> 是词汇表的大小。</p>
</li>
</ul>
<p>一个 RNN 语言模型的例子如下图所示。下图中的符号有一些的不同： <span class="arithmatex"><span class="MathJax_Preview">W_h</span><script type="math/tex">W_h</script></span> 等同于 <span class="arithmatex"><span class="MathJax_Preview">W^{(hh)}</span><script type="math/tex">W^{(hh)}</script></span> ，  <span class="arithmatex"><span class="MathJax_Preview">W_e</span><script type="math/tex">W_e</script></span> 等同于 <span class="arithmatex"><span class="MathJax_Preview">W^{(hx)}</span><script type="math/tex">W^{(hx)}</script></span> ， <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> 等同于 <span class="arithmatex"><span class="MathJax_Preview">W^{(S)}</span><script type="math/tex">W^{(S)}</script></span> 。 <span class="arithmatex"><span class="MathJax_Preview">E</span><script type="math/tex">E</script></span> 表示单词输入 <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> 转化为 <span class="arithmatex"><span class="MathJax_Preview">e^{(t)}</span><script type="math/tex">e^{(t)}</script></span> 。</p>
<p>在 RNN 中常用的损失函数是在之前介绍过的交叉熵误差。下面的公式是这个函数在时间步 t 全部单词的求和。最后计算词表中的 softmax 计算结果展示了基于前面所有的单词对输出单词 <span class="arithmatex"><span class="MathJax_Preview">x^{(5)}</span><script type="math/tex">x^{(5)}</script></span> 的不同选择的概率分布。这时的输入可以比 4 到 5 个单词更长。</p>
<p><img alt="1560998943983" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1560998943983.png" /></p>
<p><strong>2.1 RNN Loss and Perplexity</strong> </p>
<p>RNN 的损失函数一般是交叉熵误差。</p>
<div class="arithmatex">
<div class="MathJax_Preview">
J^{(t)}(\theta)=\sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</div>
<script type="math/tex; mode=display">
J^{(t)}(\theta)=\sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</script>
</div>
<p>在大小为 T 的语料库上的交叉熵误差的计算如下：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
J=-\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</div>
<script type="math/tex; mode=display">
J=-\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=-\frac{1}{T} \sum_{t=1}^{T} \sum_{j=1}^{|V|} y_{t, j} \times \log \left(\hat{y}_{t, j}\right)
</script>
</div>
<p><strong>2.2 Advantages, Disadvantages and Applications of RNNs</strong></p>
<p>RNN 有以下优点：</p>
<ol>
<li>它可以处理任意长度的序列</li>
<li>对更长的输入序列不会增加模型的参数大小</li>
<li>对时间步 t 的计算理论上可以利用前面很多时间步的信息</li>
<li>对输入的每个时间步都应用相同的权重，因此在处理输入时具有对称性</li>
</ol>
<p>但是 RNN 也有以下缺点：</p>
<ol>
<li>计算速度很慢——因为它每一个时间步需要依赖上一个时间步，所以不能并行化</li>
<li>在实际中因为梯度消失和梯度爆炸，很难利用到前面时间步的信息。</li>
</ol>
<p>运行一层 RNN 所需的内存量与语料库中的单词数成正比。例如，我们把一个句子是为一个 mini batch，那么一个有 k 个单词的句子在内存中就会占用 k 个词向量的存储空间。同时，RNN 必须维持两对 <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> 矩阵。然而 <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 的可能是非常大的，它的大小不会随着语料库的大小而变化（与传统的语言模型不一样）。对于具有 1000 个循环层的 RNN，矩阵 <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> 的大小为 <span class="arithmatex"><span class="MathJax_Preview">1000 \times 1000</span><script type="math/tex">1000 \times 1000</script></span> 而与语料库大小无关。</p>
<p>RNN 可以应用在很多任务，例如标注任务（词性标注、命名实体识别），句子分类（情感分类），编码模块（问答任务，机器翻译和其他很多任务）。在后面的两个任务，我们希望得到对句子的表示，这时可以通过采用该句子中时间步长的所有隐藏状态的 <span class="arithmatex"><span class="MathJax_Preview">element-wise</span><script type="math/tex">element-wise</script></span> 的最大值或平均值来获得。</p>
<p>下图是一些出版物中对 RNN 模型的另外一种表示。它将 RNN 的每个隐层用一个环来表示。</p>
<p><img alt="1560999097493" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1560999097493.png" /></p>
<p><strong>Vanishing gradient intuition</strong></p>
<p><img alt="1561034883163" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561034883163.png" /></p>
<ul>
<li>当这些梯度很小的时候，反向传播的越深入，梯度信号就会变得越来越小</li>
</ul>
<p><strong>Vanishing gradient proof sketch</strong>
$$
\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}<em>{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}</em>{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)
$$</p>
<ul>
<li>因此通过链式法则得到：</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}}=\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right) \boldsymbol{W}_{h}
</div>
<script type="math/tex; mode=display">
\frac{\partial \boldsymbol{h}^{(t)}}{\partial \boldsymbol{h}^{(t-1)}}=\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right) \boldsymbol{W}_{h}
</script>
</div>
<ul>
<li>考虑第 i 步上的损失梯度 <span class="arithmatex"><span class="MathJax_Preview">J^{(i)}(\theta)</span><script type="math/tex">J^{(i)}(\theta)</script></span>，相对于第 j 步上的隐藏状态 <span class="arithmatex"><span class="MathJax_Preview">\boldsymbol{h}^{(j)}</span><script type="math/tex">\boldsymbol{h}^{(j)}</script></span> </li>
</ul>
<p><img alt="1561035298878" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561035298878.png" /></p>
<p>如果权重矩阵 <span class="arithmatex"><span class="MathJax_Preview">W_h</span><script type="math/tex">W_h</script></span> 很小，那么这一项也会随着 i 和 j 的距离越来越远而变得越来越小</p>
<ul>
<li>考虑矩阵的 L2 范数</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\left\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}}\right\| \leq\left\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}}\right\|\left\|\boldsymbol{W}_{h}\right\|^{(i-j)} \prod_{j&lt;t \leq i}\left\|\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right)\right\|
</div>
<script type="math/tex; mode=display">
\left\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}}\right\| \leq\left\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(i)}}\right\|\left\|\boldsymbol{W}_{h}\right\|^{(i-j)} \prod_{j<t \leq i}\left\|\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right)\right\|
</script>
</div>
<ul>
<li>Pascanu et al 表明，如果 <span class="arithmatex"><span class="MathJax_Preview">W_h</span><script type="math/tex">W_h</script></span> 的 <strong>最大特征值 &lt; 1</strong> ，梯度 <span class="arithmatex"><span class="MathJax_Preview">\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}}\|</span><script type="math/tex">\|\frac{\partial J^{(i)}(\theta)}{\partial \boldsymbol{h}^{(j)}}\|</script></span> 将呈指数衰减<ul>
<li>这里的界限是1因为我们使用的非线性函数是 sigmoid</li>
</ul>
</li>
<li>有一个类似的证明将一个 <strong>最大的特征值 &gt;1</strong> 与 <strong>梯度爆炸</strong> 联系起来</li>
</ul>
<p><strong>Why is vanishing gradient a problem?</strong></p>
<p><img alt="1561035720401" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561035720401.png" /></p>
<ul>
<li>来自远处的梯度信号会丢失，因为它比来自近处的梯度信号小得多。</li>
<li>
<p>因此，模型权重只会根据近期效应而不是长期效应进行更新。</p>
</li>
<li>
<p><u>另一种解释</u> ：<strong>梯度</strong> 可以被看作是 <strong>过去对未来的影响</strong> 的衡量标准</p>
</li>
<li>如果梯度在较长一段距离内(从时间步 t 到 t+n )变得越来越小，那么我们就不能判断:<ul>
<li>在数据中，步骤 t 和 t+n 之间没有依赖关系</li>
<li>我们用 <strong>错误的参数</strong> 来捕获 t 和 t+n 之间的真正依赖关系</li>
</ul>
</li>
</ul>
<p><strong>Effect of vanishing gradient on RNN-LM</strong></p>
<ul>
<li>语言模型任务</li>
</ul>
<p><img alt="1561035987833" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561035987833.png" /></p>
<ul>
<li>为了从这个训练示例中学习，RNN-LM需要对第7步的“tickets”和最后的目标单词“tickets”之间的依赖关系建模。</li>
<li>但是如果梯度很小，模型就 <strong>不能学习这种依赖关系</strong><ul>
<li>因此模型无法在测试时 <strong>预测类似的长距离依赖关系</strong></li>
</ul>
</li>
</ul>
<p><img alt="1561039117289" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561039117289.png" /></p>
<ul>
<li><strong>Correct answer</strong> : The writer of the books is planning a sequel</li>
<li><strong>语法近因</strong></li>
</ul>
<p><img alt="1561039297137" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561039297137.png" /></p>
<ul>
<li><strong>顺序近因</strong></li>
</ul>
<p><img alt="1561039357717" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561039357717.png" /></p>
<ul>
<li>由于梯度的消失，RNN-LMs更善于从 <strong>顺序近因</strong> 学习而不是 <strong>语法近因</strong> ，所以他们犯这种错误的频率比我们希望的要高[Linzen et al . 2016]</li>
</ul>
<p><strong>Why is <u>exploding</u> gradient a problem?</strong></p>
<ul>
<li>如果梯度过大，则SGD更新步骤过大</li>
</ul>
<p><img alt="1561039536872" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561039536872.png" /></p>
<ul>
<li>这可能导致 <strong>错误的更新</strong> ：我们更新的太多，导致错误的参数配置(损失很大)</li>
<li>在最坏的情况下，这将导致网络中的 <strong>Inf</strong> 或 <strong>NaN</strong> (然后您必须从较早的检查点重新启动训练)</li>
</ul>
<p><strong>Gradient clipping: solution for exploding gradient</strong></p>
<ul>
<li><strong>梯度裁剪</strong> ：如果梯度的范数大于某个阈值，在应用SGD更新之前将其缩小</li>
</ul>
<p><img alt="1561039730507" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561039730507.png" /></p>
<ul>
<li><strong>直觉</strong> ：朝着同样的方向迈出一步，但要小一点</li>
</ul>
<p><img alt="1561039784182" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561039784182.png" /></p>
<ul>
<li>这显示了一个简单RNN的损失面(隐藏层状态是一个标量不是一个向量)</li>
<li>“悬崖”是危险的，因为它有陡坡</li>
<li>在左边，由于陡坡，梯度下降有两个非常大的步骤，导致攀登悬崖然后向右射击(都是胡浩的更新)</li>
<li>在右边，梯度剪裁减少了这些步骤的大小,所以效果不太激烈</li>
</ul>
<p><strong>How to fix vanishing gradient problem?</strong></p>
<ul>
<li>主要问题是RNN很难学习在多个时间步长的情况下保存信息</li>
<li>在普通的RNN中，隐藏状态不断被重写</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}\right)
</div>
<script type="math/tex; mode=display">
\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\boldsymbol{b}\right)
</script>
</div>
<ul>
<li>一个具有独立记忆的RNN怎么样？</li>
</ul>
<h3 id="3-long-short-term-memory-lstm">3 Long Short-Term Memory (LSTM)<a class="headerlink" href="#3-long-short-term-memory-lstm" title="Permanent link">&para;</a></h3>
<ul>
<li>Hochreiter和Schmidhuber在1997年提出了一种RNN，用于解决梯度消失问题。</li>
<li>在第 t 步，有一个隐藏状态 <span class="arithmatex"><span class="MathJax_Preview">h^{(t)}</span><script type="math/tex">h^{(t)}</script></span> 和一个单元状态 <span class="arithmatex"><span class="MathJax_Preview">c^{(t)}</span><script type="math/tex">c^{(t)}</script></span> <ul>
<li>都是长度为 n 的向量</li>
<li>单元存储长期信息</li>
<li>LSTM可以从单元中删除、写入和读取信息</li>
</ul>
</li>
<li>信息被 擦除 / 写入 / 读取 的选择由三个对应的门控制<ul>
<li>门也是长度为 n 的向量</li>
<li>在每个时间步长上，门的每个元素可以打开(1)、关闭(0)或介于两者之间</li>
<li>门是动态的：它们的值是基于当前上下文计算的</li>
</ul>
</li>
</ul>
<p>我们有一个输入序列 <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> ，我们将计算一个隐藏状态 <span class="arithmatex"><span class="MathJax_Preview">h^{(t)}</span><script type="math/tex">h^{(t)}</script></span> 和单元状态 <span class="arithmatex"><span class="MathJax_Preview">c^{(t)}</span><script type="math/tex">c^{(t)}</script></span> 的序列。在时间步 t 时</p>
<p><img alt="1561040420399" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561040420399.png" /></p>
<ul>
<li>遗忘门：控制上一个单元状态的保存与遗忘</li>
<li>输入门：控制写入单元格的新单元内容的哪些部分</li>
<li>输出门：控制单元的哪些内容输出到隐藏状态</li>
<li>新单元内容：这是要写入单元的新内容</li>
<li>单元状态：删除(“忘记”)上次单元状态中的一些内容，并写入(“输入”)一些新的单元内容</li>
<li>隐藏状态：从单元中读取(“output”)一些内容</li>
<li>Sigmoid函数：所有的门的值都在0到1之间</li>
<li>通过逐元素的乘积来应用门</li>
<li>这些是长度相同的向量</li>
</ul>
<p>你可以把LSTM方程想象成这样：</p>
<p><img alt="1561040704665" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561040704665.png" /></p>
<p><strong>How does LSTM solve vanishing gradients?</strong></p>
<ul>
<li>RNN的LSTM架构更容易保存许多时间步上的信息<ul>
<li>如果忘记门设置为记得每一时间步上的所有信息，那么单元中的信息被无限地保存</li>
<li>相比之下，普通RNN更难学习重复使用并且在隐藏状态中保存信息的矩阵 <span class="arithmatex"><span class="MathJax_Preview">W_h</span><script type="math/tex">W_h</script></span></li>
</ul>
</li>
<li>LSTM并不保证没有消失/爆炸梯度，但它确实为模型提供了一种更容易的方法来学习远程依赖关系</li>
</ul>
<h3 id="4-recap">4 Recap<a class="headerlink" href="#4-recap" title="Permanent link">&para;</a></h3>
<p>Long-Short-Term-Memories 是和 GRU 有一点不同的另外一种类型的复杂激活神经元。它的作用与 GRU 类似，但是神经元的结构有一点区别。我们首先来看看 LSTM 神经元的数学公式，然后再深入了解这个神经元的设计架构：</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
{i_{t}} &amp; {=\sigma\left(W^{(i)} x_{t}+U^{(i)} h_{t-1}\right)} &amp; {\text { (Input gate) }} \\
{f_{t}} &amp; {=\sigma\left(W^{(f)} x_{t}+U^{(f)} h_{t-1}\right)} &amp; {\text { (Forget gate) }} \\ 
{o_{t}} &amp; {=\sigma\left(W^{(o)} x_{t}+U^{(o)} h_{t-1}\right)} &amp; {\text { (Output/Exposure gate) }} \\
{\tilde{c}_{t}} &amp; {=\tanh \left(W^{(c)} x_{t}+U^{(c)} h_{t-1}\right)} &amp; {\text { (New memory cell) }} \\ 
{c_{t}} &amp; {=f_{t} \circ c_{t-1}+i_{t} \circ \tilde{c}_{t}} &amp; {\text { (Final memory cell) }} \\ 
{h_{t}} &amp; {=o_{t} \circ \tanh \left(c_{t}\right)}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
{i_{t}} & {=\sigma\left(W^{(i)} x_{t}+U^{(i)} h_{t-1}\right)} & {\text { (Input gate) }} \\
{f_{t}} & {=\sigma\left(W^{(f)} x_{t}+U^{(f)} h_{t-1}\right)} & {\text { (Forget gate) }} \\ 
{o_{t}} & {=\sigma\left(W^{(o)} x_{t}+U^{(o)} h_{t-1}\right)} & {\text { (Output/Exposure gate) }} \\
{\tilde{c}_{t}} & {=\tanh \left(W^{(c)} x_{t}+U^{(c)} h_{t-1}\right)} & {\text { (New memory cell) }} \\ 
{c_{t}} & {=f_{t} \circ c_{t-1}+i_{t} \circ \tilde{c}_{t}} & {\text { (Final memory cell) }} \\ 
{h_{t}} & {=o_{t} \circ \tanh \left(c_{t}\right)}
\end{aligned}
</script>
</div>
<p>下图是LSTM的计算图示</p>
<p><img alt="1561010965268" src="/Users/looper/Documents/笔记/blog - Markdown笔记/docs/imgs/1561010965268.png" /></p>
<p>我们可以通过以下步骤了解 LSTM 的架构以及这个架构背后的意义：</p>
<ol>
<li><strong>New memory generation</strong>：这个阶段是类似于 GRU 生成新的记忆的阶段。我们基本上是用输入单词 <span class="arithmatex"><span class="MathJax_Preview">x_t</span><script type="math/tex">x_t</script></span> 和过去的隐藏状态来生成一个包括新单词 <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> 的新的记忆 <span class="arithmatex"><span class="MathJax_Preview">\tilde{c}_{t}</span><script type="math/tex">\tilde{c}_{t}</script></span> 。</li>
<li><strong>Input Gate</strong>：我们看到在生成新的记忆之前，新的记忆的生成阶段不会检查新单词是否重要——这需要输入门函数来做这个判断。输入门使用输入词和过去的隐藏状态来决定输入值是否值得保存，从而用来进入新内存。因此，它产生它作为这个信息的指示器。</li>
<li><strong>Forget Gate</strong>：这个门与输入门类似，只是它不确定输入单词的有用性——而是评估过去的记忆是否对当前记忆的计算有用。因此，遗忘门查看输入单词和过去的隐藏状态，并生成 <span class="arithmatex"><span class="MathJax_Preview">f_t</span><script type="math/tex">f_t</script></span>。</li>
<li><strong>Final memory generation</strong>：这个阶段首先根据忘记门 <span class="arithmatex"><span class="MathJax_Preview">f_t</span><script type="math/tex">f_t</script></span> 的判断，相应地忘记过去的记忆  <span class="arithmatex"><span class="MathJax_Preview">c_{t-1}</span><script type="math/tex">c_{t-1}</script></span> 。类似地，根据输入门  <span class="arithmatex"><span class="MathJax_Preview">i_t</span><script type="math/tex">i_t</script></span> 的判断，相应地输入新的记忆 <span class="arithmatex"><span class="MathJax_Preview">\tilde c_t</span><script type="math/tex">\tilde c_t</script></span> 。然后将上面的两个结果相加生成最终的记忆 <span class="arithmatex"><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 。</li>
<li><strong>Output/Exposure Gate</strong>：这是 GRU 中没有明确存在的门。这个门的目的是从隐藏状态中分离最终的记忆。最终的记忆 <span class="arithmatex"><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 包含很多不需要存储在隐藏状态的信息。隐藏状态用于 LSTM 的每个单个门，因此，该门是要评估关于记忆单元 <span class="arithmatex"><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 的哪些部分需要显露在隐藏状态 <span class="arithmatex"><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 中。用于评估的信号是 <span class="arithmatex"><span class="MathJax_Preview">o_t</span><script type="math/tex">o_t</script></span> ，然后与 <span class="arithmatex"><span class="MathJax_Preview">c_t</span><script type="math/tex">c_t</script></span> 通过 <span class="arithmatex"><span class="MathJax_Preview">o_{t} \circ \tanh \left(c_{t}\right)</span><script type="math/tex">o_{t} \circ \tanh \left(c_{t}\right)</script></span> 运算得到最终的 <span class="arithmatex"><span class="MathJax_Preview">h_t</span><script type="math/tex">h_t</script></span> 。</li>
</ol>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2019 - 2020 Xiao Xu
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/looperXX" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://www.linkedin.com/in/%E5%95%B8-%E5%BE%90-012456163/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "header.autohide"], "translations": {"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.placeholder": "\u641c\u7d22", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.term.missing": "\u7f3a\u5c11", "select.version.title": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "search": "../assets/javascripts/workers/search.0bbba5b5.min.js"}</script>
    
    
      <script src="../assets/javascripts/bundle.e1a181d9.min.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
        <script src="../js/baidu-tongji.js"></script>
      
    
  </body>
</html>